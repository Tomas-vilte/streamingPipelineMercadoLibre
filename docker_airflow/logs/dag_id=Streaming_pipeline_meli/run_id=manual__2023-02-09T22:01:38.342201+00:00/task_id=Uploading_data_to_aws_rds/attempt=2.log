[2023-02-09T22:09:05.751+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: Streaming_pipeline_meli.Uploading_data_to_aws_rds manual__2023-02-09T22:01:38.342201+00:00 [queued]>
[2023-02-09T22:09:05.759+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: Streaming_pipeline_meli.Uploading_data_to_aws_rds manual__2023-02-09T22:01:38.342201+00:00 [queued]>
[2023-02-09T22:09:05.759+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-02-09T22:09:05.760+0000] {taskinstance.py:1284} INFO - Starting attempt 2 of 6
[2023-02-09T22:09:05.760+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-02-09T22:09:05.774+0000] {taskinstance.py:1304} INFO - Executing <Task(PythonOperator): Uploading_data_to_aws_rds> on 2023-02-09 22:01:38.342201+00:00
[2023-02-09T22:09:05.792+0000] {standard_task_runner.py:55} INFO - Started process 3302 to run task
[2023-02-09T22:09:05.797+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'Streaming_pipeline_meli', 'Uploading_data_to_aws_rds', 'manual__2023-02-09T22:01:38.342201+00:00', '--job-id', '1532', '--raw', '--subdir', 'DAGS_FOLDER/streamPipeline.py', '--cfg-path', '/tmp/tmpy9fwbvq1']
[2023-02-09T22:09:05.798+0000] {standard_task_runner.py:83} INFO - Job 1532: Subtask Uploading_data_to_aws_rds
[2023-02-09T22:09:05.851+0000] {task_command.py:389} INFO - Running <TaskInstance: Streaming_pipeline_meli.Uploading_data_to_aws_rds manual__2023-02-09T22:01:38.342201+00:00 [running]> on host b0a2a9b8da88
[2023-02-09T22:09:05.971+0000] {taskinstance.py:1513} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=***
AIRFLOW_CTX_DAG_ID=Streaming_pipeline_meli
AIRFLOW_CTX_TASK_ID=Uploading_data_to_aws_rds
AIRFLOW_CTX_EXECUTION_DATE=2023-02-09T22:01:38.342201+00:00
AIRFLOW_CTX_TRY_NUMBER=2
AIRFLOW_CTX_DAG_RUN_ID=manual__2023-02-09T22:01:38.342201+00:00
[2023-02-09T22:09:36.016+0000] {getDataOfMongo.py:48} ERROR - Hubo un error al obtener los datos: 172.20.0.8:27017: timed out, Timeout: 30s, Topology Description: <TopologyDescription id: 63e56f01ad52e47bdf731086, topology_type: Unknown, servers: [<ServerDescription ('172.20.0.8', 27017) server_type: Unknown, rtt: None, error=NetworkTimeout('172.20.0.8:27017: timed out')>]>
[2023-02-09T22:09:36.017+0000] {taskinstance.py:1772} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/operators/python.py", line 175, in execute
    return_value = self.execute_callable()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/operators/python.py", line 192, in execute_callable
    return self.python_callable(*self.op_args, **self.op_kwargs)
  File "/opt/airflow/dags/loadDataToRds.py", line 76, in uploadData
    client = getDataOfMongo(limit=50, database='mercadolibredb', coleccion='meliproduct')
  File "/opt/airflow/dags/getDataOfMongo.py", line 24, in getDataOfMongo
    if database not in clients.list_database_names():
  File "/home/airflow/.local/lib/python3.7/site-packages/pymongo/mongo_client.py", line 1867, in list_database_names
    return [doc["name"] for doc in self.list_databases(session, nameOnly=True, comment=comment)]
  File "/home/airflow/.local/lib/python3.7/site-packages/pymongo/mongo_client.py", line 1840, in list_databases
    res = admin._retryable_read_command(cmd, session=session)
  File "/home/airflow/.local/lib/python3.7/site-packages/pymongo/database.py", line 849, in _retryable_read_command
    return self.__client._retryable_read(_cmd, read_preference, session)
  File "/home/airflow/.local/lib/python3.7/site-packages/pymongo/_csot.py", line 105, in csot_wrapper
    return func(self, *args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/pymongo/mongo_client.py", line 1441, in _retryable_read
    server = self._select_server(read_pref, session, address=address)
  File "/home/airflow/.local/lib/python3.7/site-packages/pymongo/mongo_client.py", line 1257, in _select_server
    server = topology.select_server(server_selector)
  File "/home/airflow/.local/lib/python3.7/site-packages/pymongo/topology.py", line 272, in select_server
    server = self._select_server(selector, server_selection_timeout, address)
  File "/home/airflow/.local/lib/python3.7/site-packages/pymongo/topology.py", line 261, in _select_server
    servers = self.select_servers(selector, server_selection_timeout, address)
  File "/home/airflow/.local/lib/python3.7/site-packages/pymongo/topology.py", line 223, in select_servers
    server_descriptions = self._select_servers_loop(selector, server_timeout, address)
  File "/home/airflow/.local/lib/python3.7/site-packages/pymongo/topology.py", line 240, in _select_servers_loop
    % (self._error_message(selector), timeout, self.description)
pymongo.errors.ServerSelectionTimeoutError: 172.20.0.8:27017: timed out, Timeout: 30s, Topology Description: <TopologyDescription id: 63e56f01ad52e47bdf731086, topology_type: Unknown, servers: [<ServerDescription ('172.20.0.8', 27017) server_type: Unknown, rtt: None, error=NetworkTimeout('172.20.0.8:27017: timed out')>]>
[2023-02-09T22:09:36.027+0000] {taskinstance.py:1327} INFO - Marking task as UP_FOR_RETRY. dag_id=Streaming_pipeline_meli, task_id=Uploading_data_to_aws_rds, execution_date=20230209T220138, start_date=20230209T220905, end_date=20230209T220936
[2023-02-09T22:09:36.052+0000] {standard_task_runner.py:105} ERROR - Failed to execute job 1532 for task Uploading_data_to_aws_rds (172.20.0.8:27017: timed out, Timeout: 30s, Topology Description: <TopologyDescription id: 63e56f01ad52e47bdf731086, topology_type: Unknown, servers: [<ServerDescription ('172.20.0.8', 27017) server_type: Unknown, rtt: None, error=NetworkTimeout('172.20.0.8:27017: timed out')>]>; 3302)
[2023-02-09T22:09:36.110+0000] {local_task_job.py:159} INFO - Task exited with return code 1
[2023-02-09T22:09:36.124+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
