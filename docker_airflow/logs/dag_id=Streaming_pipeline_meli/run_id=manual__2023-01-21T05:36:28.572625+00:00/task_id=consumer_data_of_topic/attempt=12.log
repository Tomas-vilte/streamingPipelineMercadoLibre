[2023-01-26T18:10:07.813+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: Streaming_pipeline_meli.consumer_data_of_topic manual__2023-01-21T05:36:28.572625+00:00 [queued]>
[2023-01-26T18:10:07.822+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: Streaming_pipeline_meli.consumer_data_of_topic manual__2023-01-21T05:36:28.572625+00:00 [queued]>
[2023-01-26T18:10:07.822+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2023-01-26T18:10:07.823+0000] {taskinstance.py:1284} INFO - Starting attempt 12 of 12
[2023-01-26T18:10:07.823+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2023-01-26T18:10:07.835+0000] {taskinstance.py:1304} INFO - Executing <Task(SparkSubmitOperator): consumer_data_of_topic> on 2023-01-21 05:36:28.572625+00:00
[2023-01-26T18:10:07.840+0000] {standard_task_runner.py:55} INFO - Started process 1401 to run task
[2023-01-26T18:10:07.842+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'Streaming_pipeline_meli', 'consumer_data_of_topic', 'manual__2023-01-21T05:36:28.572625+00:00', '--job-id', '1041', '--raw', '--subdir', 'DAGS_FOLDER/producer_dag.py', '--cfg-path', '/tmp/tmp85lnhbdg']
[2023-01-26T18:10:07.843+0000] {standard_task_runner.py:83} INFO - Job 1041: Subtask consumer_data_of_topic
[2023-01-26T18:10:07.891+0000] {task_command.py:389} INFO - Running <TaskInstance: Streaming_pipeline_meli.consumer_data_of_topic manual__2023-01-21T05:36:28.572625+00:00 [running]> on host f1cbb5df32e6
[2023-01-26T18:10:07.949+0000] {taskinstance.py:1513} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=***
AIRFLOW_CTX_DAG_ID=Streaming_pipeline_meli
AIRFLOW_CTX_TASK_ID=consumer_data_of_topic
AIRFLOW_CTX_EXECUTION_DATE=2023-01-21T05:36:28.572625+00:00
AIRFLOW_CTX_TRY_NUMBER=12
AIRFLOW_CTX_DAG_RUN_ID=manual__2023-01-21T05:36:28.572625+00:00
[2023-01-26T18:10:07.956+0000] {base.py:73} INFO - Using connection ID 'spark_default' for task execution.
[2023-01-26T18:10:07.958+0000] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master spark://tomi-H310:7077 --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.1.3,org.mongodb.spark:mongo-spark-connector_2.12:3.0.1 --name arrow-spark --queue root.default /opt/***/dags/consumerSpark.py
[2023-01-26T18:10:08.064+0000] {spark_submit.py:495} INFO - /home/***/.local/lib/python3.7/site-packages/pyspark/bin/load-spark-env.sh: line 68: ps: command not found
[2023-01-26T18:10:09.489+0000] {spark_submit.py:495} INFO - :: loading settings :: url = jar:file:/home/***/.local/lib/python3.7/site-packages/pyspark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
[2023-01-26T18:10:09.542+0000] {spark_submit.py:495} INFO - Ivy Default Cache set to: /home/***/.ivy2/cache
[2023-01-26T18:10:09.543+0000] {spark_submit.py:495} INFO - The jars for the packages stored in: /home/***/.ivy2/jars
[2023-01-26T18:10:09.547+0000] {spark_submit.py:495} INFO - org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency
[2023-01-26T18:10:09.547+0000] {spark_submit.py:495} INFO - org.mongodb.spark#mongo-spark-connector_2.12 added as a dependency
[2023-01-26T18:10:09.548+0000] {spark_submit.py:495} INFO - :: resolving dependencies :: org.apache.spark#spark-submit-parent-3716066a-78ed-47cc-ba64-0c9efd75e3ec;1.0
[2023-01-26T18:10:09.548+0000] {spark_submit.py:495} INFO - confs: [default]
[2023-01-26T18:10:09.693+0000] {spark_submit.py:495} INFO - found org.apache.spark#spark-sql-kafka-0-10_2.12;3.1.3 in central
[2023-01-26T18:10:09.744+0000] {spark_submit.py:495} INFO - found org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.1.3 in central
[2023-01-26T18:10:09.770+0000] {spark_submit.py:495} INFO - found org.apache.kafka#kafka-clients;2.6.0 in central
[2023-01-26T18:10:09.793+0000] {spark_submit.py:495} INFO - found com.github.luben#zstd-jni;1.4.8-1 in central
[2023-01-26T18:10:09.811+0000] {spark_submit.py:495} INFO - found org.lz4#lz4-java;1.7.1 in central
[2023-01-26T18:10:09.824+0000] {spark_submit.py:495} INFO - found org.xerial.snappy#snappy-java;1.1.8.2 in central
[2023-01-26T18:10:09.836+0000] {spark_submit.py:495} INFO - found org.slf4j#slf4j-api;1.7.30 in central
[2023-01-26T18:10:09.847+0000] {spark_submit.py:495} INFO - found org.spark-project.spark#unused;1.0.0 in central
[2023-01-26T18:10:09.859+0000] {spark_submit.py:495} INFO - found org.apache.commons#commons-pool2;2.6.2 in central
[2023-01-26T18:10:09.871+0000] {spark_submit.py:495} INFO - found org.mongodb.spark#mongo-spark-connector_2.12;3.0.1 in central
[2023-01-26T18:10:09.879+0000] {spark_submit.py:495} INFO - found org.mongodb#mongodb-driver-sync;4.0.5 in central
[2023-01-26T18:10:09.886+0000] {spark_submit.py:495} INFO - found org.mongodb#bson;4.0.5 in central
[2023-01-26T18:10:09.894+0000] {spark_submit.py:495} INFO - found org.mongodb#mongodb-driver-core;4.0.5 in central
[2023-01-26T18:10:09.919+0000] {spark_submit.py:495} INFO - :: resolution report :: resolve 357ms :: artifacts dl 14ms
[2023-01-26T18:10:09.919+0000] {spark_submit.py:495} INFO - :: modules in use:
[2023-01-26T18:10:09.919+0000] {spark_submit.py:495} INFO - com.github.luben#zstd-jni;1.4.8-1 from central in [default]
[2023-01-26T18:10:09.920+0000] {spark_submit.py:495} INFO - org.apache.commons#commons-pool2;2.6.2 from central in [default]
[2023-01-26T18:10:09.920+0000] {spark_submit.py:495} INFO - org.apache.kafka#kafka-clients;2.6.0 from central in [default]
[2023-01-26T18:10:09.920+0000] {spark_submit.py:495} INFO - org.apache.spark#spark-sql-kafka-0-10_2.12;3.1.3 from central in [default]
[2023-01-26T18:10:09.920+0000] {spark_submit.py:495} INFO - org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.1.3 from central in [default]
[2023-01-26T18:10:09.920+0000] {spark_submit.py:495} INFO - org.lz4#lz4-java;1.7.1 from central in [default]
[2023-01-26T18:10:09.921+0000] {spark_submit.py:495} INFO - org.mongodb#bson;4.0.5 from central in [default]
[2023-01-26T18:10:09.921+0000] {spark_submit.py:495} INFO - org.mongodb#mongodb-driver-core;4.0.5 from central in [default]
[2023-01-26T18:10:09.921+0000] {spark_submit.py:495} INFO - org.mongodb#mongodb-driver-sync;4.0.5 from central in [default]
[2023-01-26T18:10:09.921+0000] {spark_submit.py:495} INFO - org.mongodb.spark#mongo-spark-connector_2.12;3.0.1 from central in [default]
[2023-01-26T18:10:09.921+0000] {spark_submit.py:495} INFO - org.slf4j#slf4j-api;1.7.30 from central in [default]
[2023-01-26T18:10:09.922+0000] {spark_submit.py:495} INFO - org.spark-project.spark#unused;1.0.0 from central in [default]
[2023-01-26T18:10:09.922+0000] {spark_submit.py:495} INFO - org.xerial.snappy#snappy-java;1.1.8.2 from central in [default]
[2023-01-26T18:10:09.922+0000] {spark_submit.py:495} INFO - ---------------------------------------------------------------------
[2023-01-26T18:10:09.922+0000] {spark_submit.py:495} INFO - |                  |            modules            ||   artifacts   |
[2023-01-26T18:10:09.922+0000] {spark_submit.py:495} INFO - |       conf       | number| search|dwnlded|evicted|| number|dwnlded|
[2023-01-26T18:10:09.922+0000] {spark_submit.py:495} INFO - ---------------------------------------------------------------------
[2023-01-26T18:10:09.923+0000] {spark_submit.py:495} INFO - |      default     |   13  |   0   |   0   |   0   ||   13  |   0   |
[2023-01-26T18:10:09.923+0000] {spark_submit.py:495} INFO - ---------------------------------------------------------------------
[2023-01-26T18:10:09.928+0000] {spark_submit.py:495} INFO - :: retrieving :: org.apache.spark#spark-submit-parent-3716066a-78ed-47cc-ba64-0c9efd75e3ec
[2023-01-26T18:10:09.928+0000] {spark_submit.py:495} INFO - confs: [default]
[2023-01-26T18:10:09.934+0000] {spark_submit.py:495} INFO - 0 artifacts copied, 13 already retrieved (0kB/6ms)
[2023-01-26T18:10:10.123+0000] {spark_submit.py:495} INFO - 23/01/26 18:10:10 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2023-01-26T18:10:11.582+0000] {spark_submit.py:495} INFO - 23/01/26 18:10:11 INFO SparkContext: Running Spark version 3.3.1
[2023-01-26T18:10:11.600+0000] {spark_submit.py:495} INFO - 23/01/26 18:10:11 INFO ResourceUtils: ==============================================================
[2023-01-26T18:10:11.600+0000] {spark_submit.py:495} INFO - 23/01/26 18:10:11 INFO ResourceUtils: No custom resources configured for spark.driver.
[2023-01-26T18:10:11.601+0000] {spark_submit.py:495} INFO - 23/01/26 18:10:11 INFO ResourceUtils: ==============================================================
[2023-01-26T18:10:11.601+0000] {spark_submit.py:495} INFO - 23/01/26 18:10:11 INFO SparkContext: Submitted application: arrow-spark
[2023-01-26T18:10:11.620+0000] {spark_submit.py:495} INFO - 23/01/26 18:10:11 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2023-01-26T18:10:11.652+0000] {spark_submit.py:495} INFO - 23/01/26 18:10:11 INFO ResourceProfile: Limiting resource is cpu
[2023-01-26T18:10:11.652+0000] {spark_submit.py:495} INFO - 23/01/26 18:10:11 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2023-01-26T18:10:11.703+0000] {spark_submit.py:495} INFO - 23/01/26 18:10:11 INFO SecurityManager: Changing view acls to: default
[2023-01-26T18:10:11.704+0000] {spark_submit.py:495} INFO - 23/01/26 18:10:11 INFO SecurityManager: Changing modify acls to: default
[2023-01-26T18:10:11.705+0000] {spark_submit.py:495} INFO - 23/01/26 18:10:11 INFO SecurityManager: Changing view acls groups to:
[2023-01-26T18:10:11.712+0000] {spark_submit.py:495} INFO - 23/01/26 18:10:11 INFO SecurityManager: Changing modify acls groups to:
[2023-01-26T18:10:11.713+0000] {spark_submit.py:495} INFO - 23/01/26 18:10:11 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(default); groups with view permissions: Set(); users  with modify permissions: Set(default); groups with modify permissions: Set()
[2023-01-26T18:10:11.928+0000] {spark_submit.py:495} INFO - 23/01/26 18:10:11 INFO Utils: Successfully started service 'sparkDriver' on port 37791.
[2023-01-26T18:10:11.957+0000] {spark_submit.py:495} INFO - 23/01/26 18:10:11 INFO SparkEnv: Registering MapOutputTracker
[2023-01-26T18:10:11.986+0000] {spark_submit.py:495} INFO - 23/01/26 18:10:11 INFO SparkEnv: Registering BlockManagerMaster
[2023-01-26T18:10:12.002+0000] {spark_submit.py:495} INFO - 23/01/26 18:10:12 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2023-01-26T18:10:12.002+0000] {spark_submit.py:495} INFO - 23/01/26 18:10:12 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2023-01-26T18:10:12.005+0000] {spark_submit.py:495} INFO - 23/01/26 18:10:12 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2023-01-26T18:10:12.024+0000] {spark_submit.py:495} INFO - 23/01/26 18:10:12 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-9c61fc1f-0d12-4913-80f2-a5fb5ff4cce0
[2023-01-26T18:10:12.039+0000] {spark_submit.py:495} INFO - 23/01/26 18:10:12 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2023-01-26T18:10:12.054+0000] {spark_submit.py:495} INFO - 23/01/26 18:10:12 INFO SparkEnv: Registering OutputCommitCoordinator
[2023-01-26T18:10:12.266+0000] {spark_submit.py:495} INFO - 23/01/26 18:10:12 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[2023-01-26T18:10:12.272+0000] {spark_submit.py:495} INFO - 23/01/26 18:10:12 INFO Utils: Successfully started service 'SparkUI' on port 4041.
[2023-01-26T18:10:12.300+0000] {spark_submit.py:495} INFO - 23/01/26 18:10:12 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.1.3.jar at spark://f1cbb5df32e6:37791/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.1.3.jar with timestamp 1674756611562
[2023-01-26T18:10:12.300+0000] {spark_submit.py:495} INFO - 23/01/26 18:10:12 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.mongodb.spark_mongo-spark-connector_2.12-3.0.1.jar at spark://f1cbb5df32e6:37791/jars/org.mongodb.spark_mongo-spark-connector_2.12-3.0.1.jar with timestamp 1674756611562
[2023-01-26T18:10:12.301+0000] {spark_submit.py:495} INFO - 23/01/26 18:10:12 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.1.3.jar at spark://f1cbb5df32e6:37791/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.1.3.jar with timestamp 1674756611562
[2023-01-26T18:10:12.301+0000] {spark_submit.py:495} INFO - 23/01/26 18:10:12 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.apache.kafka_kafka-clients-2.6.0.jar at spark://f1cbb5df32e6:37791/jars/org.apache.kafka_kafka-clients-2.6.0.jar with timestamp 1674756611562
[2023-01-26T18:10:12.301+0000] {spark_submit.py:495} INFO - 23/01/26 18:10:12 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.apache.commons_commons-pool2-2.6.2.jar at spark://f1cbb5df32e6:37791/jars/org.apache.commons_commons-pool2-2.6.2.jar with timestamp 1674756611562
[2023-01-26T18:10:12.301+0000] {spark_submit.py:495} INFO - 23/01/26 18:10:12 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar at spark://f1cbb5df32e6:37791/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1674756611562
[2023-01-26T18:10:12.301+0000] {spark_submit.py:495} INFO - 23/01/26 18:10:12 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/com.github.luben_zstd-jni-1.4.8-1.jar at spark://f1cbb5df32e6:37791/jars/com.github.luben_zstd-jni-1.4.8-1.jar with timestamp 1674756611562
[2023-01-26T18:10:12.301+0000] {spark_submit.py:495} INFO - 23/01/26 18:10:12 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.lz4_lz4-java-1.7.1.jar at spark://f1cbb5df32e6:37791/jars/org.lz4_lz4-java-1.7.1.jar with timestamp 1674756611562
[2023-01-26T18:10:12.301+0000] {spark_submit.py:495} INFO - 23/01/26 18:10:12 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.8.2.jar at spark://f1cbb5df32e6:37791/jars/org.xerial.snappy_snappy-java-1.1.8.2.jar with timestamp 1674756611562
[2023-01-26T18:10:12.301+0000] {spark_submit.py:495} INFO - 23/01/26 18:10:12 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.slf4j_slf4j-api-1.7.30.jar at spark://f1cbb5df32e6:37791/jars/org.slf4j_slf4j-api-1.7.30.jar with timestamp 1674756611562
[2023-01-26T18:10:12.301+0000] {spark_submit.py:495} INFO - 23/01/26 18:10:12 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.mongodb_mongodb-driver-sync-4.0.5.jar at spark://f1cbb5df32e6:37791/jars/org.mongodb_mongodb-driver-sync-4.0.5.jar with timestamp 1674756611562
[2023-01-26T18:10:12.301+0000] {spark_submit.py:495} INFO - 23/01/26 18:10:12 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.mongodb_bson-4.0.5.jar at spark://f1cbb5df32e6:37791/jars/org.mongodb_bson-4.0.5.jar with timestamp 1674756611562
[2023-01-26T18:10:12.301+0000] {spark_submit.py:495} INFO - 23/01/26 18:10:12 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.mongodb_mongodb-driver-core-4.0.5.jar at spark://f1cbb5df32e6:37791/jars/org.mongodb_mongodb-driver-core-4.0.5.jar with timestamp 1674756611562
[2023-01-26T18:10:12.303+0000] {spark_submit.py:495} INFO - 23/01/26 18:10:12 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.1.3.jar at file:///home/***/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.1.3.jar with timestamp 1674756611562
[2023-01-26T18:10:12.304+0000] {spark_submit.py:495} INFO - 23/01/26 18:10:12 INFO Utils: Copying /home/***/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.1.3.jar to /tmp/spark-3b0da480-f741-4365-9651-6beefbf6a56f/userFiles-90003119-4276-4d5a-b1a1-69944cae6352/org.apache.spark_spark-sql-kafka-0-10_2.12-3.1.3.jar
[2023-01-26T18:10:12.314+0000] {spark_submit.py:495} INFO - 23/01/26 18:10:12 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.mongodb.spark_mongo-spark-connector_2.12-3.0.1.jar at file:///home/***/.ivy2/jars/org.mongodb.spark_mongo-spark-connector_2.12-3.0.1.jar with timestamp 1674756611562
[2023-01-26T18:10:12.314+0000] {spark_submit.py:495} INFO - 23/01/26 18:10:12 INFO Utils: Copying /home/***/.ivy2/jars/org.mongodb.spark_mongo-spark-connector_2.12-3.0.1.jar to /tmp/spark-3b0da480-f741-4365-9651-6beefbf6a56f/userFiles-90003119-4276-4d5a-b1a1-69944cae6352/org.mongodb.spark_mongo-spark-connector_2.12-3.0.1.jar
[2023-01-26T18:10:12.317+0000] {spark_submit.py:495} INFO - 23/01/26 18:10:12 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.1.3.jar at file:///home/***/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.1.3.jar with timestamp 1674756611562
[2023-01-26T18:10:12.317+0000] {spark_submit.py:495} INFO - 23/01/26 18:10:12 INFO Utils: Copying /home/***/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.1.3.jar to /tmp/spark-3b0da480-f741-4365-9651-6beefbf6a56f/userFiles-90003119-4276-4d5a-b1a1-69944cae6352/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.1.3.jar
[2023-01-26T18:10:12.320+0000] {spark_submit.py:495} INFO - 23/01/26 18:10:12 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.apache.kafka_kafka-clients-2.6.0.jar at file:///home/***/.ivy2/jars/org.apache.kafka_kafka-clients-2.6.0.jar with timestamp 1674756611562
[2023-01-26T18:10:12.320+0000] {spark_submit.py:495} INFO - 23/01/26 18:10:12 INFO Utils: Copying /home/***/.ivy2/jars/org.apache.kafka_kafka-clients-2.6.0.jar to /tmp/spark-3b0da480-f741-4365-9651-6beefbf6a56f/userFiles-90003119-4276-4d5a-b1a1-69944cae6352/org.apache.kafka_kafka-clients-2.6.0.jar
[2023-01-26T18:10:12.326+0000] {spark_submit.py:495} INFO - 23/01/26 18:10:12 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.apache.commons_commons-pool2-2.6.2.jar at file:///home/***/.ivy2/jars/org.apache.commons_commons-pool2-2.6.2.jar with timestamp 1674756611562
[2023-01-26T18:10:12.327+0000] {spark_submit.py:495} INFO - 23/01/26 18:10:12 INFO Utils: Copying /home/***/.ivy2/jars/org.apache.commons_commons-pool2-2.6.2.jar to /tmp/spark-3b0da480-f741-4365-9651-6beefbf6a56f/userFiles-90003119-4276-4d5a-b1a1-69944cae6352/org.apache.commons_commons-pool2-2.6.2.jar
[2023-01-26T18:10:12.330+0000] {spark_submit.py:495} INFO - 23/01/26 18:10:12 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar at file:///home/***/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1674756611562
[2023-01-26T18:10:12.330+0000] {spark_submit.py:495} INFO - 23/01/26 18:10:12 INFO Utils: Copying /home/***/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar to /tmp/spark-3b0da480-f741-4365-9651-6beefbf6a56f/userFiles-90003119-4276-4d5a-b1a1-69944cae6352/org.spark-project.spark_unused-1.0.0.jar
[2023-01-26T18:10:12.338+0000] {spark_submit.py:495} INFO - 23/01/26 18:10:12 INFO SparkContext: Added file file:///home/***/.ivy2/jars/com.github.luben_zstd-jni-1.4.8-1.jar at file:///home/***/.ivy2/jars/com.github.luben_zstd-jni-1.4.8-1.jar with timestamp 1674756611562
[2023-01-26T18:10:12.338+0000] {spark_submit.py:495} INFO - 23/01/26 18:10:12 INFO Utils: Copying /home/***/.ivy2/jars/com.github.luben_zstd-jni-1.4.8-1.jar to /tmp/spark-3b0da480-f741-4365-9651-6beefbf6a56f/userFiles-90003119-4276-4d5a-b1a1-69944cae6352/com.github.luben_zstd-jni-1.4.8-1.jar
[2023-01-26T18:10:12.348+0000] {spark_submit.py:495} INFO - 23/01/26 18:10:12 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.lz4_lz4-java-1.7.1.jar at file:///home/***/.ivy2/jars/org.lz4_lz4-java-1.7.1.jar with timestamp 1674756611562
[2023-01-26T18:10:12.348+0000] {spark_submit.py:495} INFO - 23/01/26 18:10:12 INFO Utils: Copying /home/***/.ivy2/jars/org.lz4_lz4-java-1.7.1.jar to /tmp/spark-3b0da480-f741-4365-9651-6beefbf6a56f/userFiles-90003119-4276-4d5a-b1a1-69944cae6352/org.lz4_lz4-java-1.7.1.jar
[2023-01-26T18:10:12.361+0000] {spark_submit.py:495} INFO - 23/01/26 18:10:12 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.8.2.jar at file:///home/***/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.8.2.jar with timestamp 1674756611562
[2023-01-26T18:10:12.362+0000] {spark_submit.py:495} INFO - 23/01/26 18:10:12 INFO Utils: Copying /home/***/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.8.2.jar to /tmp/spark-3b0da480-f741-4365-9651-6beefbf6a56f/userFiles-90003119-4276-4d5a-b1a1-69944cae6352/org.xerial.snappy_snappy-java-1.1.8.2.jar
[2023-01-26T18:10:12.373+0000] {spark_submit.py:495} INFO - 23/01/26 18:10:12 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.slf4j_slf4j-api-1.7.30.jar at file:///home/***/.ivy2/jars/org.slf4j_slf4j-api-1.7.30.jar with timestamp 1674756611562
[2023-01-26T18:10:12.373+0000] {spark_submit.py:495} INFO - 23/01/26 18:10:12 INFO Utils: Copying /home/***/.ivy2/jars/org.slf4j_slf4j-api-1.7.30.jar to /tmp/spark-3b0da480-f741-4365-9651-6beefbf6a56f/userFiles-90003119-4276-4d5a-b1a1-69944cae6352/org.slf4j_slf4j-api-1.7.30.jar
[2023-01-26T18:10:12.378+0000] {spark_submit.py:495} INFO - 23/01/26 18:10:12 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.mongodb_mongodb-driver-sync-4.0.5.jar at file:///home/***/.ivy2/jars/org.mongodb_mongodb-driver-sync-4.0.5.jar with timestamp 1674756611562
[2023-01-26T18:10:12.378+0000] {spark_submit.py:495} INFO - 23/01/26 18:10:12 INFO Utils: Copying /home/***/.ivy2/jars/org.mongodb_mongodb-driver-sync-4.0.5.jar to /tmp/spark-3b0da480-f741-4365-9651-6beefbf6a56f/userFiles-90003119-4276-4d5a-b1a1-69944cae6352/org.mongodb_mongodb-driver-sync-4.0.5.jar
[2023-01-26T18:10:12.400+0000] {spark_submit.py:495} INFO - 23/01/26 18:10:12 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.mongodb_bson-4.0.5.jar at file:///home/***/.ivy2/jars/org.mongodb_bson-4.0.5.jar with timestamp 1674756611562
[2023-01-26T18:10:12.400+0000] {spark_submit.py:495} INFO - 23/01/26 18:10:12 INFO Utils: Copying /home/***/.ivy2/jars/org.mongodb_bson-4.0.5.jar to /tmp/spark-3b0da480-f741-4365-9651-6beefbf6a56f/userFiles-90003119-4276-4d5a-b1a1-69944cae6352/org.mongodb_bson-4.0.5.jar
[2023-01-26T18:10:12.404+0000] {spark_submit.py:495} INFO - 23/01/26 18:10:12 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.mongodb_mongodb-driver-core-4.0.5.jar at file:///home/***/.ivy2/jars/org.mongodb_mongodb-driver-core-4.0.5.jar with timestamp 1674756611562
[2023-01-26T18:10:12.405+0000] {spark_submit.py:495} INFO - 23/01/26 18:10:12 INFO Utils: Copying /home/***/.ivy2/jars/org.mongodb_mongodb-driver-core-4.0.5.jar to /tmp/spark-3b0da480-f741-4365-9651-6beefbf6a56f/userFiles-90003119-4276-4d5a-b1a1-69944cae6352/org.mongodb_mongodb-driver-core-4.0.5.jar
[2023-01-26T18:10:12.479+0000] {spark_submit.py:495} INFO - 23/01/26 18:10:12 INFO Executor: Starting executor ID driver on host f1cbb5df32e6
[2023-01-26T18:10:12.485+0000] {spark_submit.py:495} INFO - 23/01/26 18:10:12 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
[2023-01-26T18:10:12.494+0000] {spark_submit.py:495} INFO - 23/01/26 18:10:12 INFO Executor: Fetching file:///home/***/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.1.3.jar with timestamp 1674756611562
[2023-01-26T18:10:12.509+0000] {spark_submit.py:495} INFO - 23/01/26 18:10:12 INFO Utils: /home/***/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.1.3.jar has been previously copied to /tmp/spark-3b0da480-f741-4365-9651-6beefbf6a56f/userFiles-90003119-4276-4d5a-b1a1-69944cae6352/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.1.3.jar
[2023-01-26T18:10:12.512+0000] {spark_submit.py:495} INFO - 23/01/26 18:10:12 INFO Executor: Fetching file:///home/***/.ivy2/jars/org.mongodb_mongodb-driver-sync-4.0.5.jar with timestamp 1674756611562
[2023-01-26T18:10:12.513+0000] {spark_submit.py:495} INFO - 23/01/26 18:10:12 INFO Utils: /home/***/.ivy2/jars/org.mongodb_mongodb-driver-sync-4.0.5.jar has been previously copied to /tmp/spark-3b0da480-f741-4365-9651-6beefbf6a56f/userFiles-90003119-4276-4d5a-b1a1-69944cae6352/org.mongodb_mongodb-driver-sync-4.0.5.jar
[2023-01-26T18:10:12.516+0000] {spark_submit.py:495} INFO - 23/01/26 18:10:12 INFO Executor: Fetching file:///home/***/.ivy2/jars/org.apache.kafka_kafka-clients-2.6.0.jar with timestamp 1674756611562
[2023-01-26T18:10:12.521+0000] {spark_submit.py:495} INFO - 23/01/26 18:10:12 INFO Utils: /home/***/.ivy2/jars/org.apache.kafka_kafka-clients-2.6.0.jar has been previously copied to /tmp/spark-3b0da480-f741-4365-9651-6beefbf6a56f/userFiles-90003119-4276-4d5a-b1a1-69944cae6352/org.apache.kafka_kafka-clients-2.6.0.jar
[2023-01-26T18:10:12.524+0000] {spark_submit.py:495} INFO - 23/01/26 18:10:12 INFO Executor: Fetching file:///home/***/.ivy2/jars/org.mongodb_bson-4.0.5.jar with timestamp 1674756611562
[2023-01-26T18:10:12.525+0000] {spark_submit.py:495} INFO - 23/01/26 18:10:12 INFO Utils: /home/***/.ivy2/jars/org.mongodb_bson-4.0.5.jar has been previously copied to /tmp/spark-3b0da480-f741-4365-9651-6beefbf6a56f/userFiles-90003119-4276-4d5a-b1a1-69944cae6352/org.mongodb_bson-4.0.5.jar
[2023-01-26T18:10:12.528+0000] {spark_submit.py:495} INFO - 23/01/26 18:10:12 INFO Executor: Fetching file:///home/***/.ivy2/jars/com.github.luben_zstd-jni-1.4.8-1.jar with timestamp 1674756611562
[2023-01-26T18:10:12.533+0000] {spark_submit.py:495} INFO - 23/01/26 18:10:12 INFO Utils: /home/***/.ivy2/jars/com.github.luben_zstd-jni-1.4.8-1.jar has been previously copied to /tmp/spark-3b0da480-f741-4365-9651-6beefbf6a56f/userFiles-90003119-4276-4d5a-b1a1-69944cae6352/com.github.luben_zstd-jni-1.4.8-1.jar
[2023-01-26T18:10:12.538+0000] {spark_submit.py:495} INFO - 23/01/26 18:10:12 INFO Executor: Fetching file:///home/***/.ivy2/jars/org.lz4_lz4-java-1.7.1.jar with timestamp 1674756611562
[2023-01-26T18:10:12.539+0000] {spark_submit.py:495} INFO - 23/01/26 18:10:12 INFO Utils: /home/***/.ivy2/jars/org.lz4_lz4-java-1.7.1.jar has been previously copied to /tmp/spark-3b0da480-f741-4365-9651-6beefbf6a56f/userFiles-90003119-4276-4d5a-b1a1-69944cae6352/org.lz4_lz4-java-1.7.1.jar
[2023-01-26T18:10:12.546+0000] {spark_submit.py:495} INFO - 23/01/26 18:10:12 INFO Executor: Fetching file:///home/***/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1674756611562
[2023-01-26T18:10:12.546+0000] {spark_submit.py:495} INFO - 23/01/26 18:10:12 INFO Utils: /home/***/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar has been previously copied to /tmp/spark-3b0da480-f741-4365-9651-6beefbf6a56f/userFiles-90003119-4276-4d5a-b1a1-69944cae6352/org.spark-project.spark_unused-1.0.0.jar
[2023-01-26T18:10:12.550+0000] {spark_submit.py:495} INFO - 23/01/26 18:10:12 INFO Executor: Fetching file:///home/***/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.8.2.jar with timestamp 1674756611562
[2023-01-26T18:10:12.552+0000] {spark_submit.py:495} INFO - 23/01/26 18:10:12 INFO Utils: /home/***/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.8.2.jar has been previously copied to /tmp/spark-3b0da480-f741-4365-9651-6beefbf6a56f/userFiles-90003119-4276-4d5a-b1a1-69944cae6352/org.xerial.snappy_snappy-java-1.1.8.2.jar
[2023-01-26T18:10:12.556+0000] {spark_submit.py:495} INFO - 23/01/26 18:10:12 INFO Executor: Fetching file:///home/***/.ivy2/jars/org.mongodb_mongodb-driver-core-4.0.5.jar with timestamp 1674756611562
[2023-01-26T18:10:12.557+0000] {spark_submit.py:495} INFO - 23/01/26 18:10:12 INFO Utils: /home/***/.ivy2/jars/org.mongodb_mongodb-driver-core-4.0.5.jar has been previously copied to /tmp/spark-3b0da480-f741-4365-9651-6beefbf6a56f/userFiles-90003119-4276-4d5a-b1a1-69944cae6352/org.mongodb_mongodb-driver-core-4.0.5.jar
[2023-01-26T18:10:12.560+0000] {spark_submit.py:495} INFO - 23/01/26 18:10:12 INFO Executor: Fetching file:///home/***/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.1.3.jar with timestamp 1674756611562
[2023-01-26T18:10:12.561+0000] {spark_submit.py:495} INFO - 23/01/26 18:10:12 INFO Utils: /home/***/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.1.3.jar has been previously copied to /tmp/spark-3b0da480-f741-4365-9651-6beefbf6a56f/userFiles-90003119-4276-4d5a-b1a1-69944cae6352/org.apache.spark_spark-sql-kafka-0-10_2.12-3.1.3.jar
[2023-01-26T18:10:12.564+0000] {spark_submit.py:495} INFO - 23/01/26 18:10:12 INFO Executor: Fetching file:///home/***/.ivy2/jars/org.mongodb.spark_mongo-spark-connector_2.12-3.0.1.jar with timestamp 1674756611562
[2023-01-26T18:10:12.564+0000] {spark_submit.py:495} INFO - 23/01/26 18:10:12 INFO Utils: /home/***/.ivy2/jars/org.mongodb.spark_mongo-spark-connector_2.12-3.0.1.jar has been previously copied to /tmp/spark-3b0da480-f741-4365-9651-6beefbf6a56f/userFiles-90003119-4276-4d5a-b1a1-69944cae6352/org.mongodb.spark_mongo-spark-connector_2.12-3.0.1.jar
[2023-01-26T18:10:12.567+0000] {spark_submit.py:495} INFO - 23/01/26 18:10:12 INFO Executor: Fetching file:///home/***/.ivy2/jars/org.apache.commons_commons-pool2-2.6.2.jar with timestamp 1674756611562
[2023-01-26T18:10:12.567+0000] {spark_submit.py:495} INFO - 23/01/26 18:10:12 INFO Utils: /home/***/.ivy2/jars/org.apache.commons_commons-pool2-2.6.2.jar has been previously copied to /tmp/spark-3b0da480-f741-4365-9651-6beefbf6a56f/userFiles-90003119-4276-4d5a-b1a1-69944cae6352/org.apache.commons_commons-pool2-2.6.2.jar
[2023-01-26T18:10:12.570+0000] {spark_submit.py:495} INFO - 23/01/26 18:10:12 INFO Executor: Fetching file:///home/***/.ivy2/jars/org.slf4j_slf4j-api-1.7.30.jar with timestamp 1674756611562
[2023-01-26T18:10:12.571+0000] {spark_submit.py:495} INFO - 23/01/26 18:10:12 INFO Utils: /home/***/.ivy2/jars/org.slf4j_slf4j-api-1.7.30.jar has been previously copied to /tmp/spark-3b0da480-f741-4365-9651-6beefbf6a56f/userFiles-90003119-4276-4d5a-b1a1-69944cae6352/org.slf4j_slf4j-api-1.7.30.jar
[2023-01-26T18:10:12.576+0000] {spark_submit.py:495} INFO - 23/01/26 18:10:12 INFO Executor: Fetching spark://f1cbb5df32e6:37791/jars/org.mongodb_mongodb-driver-sync-4.0.5.jar with timestamp 1674756611562
[2023-01-26T18:10:12.608+0000] {spark_submit.py:495} INFO - 23/01/26 18:10:12 INFO TransportClientFactory: Successfully created connection to f1cbb5df32e6/172.23.0.4:37791 after 21 ms (0 ms spent in bootstraps)
[2023-01-26T18:10:12.613+0000] {spark_submit.py:495} INFO - 23/01/26 18:10:12 INFO Utils: Fetching spark://f1cbb5df32e6:37791/jars/org.mongodb_mongodb-driver-sync-4.0.5.jar to /tmp/spark-3b0da480-f741-4365-9651-6beefbf6a56f/userFiles-90003119-4276-4d5a-b1a1-69944cae6352/fetchFileTemp12651053171334861469.tmp
[2023-01-26T18:10:12.644+0000] {spark_submit.py:495} INFO - 23/01/26 18:10:12 INFO Utils: /tmp/spark-3b0da480-f741-4365-9651-6beefbf6a56f/userFiles-90003119-4276-4d5a-b1a1-69944cae6352/fetchFileTemp12651053171334861469.tmp has been previously copied to /tmp/spark-3b0da480-f741-4365-9651-6beefbf6a56f/userFiles-90003119-4276-4d5a-b1a1-69944cae6352/org.mongodb_mongodb-driver-sync-4.0.5.jar
[2023-01-26T18:10:12.647+0000] {spark_submit.py:495} INFO - 23/01/26 18:10:12 INFO Executor: Adding file:/tmp/spark-3b0da480-f741-4365-9651-6beefbf6a56f/userFiles-90003119-4276-4d5a-b1a1-69944cae6352/org.mongodb_mongodb-driver-sync-4.0.5.jar to class loader
[2023-01-26T18:10:12.647+0000] {spark_submit.py:495} INFO - 23/01/26 18:10:12 INFO Executor: Fetching spark://f1cbb5df32e6:37791/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1674756611562
[2023-01-26T18:10:12.647+0000] {spark_submit.py:495} INFO - 23/01/26 18:10:12 INFO Utils: Fetching spark://f1cbb5df32e6:37791/jars/org.spark-project.spark_unused-1.0.0.jar to /tmp/spark-3b0da480-f741-4365-9651-6beefbf6a56f/userFiles-90003119-4276-4d5a-b1a1-69944cae6352/fetchFileTemp16199120037498374650.tmp
[2023-01-26T18:10:12.649+0000] {spark_submit.py:495} INFO - 23/01/26 18:10:12 INFO Utils: /tmp/spark-3b0da480-f741-4365-9651-6beefbf6a56f/userFiles-90003119-4276-4d5a-b1a1-69944cae6352/fetchFileTemp16199120037498374650.tmp has been previously copied to /tmp/spark-3b0da480-f741-4365-9651-6beefbf6a56f/userFiles-90003119-4276-4d5a-b1a1-69944cae6352/org.spark-project.spark_unused-1.0.0.jar
[2023-01-26T18:10:12.653+0000] {spark_submit.py:495} INFO - 23/01/26 18:10:12 INFO Executor: Adding file:/tmp/spark-3b0da480-f741-4365-9651-6beefbf6a56f/userFiles-90003119-4276-4d5a-b1a1-69944cae6352/org.spark-project.spark_unused-1.0.0.jar to class loader
[2023-01-26T18:10:12.653+0000] {spark_submit.py:495} INFO - 23/01/26 18:10:12 INFO Executor: Fetching spark://f1cbb5df32e6:37791/jars/org.apache.commons_commons-pool2-2.6.2.jar with timestamp 1674756611562
[2023-01-26T18:10:12.653+0000] {spark_submit.py:495} INFO - 23/01/26 18:10:12 INFO Utils: Fetching spark://f1cbb5df32e6:37791/jars/org.apache.commons_commons-pool2-2.6.2.jar to /tmp/spark-3b0da480-f741-4365-9651-6beefbf6a56f/userFiles-90003119-4276-4d5a-b1a1-69944cae6352/fetchFileTemp10811393910341461422.tmp
[2023-01-26T18:10:12.655+0000] {spark_submit.py:495} INFO - 23/01/26 18:10:12 INFO Utils: /tmp/spark-3b0da480-f741-4365-9651-6beefbf6a56f/userFiles-90003119-4276-4d5a-b1a1-69944cae6352/fetchFileTemp10811393910341461422.tmp has been previously copied to /tmp/spark-3b0da480-f741-4365-9651-6beefbf6a56f/userFiles-90003119-4276-4d5a-b1a1-69944cae6352/org.apache.commons_commons-pool2-2.6.2.jar
[2023-01-26T18:10:12.658+0000] {spark_submit.py:495} INFO - 23/01/26 18:10:12 INFO Executor: Adding file:/tmp/spark-3b0da480-f741-4365-9651-6beefbf6a56f/userFiles-90003119-4276-4d5a-b1a1-69944cae6352/org.apache.commons_commons-pool2-2.6.2.jar to class loader
[2023-01-26T18:10:12.658+0000] {spark_submit.py:495} INFO - 23/01/26 18:10:12 INFO Executor: Fetching spark://f1cbb5df32e6:37791/jars/org.lz4_lz4-java-1.7.1.jar with timestamp 1674756611562
[2023-01-26T18:10:12.660+0000] {spark_submit.py:495} INFO - 23/01/26 18:10:12 INFO Utils: Fetching spark://f1cbb5df32e6:37791/jars/org.lz4_lz4-java-1.7.1.jar to /tmp/spark-3b0da480-f741-4365-9651-6beefbf6a56f/userFiles-90003119-4276-4d5a-b1a1-69944cae6352/fetchFileTemp4832654247687587147.tmp
[2023-01-26T18:10:12.663+0000] {spark_submit.py:495} INFO - 23/01/26 18:10:12 INFO Utils: /tmp/spark-3b0da480-f741-4365-9651-6beefbf6a56f/userFiles-90003119-4276-4d5a-b1a1-69944cae6352/fetchFileTemp4832654247687587147.tmp has been previously copied to /tmp/spark-3b0da480-f741-4365-9651-6beefbf6a56f/userFiles-90003119-4276-4d5a-b1a1-69944cae6352/org.lz4_lz4-java-1.7.1.jar
[2023-01-26T18:10:12.669+0000] {spark_submit.py:495} INFO - 23/01/26 18:10:12 INFO Executor: Adding file:/tmp/spark-3b0da480-f741-4365-9651-6beefbf6a56f/userFiles-90003119-4276-4d5a-b1a1-69944cae6352/org.lz4_lz4-java-1.7.1.jar to class loader
[2023-01-26T18:10:12.669+0000] {spark_submit.py:495} INFO - 23/01/26 18:10:12 INFO Executor: Fetching spark://f1cbb5df32e6:37791/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.1.3.jar with timestamp 1674756611562
[2023-01-26T18:10:12.669+0000] {spark_submit.py:495} INFO - 23/01/26 18:10:12 INFO Utils: Fetching spark://f1cbb5df32e6:37791/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.1.3.jar to /tmp/spark-3b0da480-f741-4365-9651-6beefbf6a56f/userFiles-90003119-4276-4d5a-b1a1-69944cae6352/fetchFileTemp16751429710308819346.tmp
[2023-01-26T18:10:12.671+0000] {spark_submit.py:495} INFO - 23/01/26 18:10:12 INFO Utils: /tmp/spark-3b0da480-f741-4365-9651-6beefbf6a56f/userFiles-90003119-4276-4d5a-b1a1-69944cae6352/fetchFileTemp16751429710308819346.tmp has been previously copied to /tmp/spark-3b0da480-f741-4365-9651-6beefbf6a56f/userFiles-90003119-4276-4d5a-b1a1-69944cae6352/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.1.3.jar
[2023-01-26T18:10:12.674+0000] {spark_submit.py:495} INFO - 23/01/26 18:10:12 INFO Executor: Adding file:/tmp/spark-3b0da480-f741-4365-9651-6beefbf6a56f/userFiles-90003119-4276-4d5a-b1a1-69944cae6352/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.1.3.jar to class loader
[2023-01-26T18:10:12.674+0000] {spark_submit.py:495} INFO - 23/01/26 18:10:12 INFO Executor: Fetching spark://f1cbb5df32e6:37791/jars/org.mongodb.spark_mongo-spark-connector_2.12-3.0.1.jar with timestamp 1674756611562
[2023-01-26T18:10:12.674+0000] {spark_submit.py:495} INFO - 23/01/26 18:10:12 INFO Utils: Fetching spark://f1cbb5df32e6:37791/jars/org.mongodb.spark_mongo-spark-connector_2.12-3.0.1.jar to /tmp/spark-3b0da480-f741-4365-9651-6beefbf6a56f/userFiles-90003119-4276-4d5a-b1a1-69944cae6352/fetchFileTemp2428995602949984872.tmp
[2023-01-26T18:10:12.678+0000] {spark_submit.py:495} INFO - 23/01/26 18:10:12 INFO Utils: /tmp/spark-3b0da480-f741-4365-9651-6beefbf6a56f/userFiles-90003119-4276-4d5a-b1a1-69944cae6352/fetchFileTemp2428995602949984872.tmp has been previously copied to /tmp/spark-3b0da480-f741-4365-9651-6beefbf6a56f/userFiles-90003119-4276-4d5a-b1a1-69944cae6352/org.mongodb.spark_mongo-spark-connector_2.12-3.0.1.jar
[2023-01-26T18:10:12.682+0000] {spark_submit.py:495} INFO - 23/01/26 18:10:12 INFO Executor: Adding file:/tmp/spark-3b0da480-f741-4365-9651-6beefbf6a56f/userFiles-90003119-4276-4d5a-b1a1-69944cae6352/org.mongodb.spark_mongo-spark-connector_2.12-3.0.1.jar to class loader
[2023-01-26T18:10:12.682+0000] {spark_submit.py:495} INFO - 23/01/26 18:10:12 INFO Executor: Fetching spark://f1cbb5df32e6:37791/jars/org.xerial.snappy_snappy-java-1.1.8.2.jar with timestamp 1674756611562
[2023-01-26T18:10:12.682+0000] {spark_submit.py:495} INFO - 23/01/26 18:10:12 INFO Utils: Fetching spark://f1cbb5df32e6:37791/jars/org.xerial.snappy_snappy-java-1.1.8.2.jar to /tmp/spark-3b0da480-f741-4365-9651-6beefbf6a56f/userFiles-90003119-4276-4d5a-b1a1-69944cae6352/fetchFileTemp15551718235891585004.tmp
[2023-01-26T18:10:12.692+0000] {spark_submit.py:495} INFO - 23/01/26 18:10:12 INFO Utils: /tmp/spark-3b0da480-f741-4365-9651-6beefbf6a56f/userFiles-90003119-4276-4d5a-b1a1-69944cae6352/fetchFileTemp15551718235891585004.tmp has been previously copied to /tmp/spark-3b0da480-f741-4365-9651-6beefbf6a56f/userFiles-90003119-4276-4d5a-b1a1-69944cae6352/org.xerial.snappy_snappy-java-1.1.8.2.jar
[2023-01-26T18:10:12.697+0000] {spark_submit.py:495} INFO - 23/01/26 18:10:12 INFO Executor: Adding file:/tmp/spark-3b0da480-f741-4365-9651-6beefbf6a56f/userFiles-90003119-4276-4d5a-b1a1-69944cae6352/org.xerial.snappy_snappy-java-1.1.8.2.jar to class loader
[2023-01-26T18:10:12.697+0000] {spark_submit.py:495} INFO - 23/01/26 18:10:12 INFO Executor: Fetching spark://f1cbb5df32e6:37791/jars/org.mongodb_mongodb-driver-core-4.0.5.jar with timestamp 1674756611562
[2023-01-26T18:10:12.697+0000] {spark_submit.py:495} INFO - 23/01/26 18:10:12 INFO Utils: Fetching spark://f1cbb5df32e6:37791/jars/org.mongodb_mongodb-driver-core-4.0.5.jar to /tmp/spark-3b0da480-f741-4365-9651-6beefbf6a56f/userFiles-90003119-4276-4d5a-b1a1-69944cae6352/fetchFileTemp16298912367402861883.tmp
[2023-01-26T18:10:12.702+0000] {spark_submit.py:495} INFO - 23/01/26 18:10:12 INFO Utils: /tmp/spark-3b0da480-f741-4365-9651-6beefbf6a56f/userFiles-90003119-4276-4d5a-b1a1-69944cae6352/fetchFileTemp16298912367402861883.tmp has been previously copied to /tmp/spark-3b0da480-f741-4365-9651-6beefbf6a56f/userFiles-90003119-4276-4d5a-b1a1-69944cae6352/org.mongodb_mongodb-driver-core-4.0.5.jar
[2023-01-26T18:10:12.707+0000] {spark_submit.py:495} INFO - 23/01/26 18:10:12 INFO Executor: Adding file:/tmp/spark-3b0da480-f741-4365-9651-6beefbf6a56f/userFiles-90003119-4276-4d5a-b1a1-69944cae6352/org.mongodb_mongodb-driver-core-4.0.5.jar to class loader
[2023-01-26T18:10:12.707+0000] {spark_submit.py:495} INFO - 23/01/26 18:10:12 INFO Executor: Fetching spark://f1cbb5df32e6:37791/jars/com.github.luben_zstd-jni-1.4.8-1.jar with timestamp 1674756611562
[2023-01-26T18:10:12.707+0000] {spark_submit.py:495} INFO - 23/01/26 18:10:12 INFO Utils: Fetching spark://f1cbb5df32e6:37791/jars/com.github.luben_zstd-jni-1.4.8-1.jar to /tmp/spark-3b0da480-f741-4365-9651-6beefbf6a56f/userFiles-90003119-4276-4d5a-b1a1-69944cae6352/fetchFileTemp1280581828231856482.tmp
[2023-01-26T18:10:12.777+0000] {spark_submit.py:495} INFO - 23/01/26 18:10:12 INFO Utils: /tmp/spark-3b0da480-f741-4365-9651-6beefbf6a56f/userFiles-90003119-4276-4d5a-b1a1-69944cae6352/fetchFileTemp1280581828231856482.tmp has been previously copied to /tmp/spark-3b0da480-f741-4365-9651-6beefbf6a56f/userFiles-90003119-4276-4d5a-b1a1-69944cae6352/com.github.luben_zstd-jni-1.4.8-1.jar
[2023-01-26T18:10:12.782+0000] {spark_submit.py:495} INFO - 23/01/26 18:10:12 INFO Executor: Adding file:/tmp/spark-3b0da480-f741-4365-9651-6beefbf6a56f/userFiles-90003119-4276-4d5a-b1a1-69944cae6352/com.github.luben_zstd-jni-1.4.8-1.jar to class loader
[2023-01-26T18:10:12.782+0000] {spark_submit.py:495} INFO - 23/01/26 18:10:12 INFO Executor: Fetching spark://f1cbb5df32e6:37791/jars/org.apache.kafka_kafka-clients-2.6.0.jar with timestamp 1674756611562
[2023-01-26T18:10:12.782+0000] {spark_submit.py:495} INFO - 23/01/26 18:10:12 INFO Utils: Fetching spark://f1cbb5df32e6:37791/jars/org.apache.kafka_kafka-clients-2.6.0.jar to /tmp/spark-3b0da480-f741-4365-9651-6beefbf6a56f/userFiles-90003119-4276-4d5a-b1a1-69944cae6352/fetchFileTemp13324741891054766070.tmp
[2023-01-26T18:10:12.795+0000] {spark_submit.py:495} INFO - 23/01/26 18:10:12 INFO Utils: /tmp/spark-3b0da480-f741-4365-9651-6beefbf6a56f/userFiles-90003119-4276-4d5a-b1a1-69944cae6352/fetchFileTemp13324741891054766070.tmp has been previously copied to /tmp/spark-3b0da480-f741-4365-9651-6beefbf6a56f/userFiles-90003119-4276-4d5a-b1a1-69944cae6352/org.apache.kafka_kafka-clients-2.6.0.jar
[2023-01-26T18:10:12.801+0000] {spark_submit.py:495} INFO - 23/01/26 18:10:12 INFO Executor: Adding file:/tmp/spark-3b0da480-f741-4365-9651-6beefbf6a56f/userFiles-90003119-4276-4d5a-b1a1-69944cae6352/org.apache.kafka_kafka-clients-2.6.0.jar to class loader
[2023-01-26T18:10:12.802+0000] {spark_submit.py:495} INFO - 23/01/26 18:10:12 INFO Executor: Fetching spark://f1cbb5df32e6:37791/jars/org.slf4j_slf4j-api-1.7.30.jar with timestamp 1674756611562
[2023-01-26T18:10:12.802+0000] {spark_submit.py:495} INFO - 23/01/26 18:10:12 INFO Utils: Fetching spark://f1cbb5df32e6:37791/jars/org.slf4j_slf4j-api-1.7.30.jar to /tmp/spark-3b0da480-f741-4365-9651-6beefbf6a56f/userFiles-90003119-4276-4d5a-b1a1-69944cae6352/fetchFileTemp14662744380772076735.tmp
[2023-01-26T18:10:12.803+0000] {spark_submit.py:495} INFO - 23/01/26 18:10:12 INFO Utils: /tmp/spark-3b0da480-f741-4365-9651-6beefbf6a56f/userFiles-90003119-4276-4d5a-b1a1-69944cae6352/fetchFileTemp14662744380772076735.tmp has been previously copied to /tmp/spark-3b0da480-f741-4365-9651-6beefbf6a56f/userFiles-90003119-4276-4d5a-b1a1-69944cae6352/org.slf4j_slf4j-api-1.7.30.jar
[2023-01-26T18:10:12.807+0000] {spark_submit.py:495} INFO - 23/01/26 18:10:12 INFO Executor: Adding file:/tmp/spark-3b0da480-f741-4365-9651-6beefbf6a56f/userFiles-90003119-4276-4d5a-b1a1-69944cae6352/org.slf4j_slf4j-api-1.7.30.jar to class loader
[2023-01-26T18:10:12.807+0000] {spark_submit.py:495} INFO - 23/01/26 18:10:12 INFO Executor: Fetching spark://f1cbb5df32e6:37791/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.1.3.jar with timestamp 1674756611562
[2023-01-26T18:10:12.807+0000] {spark_submit.py:495} INFO - 23/01/26 18:10:12 INFO Utils: Fetching spark://f1cbb5df32e6:37791/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.1.3.jar to /tmp/spark-3b0da480-f741-4365-9651-6beefbf6a56f/userFiles-90003119-4276-4d5a-b1a1-69944cae6352/fetchFileTemp4951649372488225081.tmp
[2023-01-26T18:10:12.810+0000] {spark_submit.py:495} INFO - 23/01/26 18:10:12 INFO Utils: /tmp/spark-3b0da480-f741-4365-9651-6beefbf6a56f/userFiles-90003119-4276-4d5a-b1a1-69944cae6352/fetchFileTemp4951649372488225081.tmp has been previously copied to /tmp/spark-3b0da480-f741-4365-9651-6beefbf6a56f/userFiles-90003119-4276-4d5a-b1a1-69944cae6352/org.apache.spark_spark-sql-kafka-0-10_2.12-3.1.3.jar
[2023-01-26T18:10:12.813+0000] {spark_submit.py:495} INFO - 23/01/26 18:10:12 INFO Executor: Adding file:/tmp/spark-3b0da480-f741-4365-9651-6beefbf6a56f/userFiles-90003119-4276-4d5a-b1a1-69944cae6352/org.apache.spark_spark-sql-kafka-0-10_2.12-3.1.3.jar to class loader
[2023-01-26T18:10:12.813+0000] {spark_submit.py:495} INFO - 23/01/26 18:10:12 INFO Executor: Fetching spark://f1cbb5df32e6:37791/jars/org.mongodb_bson-4.0.5.jar with timestamp 1674756611562
[2023-01-26T18:10:12.814+0000] {spark_submit.py:495} INFO - 23/01/26 18:10:12 INFO Utils: Fetching spark://f1cbb5df32e6:37791/jars/org.mongodb_bson-4.0.5.jar to /tmp/spark-3b0da480-f741-4365-9651-6beefbf6a56f/userFiles-90003119-4276-4d5a-b1a1-69944cae6352/fetchFileTemp10146860914310506515.tmp
[2023-01-26T18:10:12.817+0000] {spark_submit.py:495} INFO - 23/01/26 18:10:12 INFO Utils: /tmp/spark-3b0da480-f741-4365-9651-6beefbf6a56f/userFiles-90003119-4276-4d5a-b1a1-69944cae6352/fetchFileTemp10146860914310506515.tmp has been previously copied to /tmp/spark-3b0da480-f741-4365-9651-6beefbf6a56f/userFiles-90003119-4276-4d5a-b1a1-69944cae6352/org.mongodb_bson-4.0.5.jar
[2023-01-26T18:10:12.820+0000] {spark_submit.py:495} INFO - 23/01/26 18:10:12 INFO Executor: Adding file:/tmp/spark-3b0da480-f741-4365-9651-6beefbf6a56f/userFiles-90003119-4276-4d5a-b1a1-69944cae6352/org.mongodb_bson-4.0.5.jar to class loader
[2023-01-26T18:10:12.827+0000] {spark_submit.py:495} INFO - 23/01/26 18:10:12 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 45939.
[2023-01-26T18:10:12.828+0000] {spark_submit.py:495} INFO - 23/01/26 18:10:12 INFO NettyBlockTransferService: Server created on f1cbb5df32e6:45939
[2023-01-26T18:10:12.830+0000] {spark_submit.py:495} INFO - 23/01/26 18:10:12 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2023-01-26T18:10:12.836+0000] {spark_submit.py:495} INFO - 23/01/26 18:10:12 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, f1cbb5df32e6, 45939, None)
[2023-01-26T18:10:12.841+0000] {spark_submit.py:495} INFO - 23/01/26 18:10:12 INFO BlockManagerMasterEndpoint: Registering block manager f1cbb5df32e6:45939 with 434.4 MiB RAM, BlockManagerId(driver, f1cbb5df32e6, 45939, None)
[2023-01-26T18:10:12.843+0000] {spark_submit.py:495} INFO - 23/01/26 18:10:12 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, f1cbb5df32e6, 45939, None)
[2023-01-26T18:10:12.844+0000] {spark_submit.py:495} INFO - 23/01/26 18:10:12 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, f1cbb5df32e6, 45939, None)
[2023-01-26T18:10:16.511+0000] {spark_submit.py:495} INFO - [[34m2023-01-26 18:10:16,509[0m] {[34mjava_gateway.py:[0m2270} INFO[0m - Callback Server Starting[0m
[2023-01-26T18:10:16.511+0000] {spark_submit.py:495} INFO - [[34m2023-01-26 18:10:16,509[0m] {[34mjava_gateway.py:[0m2274} INFO[0m - Socket listening on ('127.0.0.1', 35199)[0m
[2023-01-26T18:10:20.944+0000] {spark_submit.py:495} INFO - [[34m2023-01-26 18:10:20,943[0m] {[34mclientserver.py:[0m561} INFO[0m - Python Server ready to receive messages[0m
[2023-01-26T18:10:20.945+0000] {spark_submit.py:495} INFO - [[34m2023-01-26 18:10:20,944[0m] {[34mclientserver.py:[0m577} INFO[0m - Received command c on object id p0[0m
[2023-01-26T18:10:21.284+0000] {spark_submit.py:495} INFO - -------------------------------------------
[2023-01-26T18:10:21.285+0000] {spark_submit.py:495} INFO - Batch: 0
[2023-01-26T18:10:21.285+0000] {spark_submit.py:495} INFO - -------------------------------------------
[2023-01-26T18:10:21.337+0000] {spark_submit.py:495} INFO - +---+-------------+-----+-------------+----------+----------------+------------+-----------------+--------------+-------------+
[2023-01-26T18:10:21.337+0000] {spark_submit.py:495} INFO - | id|nameOfProduct|price|ProductStatus|SellerName|RegistrationDate|QuantitySold|QuantityAvailable|Free shipping?|Store ratings|
[2023-01-26T18:10:21.337+0000] {spark_submit.py:495} INFO - +---+-------------+-----+-------------+----------+----------------+------------+-----------------+--------------+-------------+
[2023-01-26T18:10:21.337+0000] {spark_submit.py:495} INFO - +---+-------------+-----+-------------+----------+----------------+------------+-----------------+--------------+-------------+
[2023-01-26T18:10:21.337+0000] {spark_submit.py:495} INFO - 
[2023-01-26T18:11:46.976+0000] {spark_submit.py:495} INFO - [[34m2023-01-26 18:11:46,969[0m] {[34mclientserver.py:[0m577} INFO[0m - Received command c on object id p0[0m
[2023-01-26T18:11:50.767+0000] {spark_submit.py:495} INFO - -------------------------------------------
[2023-01-26T18:11:50.768+0000] {spark_submit.py:495} INFO - Batch: 1
[2023-01-26T18:11:50.768+0000] {spark_submit.py:495} INFO - -------------------------------------------
[2023-01-26T18:11:50.841+0000] {spark_submit.py:495} INFO - +-------------+--------------------+--------+-------------+--------------------+-------------------+------------+-----------------+--------------------+-------------+
[2023-01-26T18:11:50.841+0000] {spark_submit.py:495} INFO - |           id|       nameOfProduct|   price|ProductStatus|          SellerName|   RegistrationDate|QuantitySold|QuantityAvailable|      Free shipping?|Store ratings|
[2023-01-26T18:11:50.842+0000] {spark_submit.py:495} INFO - +-------------+--------------------+--------+-------------+--------------------+-------------------+------------+-----------------+--------------------+-------------+
[2023-01-26T18:11:50.842+0000] {spark_submit.py:495} INFO - |MLA1221192372|Disco Duro Extern...| 26299.0|          new|    OVERHARD DIGITAL|2015-01-08 12:19:41|         250|              100|Your shipment arr...|          4.5|
[2023-01-26T18:11:50.842+0000] {spark_submit.py:495} INFO - |MLA1157512489|Placa De Video Nv...|124999.0|          new|      PCREGISTRADAOK|2020-11-02 13:17:33|          50|                1|Your shipment arr...|          4.5|
[2023-01-26T18:11:50.842+0000] {spark_submit.py:495} INFO - |MLA1115295971|Procesador Amd Ry...| 93999.0|          new|      PCREGISTRADAOK|2020-11-02 13:17:33|          25|                1|Your shipment arr...|          4.5|
[2023-01-26T18:11:50.842+0000] {spark_submit.py:495} INFO - |MLA1160198557|Memoria Ram Fury ...| 26699.0|          new|    OVERHARD DIGITAL|2015-01-08 12:19:41|         500|               50|Your shipment arr...|          4.5|
[2023-01-26T18:11:50.842+0000] {spark_submit.py:495} INFO - |MLA1148481665|Microprocesador P...| 58899.0|          new|            MALL WEB|2016-05-26 20:28:35|         150|                1|Your shipment arr...|          4.5|
[2023-01-26T18:11:50.842+0000] {spark_submit.py:495} INFO - |MLA1133481354|Disco Sólido Inte...| 13799.0|          new|           DALECLÍCK|2015-03-16 12:07:23|         500|               50|You have to pay t...|          4.5|
[2023-01-26T18:11:50.842+0000] {spark_submit.py:495} INFO - | MLA923637669|Placa De Video Ge...| 28699.0|          new|         INDIGOTRADE|2018-01-05 19:21:16|         500|               50|Your shipment arr...|          4.5|
[2023-01-26T18:11:50.842+0000] {spark_submit.py:495} INFO - |MLA1139583771|Disco Sólido Inte...|  8339.0|          new|DIAMONDSYSTEMPADUAML|2015-07-01 12:32:22|         250|              500|You have to pay t...|          4.5|
[2023-01-26T18:11:50.842+0000] {spark_submit.py:495} INFO - | MLA882277177|Capturadora Video...|  2699.0|          new|         BSK-IMPORTS|2019-12-30 23:07:04|         500|               50|You have to pay t...|          3.0|
[2023-01-26T18:11:50.842+0000] {spark_submit.py:495} INFO - |MLA1283127595|Procesador Intel ...| 51379.0|          new|      PCREGISTRADAOK|2020-11-02 13:17:33|           1|                1|Your shipment arr...|          4.5|
[2023-01-26T18:11:50.842+0000] {spark_submit.py:495} INFO - |MLA1119996182|Disco Sólido Inte...| 25499.0|          new|            MALL WEB|2016-05-26 20:28:35|         500|                1|Your shipment arr...|          4.5|
[2023-01-26T18:11:50.842+0000] {spark_submit.py:495} INFO - |MLA1183925519|Placa De Video Nv...|299999.0|          new|ESTUDIOSMPSRLESTU...|2020-11-29 18:36:32|          50|                1|Your shipment arr...|          4.5|
[2023-01-26T18:11:50.843+0000] {spark_submit.py:495} INFO - | MLA844489972|Disco Duro Intern...| 11599.0|          new|           DALECLÍCK|2015-03-16 12:07:23|        5000|              500|You have to pay t...|          5.0|
[2023-01-26T18:11:50.843+0000] {spark_submit.py:495} INFO - | MLA833246699|Disco Sólido Inte...| 26999.0|          new|            MALL WEB|2016-05-26 20:28:35|         500|                1|Your shipment arr...|          4.5|
[2023-01-26T18:11:50.843+0000] {spark_submit.py:495} INFO - |MLA1314590445|Disco Sólido Inte...|  7999.0|          new|       ELECTRO-SHOWS|2010-03-26 01:08:16|         250|              250|You have to pay t...|          4.5|
[2023-01-26T18:11:50.843+0000] {spark_submit.py:495} INFO - |MLA1163233412|Placa De Video Nv...| 34659.0|          new|      PCREGISTRADAOK|2020-11-02 13:17:33|           1|                1|Your shipment arr...|          4.5|
[2023-01-26T18:11:50.843+0000] {spark_submit.py:495} INFO - |MLA1302136118|Disco Duro Intern...| 18271.0|          new|DIAMONDSYSTEMSANJ...|2016-04-08 17:21:06|         100|               50|Your shipment arr...|          4.5|
[2023-01-26T18:11:50.843+0000] {spark_submit.py:495} INFO - |MLA1157644025|Disco Duro Extern...| 38790.0|          new|   ELECTRONIC-MARKET|2011-09-15 18:30:10|          25|               50|Your shipment arr...|          4.5|
[2023-01-26T18:11:50.843+0000] {spark_submit.py:495} INFO - | MLA902808321|Procesador Gamer ...| 23699.0|          new|            MALL WEB|2016-05-26 20:28:35|         500|                1|Your shipment arr...|          4.5|
[2023-01-26T18:11:50.843+0000] {spark_submit.py:495} INFO - |MLA1272836523|Disco Duro Intern...| 16999.0|          new|        TIENDA DELTA|2011-09-15 21:48:50|         150|                1|Your shipment arr...|          4.5|
[2023-01-26T18:11:50.843+0000] {spark_submit.py:495} INFO - +-------------+--------------------+--------+-------------+--------------------+-------------------+------------+-----------------+--------------------+-------------+
[2023-01-26T18:11:50.843+0000] {spark_submit.py:495} INFO - only showing top 20 rows
[2023-01-26T18:11:50.843+0000] {spark_submit.py:495} INFO - 
[2023-01-26T18:12:18.778+0000] {spark_submit.py:495} INFO - 23/01/26 18:12:18 ERROR Executor: Exception in task 0.0 in stage 1.0 (TID 1)
[2023-01-26T18:12:18.778+0000] {spark_submit.py:495} INFO - com.mongodb.MongoTimeoutException: Timed out after 30000 ms while waiting to connect. Client view of cluster state is {type=UNKNOWN, servers=[{address=172.22.0.2:27017, type=UNKNOWN, state=CONNECTING, exception={com.mongodb.MongoSocketOpenException: Exception opening socket}, caused by {java.net.SocketTimeoutException: connect timed out}}]
[2023-01-26T18:12:18.779+0000] {spark_submit.py:495} INFO - at com.mongodb.internal.connection.BaseCluster.getDescription(BaseCluster.java:177)
[2023-01-26T18:12:18.779+0000] {spark_submit.py:495} INFO - at com.mongodb.internal.connection.SingleServerCluster.getDescription(SingleServerCluster.java:41)
[2023-01-26T18:12:18.779+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate.getConnectedClusterDescription(MongoClientDelegate.java:147)
[2023-01-26T18:12:18.780+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate.createClientSession(MongoClientDelegate.java:98)
[2023-01-26T18:12:18.780+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.getClientSession(MongoClientDelegate.java:278)
[2023-01-26T18:12:18.780+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:202)
[2023-01-26T18:12:18.780+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:441)
[2023-01-26T18:12:18.780+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:421)
[2023-01-26T18:12:18.781+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$4(MongoSpark.scala:138)
[2023-01-26T18:12:18.781+0000] {spark_submit.py:495} INFO - at scala.collection.Iterator.foreach(Iterator.scala:943)
[2023-01-26T18:12:18.781+0000] {spark_submit.py:495} INFO - at scala.collection.Iterator.foreach$(Iterator.scala:943)
[2023-01-26T18:12:18.781+0000] {spark_submit.py:495} INFO - at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
[2023-01-26T18:12:18.781+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$3(MongoSpark.scala:122)
[2023-01-26T18:12:18.781+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$3$adapted(MongoSpark.scala:121)
[2023-01-26T18:12:18.782+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.$anonfun$withCollectionDo$1(MongoConnector.scala:186)
[2023-01-26T18:12:18.782+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.$anonfun$withDatabaseDo$1(MongoConnector.scala:171)
[2023-01-26T18:12:18.782+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withMongoClientDo(MongoConnector.scala:154)
[2023-01-26T18:12:18.782+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withDatabaseDo(MongoConnector.scala:171)
[2023-01-26T18:12:18.782+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withCollectionDo(MongoConnector.scala:184)
[2023-01-26T18:12:18.782+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$2(MongoSpark.scala:121)
[2023-01-26T18:12:18.783+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$2$adapted(MongoSpark.scala:120)
[2023-01-26T18:12:18.783+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1011)
[2023-01-26T18:12:18.783+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1011)
[2023-01-26T18:12:18.783+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2268)
[2023-01-26T18:12:18.783+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
[2023-01-26T18:12:18.783+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.Task.run(Task.scala:136)
[2023-01-26T18:12:18.784+0000] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)
[2023-01-26T18:12:18.784+0000] {spark_submit.py:495} INFO - at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)
[2023-01-26T18:12:18.784+0000] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)
[2023-01-26T18:12:18.784+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[2023-01-26T18:12:18.784+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[2023-01-26T18:12:18.785+0000] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2023-01-26T18:12:18.803+0000] {spark_submit.py:495} INFO - 23/01/26 18:12:18 ERROR TaskSetManager: Task 0 in stage 1.0 failed 1 times; aborting job
[2023-01-26T18:12:18.933+0000] {spark_submit.py:495} INFO - [[34m2023-01-26 18:12:18,932[0m] {[34mconsumerSpark.py:[0m37} ERROR[0m - Error al escribir en Mongo: An error occurred while calling o124.save.
[2023-01-26T18:12:18.933+0000] {spark_submit.py:495} INFO - : org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1.0 (TID 1) (f1cbb5df32e6 executor driver): com.mongodb.MongoTimeoutException: Timed out after 30000 ms while waiting to connect. Client view of cluster state is {type=UNKNOWN, servers=[{address=172.22.0.2:27017, type=UNKNOWN, state=CONNECTING, exception={com.mongodb.MongoSocketOpenException: Exception opening socket}, caused by {java.net.SocketTimeoutException: connect timed out}}]
[2023-01-26T18:12:18.933+0000] {spark_submit.py:495} INFO - at com.mongodb.internal.connection.BaseCluster.getDescription(BaseCluster.java:177)
[2023-01-26T18:12:18.933+0000] {spark_submit.py:495} INFO - at com.mongodb.internal.connection.SingleServerCluster.getDescription(SingleServerCluster.java:41)
[2023-01-26T18:12:18.933+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate.getConnectedClusterDescription(MongoClientDelegate.java:147)
[2023-01-26T18:12:18.933+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate.createClientSession(MongoClientDelegate.java:98)
[2023-01-26T18:12:18.933+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.getClientSession(MongoClientDelegate.java:278)
[2023-01-26T18:12:18.933+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:202)
[2023-01-26T18:12:18.933+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:441)
[2023-01-26T18:12:18.933+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:421)
[2023-01-26T18:12:18.933+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$4(MongoSpark.scala:138)
[2023-01-26T18:12:18.933+0000] {spark_submit.py:495} INFO - at scala.collection.Iterator.foreach(Iterator.scala:943)
[2023-01-26T18:12:18.934+0000] {spark_submit.py:495} INFO - at scala.collection.Iterator.foreach$(Iterator.scala:943)
[2023-01-26T18:12:18.934+0000] {spark_submit.py:495} INFO - at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
[2023-01-26T18:12:18.934+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$3(MongoSpark.scala:122)
[2023-01-26T18:12:18.934+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$3$adapted(MongoSpark.scala:121)
[2023-01-26T18:12:18.934+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.$anonfun$withCollectionDo$1(MongoConnector.scala:186)
[2023-01-26T18:12:18.934+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.$anonfun$withDatabaseDo$1(MongoConnector.scala:171)
[2023-01-26T18:12:18.934+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withMongoClientDo(MongoConnector.scala:154)
[2023-01-26T18:12:18.934+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withDatabaseDo(MongoConnector.scala:171)
[2023-01-26T18:12:18.934+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withCollectionDo(MongoConnector.scala:184)
[2023-01-26T18:12:18.934+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$2(MongoSpark.scala:121)
[2023-01-26T18:12:18.934+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$2$adapted(MongoSpark.scala:120)
[2023-01-26T18:12:18.934+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1011)
[2023-01-26T18:12:18.934+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1011)
[2023-01-26T18:12:18.934+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2268)
[2023-01-26T18:12:18.934+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
[2023-01-26T18:12:18.934+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.Task.run(Task.scala:136)
[2023-01-26T18:12:18.935+0000] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)
[2023-01-26T18:12:18.935+0000] {spark_submit.py:495} INFO - at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)
[2023-01-26T18:12:18.935+0000] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)
[2023-01-26T18:12:18.935+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[2023-01-26T18:12:18.935+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[2023-01-26T18:12:18.935+0000] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2023-01-26T18:12:18.935+0000] {spark_submit.py:495} INFO - 
[2023-01-26T18:12:18.935+0000] {spark_submit.py:495} INFO - Driver stacktrace:
[2023-01-26T18:12:18.935+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)
[2023-01-26T18:12:18.935+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)
[2023-01-26T18:12:18.935+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)
[2023-01-26T18:12:18.935+0000] {spark_submit.py:495} INFO - at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
[2023-01-26T18:12:18.935+0000] {spark_submit.py:495} INFO - at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
[2023-01-26T18:12:18.935+0000] {spark_submit.py:495} INFO - at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
[2023-01-26T18:12:18.936+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)
[2023-01-26T18:12:18.936+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)
[2023-01-26T18:12:18.936+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)
[2023-01-26T18:12:18.936+0000] {spark_submit.py:495} INFO - at scala.Option.foreach(Option.scala:407)
[2023-01-26T18:12:18.936+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)
[2023-01-26T18:12:18.936+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)
[2023-01-26T18:12:18.936+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)
[2023-01-26T18:12:18.936+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)
[2023-01-26T18:12:18.936+0000] {spark_submit.py:495} INFO - at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
[2023-01-26T18:12:18.936+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)
[2023-01-26T18:12:18.936+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.runJob(SparkContext.scala:2228)
[2023-01-26T18:12:18.936+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.runJob(SparkContext.scala:2249)
[2023-01-26T18:12:18.936+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.runJob(SparkContext.scala:2268)
[2023-01-26T18:12:18.937+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.runJob(SparkContext.scala:2293)
[2023-01-26T18:12:18.937+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$1(RDD.scala:1011)
[2023-01-26T18:12:18.937+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
[2023-01-26T18:12:18.937+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
[2023-01-26T18:12:18.937+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.withScope(RDD.scala:406)
[2023-01-26T18:12:18.937+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.foreachPartition(RDD.scala:1009)
[2023-01-26T18:12:18.937+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.save(MongoSpark.scala:120)
[2023-01-26T18:12:18.937+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.save(MongoSpark.scala:169)
[2023-01-26T18:12:18.937+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.sql.DefaultSource.createRelation(DefaultSource.scala:70)
[2023-01-26T18:12:18.937+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:47)
[2023-01-26T18:12:18.937+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)
[2023-01-26T18:12:18.937+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)
[2023-01-26T18:12:18.937+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)
[2023-01-26T18:12:18.937+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)
[2023-01-26T18:12:18.937+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)
[2023-01-26T18:12:18.938+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)
[2023-01-26T18:12:18.938+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)
[2023-01-26T18:12:18.938+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
[2023-01-26T18:12:18.938+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
[2023-01-26T18:12:18.938+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)
[2023-01-26T18:12:18.938+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)
[2023-01-26T18:12:18.938+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:584)
[2023-01-26T18:12:18.938+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:176)
[2023-01-26T18:12:18.938+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:584)
[2023-01-26T18:12:18.938+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)
[2023-01-26T18:12:18.938+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
[2023-01-26T18:12:18.938+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
[2023-01-26T18:12:18.938+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
[2023-01-26T18:12:18.938+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
[2023-01-26T18:12:18.938+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:560)
[2023-01-26T18:12:18.939+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)
[2023-01-26T18:12:18.939+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)
[2023-01-26T18:12:18.939+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)
[2023-01-26T18:12:18.939+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:116)
[2023-01-26T18:12:18.939+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:860)
[2023-01-26T18:12:18.939+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:390)
[2023-01-26T18:12:18.939+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:363)
[2023-01-26T18:12:18.939+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:247)
[2023-01-26T18:12:18.939+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
[2023-01-26T18:12:18.939+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
[2023-01-26T18:12:18.939+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
[2023-01-26T18:12:18.939+0000] {spark_submit.py:495} INFO - at java.base/java.lang.reflect.Method.invoke(Method.java:566)
[2023-01-26T18:12:18.939+0000] {spark_submit.py:495} INFO - at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
[2023-01-26T18:12:18.939+0000] {spark_submit.py:495} INFO - at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
[2023-01-26T18:12:18.939+0000] {spark_submit.py:495} INFO - at py4j.Gateway.invoke(Gateway.java:282)
[2023-01-26T18:12:18.940+0000] {spark_submit.py:495} INFO - at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
[2023-01-26T18:12:18.940+0000] {spark_submit.py:495} INFO - at py4j.commands.CallCommand.execute(CallCommand.java:79)
[2023-01-26T18:12:18.940+0000] {spark_submit.py:495} INFO - at py4j.ClientServerConnection.sendCommand(ClientServerConnection.java:244)
[2023-01-26T18:12:18.940+0000] {spark_submit.py:495} INFO - at py4j.CallbackClient.sendCommand(CallbackClient.java:384)
[2023-01-26T18:12:18.940+0000] {spark_submit.py:495} INFO - at py4j.CallbackClient.sendCommand(CallbackClient.java:356)
[2023-01-26T18:12:18.940+0000] {spark_submit.py:495} INFO - at py4j.reflection.PythonProxyHandler.invoke(PythonProxyHandler.java:106)
[2023-01-26T18:12:18.951+0000] {spark_submit.py:495} INFO - at com.sun.proxy.$Proxy32.call(Unknown Source)
[2023-01-26T18:12:18.951+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1(ForeachBatchSink.scala:51)
[2023-01-26T18:12:18.952+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1$adapted(ForeachBatchSink.scala:51)
[2023-01-26T18:12:18.952+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.sources.ForeachBatchSink.addBatch(ForeachBatchSink.scala:32)
[2023-01-26T18:12:18.952+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:666)
[2023-01-26T18:12:18.952+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)
[2023-01-26T18:12:18.952+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)
[2023-01-26T18:12:18.952+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)
[2023-01-26T18:12:18.952+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
[2023-01-26T18:12:18.952+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
[2023-01-26T18:12:18.952+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:664)
[2023-01-26T18:12:18.952+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:375)
[2023-01-26T18:12:18.952+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:373)
[2023-01-26T18:12:18.952+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:68)
[2023-01-26T18:12:18.952+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:664)
[2023-01-26T18:12:18.953+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:256)
[2023-01-26T18:12:18.953+0000] {spark_submit.py:495} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2023-01-26T18:12:18.953+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:375)
[2023-01-26T18:12:18.953+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:373)
[2023-01-26T18:12:18.953+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:68)
[2023-01-26T18:12:18.953+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:219)
[2023-01-26T18:12:18.953+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)
[2023-01-26T18:12:18.953+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:213)
[2023-01-26T18:12:18.953+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:307)
[2023-01-26T18:12:18.953+0000] {spark_submit.py:495} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2023-01-26T18:12:18.953+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
[2023-01-26T18:12:18.953+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:285)
[2023-01-26T18:12:18.953+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:208)
[2023-01-26T18:12:18.953+0000] {spark_submit.py:495} INFO - Caused by: com.mongodb.MongoTimeoutException: Timed out after 30000 ms while waiting to connect. Client view of cluster state is {type=UNKNOWN, servers=[{address=172.22.0.2:27017, type=UNKNOWN, state=CONNECTING, exception={com.mongodb.MongoSocketOpenException: Exception opening socket}, caused by {java.net.SocketTimeoutException: connect timed out}}]
[2023-01-26T18:12:18.953+0000] {spark_submit.py:495} INFO - at com.mongodb.internal.connection.BaseCluster.getDescription(BaseCluster.java:177)
[2023-01-26T18:12:18.954+0000] {spark_submit.py:495} INFO - at com.mongodb.internal.connection.SingleServerCluster.getDescription(SingleServerCluster.java:41)
[2023-01-26T18:12:18.954+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate.getConnectedClusterDescription(MongoClientDelegate.java:147)
[2023-01-26T18:12:18.954+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate.createClientSession(MongoClientDelegate.java:98)
[2023-01-26T18:12:18.954+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.getClientSession(MongoClientDelegate.java:278)
[2023-01-26T18:12:18.954+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:202)
[2023-01-26T18:12:18.954+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:441)
[2023-01-26T18:12:18.954+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:421)
[2023-01-26T18:12:18.954+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$4(MongoSpark.scala:138)
[2023-01-26T18:12:18.954+0000] {spark_submit.py:495} INFO - at scala.collection.Iterator.foreach(Iterator.scala:943)
[2023-01-26T18:12:18.954+0000] {spark_submit.py:495} INFO - at scala.collection.Iterator.foreach$(Iterator.scala:943)
[2023-01-26T18:12:18.954+0000] {spark_submit.py:495} INFO - at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
[2023-01-26T18:12:18.958+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$3(MongoSpark.scala:122)
[2023-01-26T18:12:18.958+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$3$adapted(MongoSpark.scala:121)
[2023-01-26T18:12:18.958+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.$anonfun$withCollectionDo$1(MongoConnector.scala:186)
[2023-01-26T18:12:18.958+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.$anonfun$withDatabaseDo$1(MongoConnector.scala:171)
[2023-01-26T18:12:18.958+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withMongoClientDo(MongoConnector.scala:154)
[2023-01-26T18:12:18.959+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withDatabaseDo(MongoConnector.scala:171)
[2023-01-26T18:12:18.959+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withCollectionDo(MongoConnector.scala:184)
[2023-01-26T18:12:18.959+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$2(MongoSpark.scala:121)
[2023-01-26T18:12:18.959+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$2$adapted(MongoSpark.scala:120)
[2023-01-26T18:12:18.959+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1011)
[2023-01-26T18:12:18.959+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1011)
[2023-01-26T18:12:18.959+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2268)
[2023-01-26T18:12:18.959+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
[2023-01-26T18:12:18.959+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.Task.run(Task.scala:136)
[2023-01-26T18:12:18.959+0000] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)
[2023-01-26T18:12:18.959+0000] {spark_submit.py:495} INFO - at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)
[2023-01-26T18:12:18.959+0000] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)
[2023-01-26T18:12:18.959+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[2023-01-26T18:12:18.960+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[2023-01-26T18:12:18.960+0000] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2023-01-26T18:12:18.960+0000] {spark_submit.py:495} INFO - [0m
[2023-01-26T18:12:19.101+0000] {spark_submit.py:495} INFO - [[34m2023-01-26 18:12:19,101[0m] {[34mclientserver.py:[0m577} INFO[0m - Received command c on object id p0[0m
[2023-01-26T18:12:49.313+0000] {spark_submit.py:495} INFO - 23/01/26 18:12:49 ERROR Executor: Exception in task 0.0 in stage 3.0 (TID 3)
[2023-01-26T18:12:49.314+0000] {spark_submit.py:495} INFO - com.mongodb.MongoTimeoutException: Timed out after 30000 ms while waiting to connect. Client view of cluster state is {type=UNKNOWN, servers=[{address=172.22.0.2:27017, type=UNKNOWN, state=CONNECTING, exception={com.mongodb.MongoSocketOpenException: Exception opening socket}, caused by {java.net.SocketTimeoutException: connect timed out}}]
[2023-01-26T18:12:49.314+0000] {spark_submit.py:495} INFO - at com.mongodb.internal.connection.BaseCluster.getDescription(BaseCluster.java:177)
[2023-01-26T18:12:49.315+0000] {spark_submit.py:495} INFO - at com.mongodb.internal.connection.SingleServerCluster.getDescription(SingleServerCluster.java:41)
[2023-01-26T18:12:49.315+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate.getConnectedClusterDescription(MongoClientDelegate.java:147)
[2023-01-26T18:12:49.315+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate.createClientSession(MongoClientDelegate.java:98)
[2023-01-26T18:12:49.316+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.getClientSession(MongoClientDelegate.java:278)
[2023-01-26T18:12:49.316+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:202)
[2023-01-26T18:12:49.316+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:441)
[2023-01-26T18:12:49.317+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:421)
[2023-01-26T18:12:49.317+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$4(MongoSpark.scala:138)
[2023-01-26T18:12:49.318+0000] {spark_submit.py:495} INFO - at scala.collection.Iterator.foreach(Iterator.scala:943)
[2023-01-26T18:12:49.318+0000] {spark_submit.py:495} INFO - at scala.collection.Iterator.foreach$(Iterator.scala:943)
[2023-01-26T18:12:49.318+0000] {spark_submit.py:495} INFO - at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
[2023-01-26T18:12:49.318+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$3(MongoSpark.scala:122)
[2023-01-26T18:12:49.319+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$3$adapted(MongoSpark.scala:121)
[2023-01-26T18:12:49.319+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.$anonfun$withCollectionDo$1(MongoConnector.scala:186)
[2023-01-26T18:12:49.319+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.$anonfun$withDatabaseDo$1(MongoConnector.scala:171)
[2023-01-26T18:12:49.320+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withMongoClientDo(MongoConnector.scala:154)
[2023-01-26T18:12:49.320+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withDatabaseDo(MongoConnector.scala:171)
[2023-01-26T18:12:49.321+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withCollectionDo(MongoConnector.scala:184)
[2023-01-26T18:12:49.321+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$2(MongoSpark.scala:121)
[2023-01-26T18:12:49.321+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$2$adapted(MongoSpark.scala:120)
[2023-01-26T18:12:49.322+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1011)
[2023-01-26T18:12:49.323+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1011)
[2023-01-26T18:12:49.324+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2268)
[2023-01-26T18:12:49.324+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
[2023-01-26T18:12:49.325+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.Task.run(Task.scala:136)
[2023-01-26T18:12:49.325+0000] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)
[2023-01-26T18:12:49.326+0000] {spark_submit.py:495} INFO - at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)
[2023-01-26T18:12:49.326+0000] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)
[2023-01-26T18:12:49.327+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[2023-01-26T18:12:49.327+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[2023-01-26T18:12:49.328+0000] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2023-01-26T18:12:49.328+0000] {spark_submit.py:495} INFO - 23/01/26 18:12:49 ERROR TaskSetManager: Task 0 in stage 3.0 failed 1 times; aborting job
[2023-01-26T18:12:49.488+0000] {spark_submit.py:495} INFO - [[34m2023-01-26 18:12:49,488[0m] {[34mconsumerSpark.py:[0m37} ERROR[0m - Error al escribir en Mongo: An error occurred while calling o133.save.
[2023-01-26T18:12:49.489+0000] {spark_submit.py:495} INFO - : org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 3.0 failed 1 times, most recent failure: Lost task 0.0 in stage 3.0 (TID 3) (f1cbb5df32e6 executor driver): com.mongodb.MongoTimeoutException: Timed out after 30000 ms while waiting to connect. Client view of cluster state is {type=UNKNOWN, servers=[{address=172.22.0.2:27017, type=UNKNOWN, state=CONNECTING, exception={com.mongodb.MongoSocketOpenException: Exception opening socket}, caused by {java.net.SocketTimeoutException: connect timed out}}]
[2023-01-26T18:12:49.489+0000] {spark_submit.py:495} INFO - at com.mongodb.internal.connection.BaseCluster.getDescription(BaseCluster.java:177)
[2023-01-26T18:12:49.489+0000] {spark_submit.py:495} INFO - at com.mongodb.internal.connection.SingleServerCluster.getDescription(SingleServerCluster.java:41)
[2023-01-26T18:12:49.489+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate.getConnectedClusterDescription(MongoClientDelegate.java:147)
[2023-01-26T18:12:49.489+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate.createClientSession(MongoClientDelegate.java:98)
[2023-01-26T18:12:49.489+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.getClientSession(MongoClientDelegate.java:278)
[2023-01-26T18:12:49.489+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:202)
[2023-01-26T18:12:49.489+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:441)
[2023-01-26T18:12:49.489+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:421)
[2023-01-26T18:12:49.489+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$4(MongoSpark.scala:138)
[2023-01-26T18:12:49.490+0000] {spark_submit.py:495} INFO - at scala.collection.Iterator.foreach(Iterator.scala:943)
[2023-01-26T18:12:49.490+0000] {spark_submit.py:495} INFO - at scala.collection.Iterator.foreach$(Iterator.scala:943)
[2023-01-26T18:12:49.490+0000] {spark_submit.py:495} INFO - at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
[2023-01-26T18:12:49.490+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$3(MongoSpark.scala:122)
[2023-01-26T18:12:49.490+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$3$adapted(MongoSpark.scala:121)
[2023-01-26T18:12:49.490+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.$anonfun$withCollectionDo$1(MongoConnector.scala:186)
[2023-01-26T18:12:49.490+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.$anonfun$withDatabaseDo$1(MongoConnector.scala:171)
[2023-01-26T18:12:49.490+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withMongoClientDo(MongoConnector.scala:154)
[2023-01-26T18:12:49.490+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withDatabaseDo(MongoConnector.scala:171)
[2023-01-26T18:12:49.490+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withCollectionDo(MongoConnector.scala:184)
[2023-01-26T18:12:49.490+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$2(MongoSpark.scala:121)
[2023-01-26T18:12:49.490+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$2$adapted(MongoSpark.scala:120)
[2023-01-26T18:12:49.490+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1011)
[2023-01-26T18:12:49.490+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1011)
[2023-01-26T18:12:49.491+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2268)
[2023-01-26T18:12:49.491+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
[2023-01-26T18:12:49.491+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.Task.run(Task.scala:136)
[2023-01-26T18:12:49.491+0000] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)
[2023-01-26T18:12:49.491+0000] {spark_submit.py:495} INFO - at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)
[2023-01-26T18:12:49.491+0000] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)
[2023-01-26T18:12:49.491+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[2023-01-26T18:12:49.491+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[2023-01-26T18:12:49.491+0000] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2023-01-26T18:12:49.491+0000] {spark_submit.py:495} INFO - 
[2023-01-26T18:12:49.491+0000] {spark_submit.py:495} INFO - Driver stacktrace:
[2023-01-26T18:12:49.491+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)
[2023-01-26T18:12:49.491+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)
[2023-01-26T18:12:49.491+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)
[2023-01-26T18:12:49.492+0000] {spark_submit.py:495} INFO - at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
[2023-01-26T18:12:49.492+0000] {spark_submit.py:495} INFO - at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
[2023-01-26T18:12:49.492+0000] {spark_submit.py:495} INFO - at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
[2023-01-26T18:12:49.492+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)
[2023-01-26T18:12:49.492+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)
[2023-01-26T18:12:49.492+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)
[2023-01-26T18:12:49.492+0000] {spark_submit.py:495} INFO - at scala.Option.foreach(Option.scala:407)
[2023-01-26T18:12:49.492+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)
[2023-01-26T18:12:49.492+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)
[2023-01-26T18:12:49.492+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)
[2023-01-26T18:12:49.492+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)
[2023-01-26T18:12:49.492+0000] {spark_submit.py:495} INFO - at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
[2023-01-26T18:12:49.492+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)
[2023-01-26T18:12:49.493+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.runJob(SparkContext.scala:2228)
[2023-01-26T18:12:49.493+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.runJob(SparkContext.scala:2249)
[2023-01-26T18:12:49.493+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.runJob(SparkContext.scala:2268)
[2023-01-26T18:12:49.493+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.runJob(SparkContext.scala:2293)
[2023-01-26T18:12:49.494+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$1(RDD.scala:1011)
[2023-01-26T18:12:49.494+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
[2023-01-26T18:12:49.495+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
[2023-01-26T18:12:49.495+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.withScope(RDD.scala:406)
[2023-01-26T18:12:49.495+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.foreachPartition(RDD.scala:1009)
[2023-01-26T18:12:49.495+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.save(MongoSpark.scala:120)
[2023-01-26T18:12:49.495+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.save(MongoSpark.scala:169)
[2023-01-26T18:12:49.495+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.sql.DefaultSource.createRelation(DefaultSource.scala:70)
[2023-01-26T18:12:49.496+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:47)
[2023-01-26T18:12:49.496+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)
[2023-01-26T18:12:49.496+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)
[2023-01-26T18:12:49.496+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)
[2023-01-26T18:12:49.496+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)
[2023-01-26T18:12:49.496+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)
[2023-01-26T18:12:49.503+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)
[2023-01-26T18:12:49.504+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)
[2023-01-26T18:12:49.504+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
[2023-01-26T18:12:49.504+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
[2023-01-26T18:12:49.504+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)
[2023-01-26T18:12:49.504+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)
[2023-01-26T18:12:49.504+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:584)
[2023-01-26T18:12:49.504+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:176)
[2023-01-26T18:12:49.504+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:584)
[2023-01-26T18:12:49.505+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)
[2023-01-26T18:12:49.505+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
[2023-01-26T18:12:49.505+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
[2023-01-26T18:12:49.505+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
[2023-01-26T18:12:49.505+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
[2023-01-26T18:12:49.505+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:560)
[2023-01-26T18:12:49.506+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)
[2023-01-26T18:12:49.507+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)
[2023-01-26T18:12:49.507+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)
[2023-01-26T18:12:49.507+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:116)
[2023-01-26T18:12:49.507+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:860)
[2023-01-26T18:12:49.507+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:390)
[2023-01-26T18:12:49.507+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:363)
[2023-01-26T18:12:49.507+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:247)
[2023-01-26T18:12:49.507+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
[2023-01-26T18:12:49.507+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
[2023-01-26T18:12:49.507+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
[2023-01-26T18:12:49.507+0000] {spark_submit.py:495} INFO - at java.base/java.lang.reflect.Method.invoke(Method.java:566)
[2023-01-26T18:12:49.507+0000] {spark_submit.py:495} INFO - at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
[2023-01-26T18:12:49.507+0000] {spark_submit.py:495} INFO - at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
[2023-01-26T18:12:49.508+0000] {spark_submit.py:495} INFO - at py4j.Gateway.invoke(Gateway.java:282)
[2023-01-26T18:12:49.508+0000] {spark_submit.py:495} INFO - at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
[2023-01-26T18:12:49.508+0000] {spark_submit.py:495} INFO - at py4j.commands.CallCommand.execute(CallCommand.java:79)
[2023-01-26T18:12:49.508+0000] {spark_submit.py:495} INFO - at py4j.ClientServerConnection.sendCommand(ClientServerConnection.java:244)
[2023-01-26T18:12:49.508+0000] {spark_submit.py:495} INFO - at py4j.CallbackClient.sendCommand(CallbackClient.java:384)
[2023-01-26T18:12:49.508+0000] {spark_submit.py:495} INFO - at py4j.CallbackClient.sendCommand(CallbackClient.java:356)
[2023-01-26T18:12:49.508+0000] {spark_submit.py:495} INFO - at py4j.reflection.PythonProxyHandler.invoke(PythonProxyHandler.java:106)
[2023-01-26T18:12:49.508+0000] {spark_submit.py:495} INFO - at com.sun.proxy.$Proxy32.call(Unknown Source)
[2023-01-26T18:12:49.508+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1(ForeachBatchSink.scala:51)
[2023-01-26T18:12:49.508+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1$adapted(ForeachBatchSink.scala:51)
[2023-01-26T18:12:49.508+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.sources.ForeachBatchSink.addBatch(ForeachBatchSink.scala:32)
[2023-01-26T18:12:49.508+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:666)
[2023-01-26T18:12:49.508+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)
[2023-01-26T18:12:49.508+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)
[2023-01-26T18:12:49.509+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)
[2023-01-26T18:12:49.509+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
[2023-01-26T18:12:49.509+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
[2023-01-26T18:12:49.509+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:664)
[2023-01-26T18:12:49.509+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:375)
[2023-01-26T18:12:49.509+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:373)
[2023-01-26T18:12:49.509+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:68)
[2023-01-26T18:12:49.509+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:664)
[2023-01-26T18:12:49.509+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:256)
[2023-01-26T18:12:49.509+0000] {spark_submit.py:495} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2023-01-26T18:12:49.509+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:375)
[2023-01-26T18:12:49.509+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:373)
[2023-01-26T18:12:49.509+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:68)
[2023-01-26T18:12:49.509+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:219)
[2023-01-26T18:12:49.509+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)
[2023-01-26T18:12:49.510+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:213)
[2023-01-26T18:12:49.510+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:307)
[2023-01-26T18:12:49.510+0000] {spark_submit.py:495} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2023-01-26T18:12:49.510+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
[2023-01-26T18:12:49.510+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:285)
[2023-01-26T18:12:49.510+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:208)
[2023-01-26T18:12:49.510+0000] {spark_submit.py:495} INFO - Caused by: com.mongodb.MongoTimeoutException: Timed out after 30000 ms while waiting to connect. Client view of cluster state is {type=UNKNOWN, servers=[{address=172.22.0.2:27017, type=UNKNOWN, state=CONNECTING, exception={com.mongodb.MongoSocketOpenException: Exception opening socket}, caused by {java.net.SocketTimeoutException: connect timed out}}]
[2023-01-26T18:12:49.510+0000] {spark_submit.py:495} INFO - at com.mongodb.internal.connection.BaseCluster.getDescription(BaseCluster.java:177)
[2023-01-26T18:12:49.510+0000] {spark_submit.py:495} INFO - at com.mongodb.internal.connection.SingleServerCluster.getDescription(SingleServerCluster.java:41)
[2023-01-26T18:12:49.510+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate.getConnectedClusterDescription(MongoClientDelegate.java:147)
[2023-01-26T18:12:49.510+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate.createClientSession(MongoClientDelegate.java:98)
[2023-01-26T18:12:49.510+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.getClientSession(MongoClientDelegate.java:278)
[2023-01-26T18:12:49.510+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:202)
[2023-01-26T18:12:49.510+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:441)
[2023-01-26T18:12:49.510+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:421)
[2023-01-26T18:12:49.511+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$4(MongoSpark.scala:138)
[2023-01-26T18:12:49.511+0000] {spark_submit.py:495} INFO - at scala.collection.Iterator.foreach(Iterator.scala:943)
[2023-01-26T18:12:49.511+0000] {spark_submit.py:495} INFO - at scala.collection.Iterator.foreach$(Iterator.scala:943)
[2023-01-26T18:12:49.511+0000] {spark_submit.py:495} INFO - at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
[2023-01-26T18:12:49.511+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$3(MongoSpark.scala:122)
[2023-01-26T18:12:49.511+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$3$adapted(MongoSpark.scala:121)
[2023-01-26T18:12:49.511+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.$anonfun$withCollectionDo$1(MongoConnector.scala:186)
[2023-01-26T18:12:49.511+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.$anonfun$withDatabaseDo$1(MongoConnector.scala:171)
[2023-01-26T18:12:49.511+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withMongoClientDo(MongoConnector.scala:154)
[2023-01-26T18:12:49.511+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withDatabaseDo(MongoConnector.scala:171)
[2023-01-26T18:12:49.517+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withCollectionDo(MongoConnector.scala:184)
[2023-01-26T18:12:49.518+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$2(MongoSpark.scala:121)
[2023-01-26T18:12:49.518+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$2$adapted(MongoSpark.scala:120)
[2023-01-26T18:12:49.518+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1011)
[2023-01-26T18:12:49.518+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1011)
[2023-01-26T18:12:49.518+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2268)
[2023-01-26T18:12:49.518+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
[2023-01-26T18:12:49.518+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.Task.run(Task.scala:136)
[2023-01-26T18:12:49.518+0000] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)
[2023-01-26T18:12:49.518+0000] {spark_submit.py:495} INFO - at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)
[2023-01-26T18:12:49.518+0000] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)
[2023-01-26T18:12:49.518+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[2023-01-26T18:12:49.520+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[2023-01-26T18:12:49.520+0000] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2023-01-26T18:12:49.520+0000] {spark_submit.py:495} INFO - [0m
[2023-01-26T18:15:07.135+0000] {spark_submit.py:495} INFO - [[34m2023-01-26 18:15:07,135[0m] {[34mclientserver.py:[0m577} INFO[0m - Received command c on object id p0[0m
[2023-01-26T18:15:10.967+0000] {spark_submit.py:495} INFO - -------------------------------------------
[2023-01-26T18:15:10.968+0000] {spark_submit.py:495} INFO - Batch: 2
[2023-01-26T18:15:10.968+0000] {spark_submit.py:495} INFO - -------------------------------------------
[2023-01-26T18:15:10.988+0000] {spark_submit.py:495} INFO - +-------------+--------------------+--------+-------------+--------------------+-------------------+------------+-----------------+--------------------+-------------+
[2023-01-26T18:15:10.989+0000] {spark_submit.py:495} INFO - |           id|       nameOfProduct|   price|ProductStatus|          SellerName|   RegistrationDate|QuantitySold|QuantityAvailable|      Free shipping?|Store ratings|
[2023-01-26T18:15:10.989+0000] {spark_submit.py:495} INFO - +-------------+--------------------+--------+-------------+--------------------+-------------------+------------+-----------------+--------------------+-------------+
[2023-01-26T18:15:10.989+0000] {spark_submit.py:495} INFO - |MLA1221192372|Disco Duro Extern...| 26299.0|          new|    OVERHARD DIGITAL|2015-01-08 12:19:41|         250|              100|Your shipment arr...|          4.5|
[2023-01-26T18:15:10.989+0000] {spark_submit.py:495} INFO - |MLA1157512489|Placa De Video Nv...|124999.0|          new|      PCREGISTRADAOK|2020-11-02 13:17:33|          50|                1|Your shipment arr...|          4.5|
[2023-01-26T18:15:10.989+0000] {spark_submit.py:495} INFO - |MLA1115295971|Procesador Amd Ry...| 93999.0|          new|      PCREGISTRADAOK|2020-11-02 13:17:33|          25|                1|Your shipment arr...|          4.5|
[2023-01-26T18:15:10.989+0000] {spark_submit.py:495} INFO - |MLA1160198557|Memoria Ram Fury ...| 26699.0|          new|    OVERHARD DIGITAL|2015-01-08 12:19:41|         500|               50|Your shipment arr...|          4.5|
[2023-01-26T18:15:10.989+0000] {spark_submit.py:495} INFO - |MLA1148481665|Microprocesador P...| 58899.0|          new|            MALL WEB|2016-05-26 20:28:35|         150|                1|Your shipment arr...|          4.5|
[2023-01-26T18:15:10.989+0000] {spark_submit.py:495} INFO - |MLA1133481354|Disco Sólido Inte...| 13799.0|          new|           DALECLÍCK|2015-03-16 12:07:23|         500|               50|You have to pay t...|          4.5|
[2023-01-26T18:15:10.989+0000] {spark_submit.py:495} INFO - | MLA923637669|Placa De Video Ge...| 28699.0|          new|         INDIGOTRADE|2018-01-05 19:21:16|         500|               50|Your shipment arr...|          4.5|
[2023-01-26T18:15:10.989+0000] {spark_submit.py:495} INFO - |MLA1139583771|Disco Sólido Inte...|  8339.0|          new|DIAMONDSYSTEMPADUAML|2015-07-01 12:32:22|         250|              500|You have to pay t...|          4.5|
[2023-01-26T18:15:10.989+0000] {spark_submit.py:495} INFO - | MLA882277177|Capturadora Video...|  2699.0|          new|         BSK-IMPORTS|2019-12-30 23:07:04|         500|               50|You have to pay t...|          3.0|
[2023-01-26T18:15:10.990+0000] {spark_submit.py:495} INFO - |MLA1283127595|Procesador Intel ...| 51379.0|          new|      PCREGISTRADAOK|2020-11-02 13:17:33|           1|                1|Your shipment arr...|          4.5|
[2023-01-26T18:15:10.990+0000] {spark_submit.py:495} INFO - |MLA1119996182|Disco Sólido Inte...| 25499.0|          new|            MALL WEB|2016-05-26 20:28:35|         500|                1|Your shipment arr...|          4.5|
[2023-01-26T18:15:10.990+0000] {spark_submit.py:495} INFO - |MLA1183925519|Placa De Video Nv...|299999.0|          new|ESTUDIOSMPSRLESTU...|2020-11-29 18:36:32|          50|                1|Your shipment arr...|          4.5|
[2023-01-26T18:15:10.990+0000] {spark_submit.py:495} INFO - | MLA844489972|Disco Duro Intern...| 11599.0|          new|           DALECLÍCK|2015-03-16 12:07:23|        5000|              500|You have to pay t...|          5.0|
[2023-01-26T18:15:10.990+0000] {spark_submit.py:495} INFO - | MLA833246699|Disco Sólido Inte...| 26999.0|          new|            MALL WEB|2016-05-26 20:28:35|         500|                1|Your shipment arr...|          4.5|
[2023-01-26T18:15:10.990+0000] {spark_submit.py:495} INFO - |MLA1314590445|Disco Sólido Inte...|  7999.0|          new|       ELECTRO-SHOWS|2010-03-26 01:08:16|         250|              250|You have to pay t...|          4.5|
[2023-01-26T18:15:10.990+0000] {spark_submit.py:495} INFO - |MLA1163233412|Placa De Video Nv...| 34659.0|          new|      PCREGISTRADAOK|2020-11-02 13:17:33|           1|                1|Your shipment arr...|          4.5|
[2023-01-26T18:15:10.990+0000] {spark_submit.py:495} INFO - |MLA1302136118|Disco Duro Intern...| 18271.0|          new|DIAMONDSYSTEMSANJ...|2016-04-08 17:21:06|         100|               50|Your shipment arr...|          4.5|
[2023-01-26T18:15:10.990+0000] {spark_submit.py:495} INFO - |MLA1157644025|Disco Duro Extern...| 38790.0|          new|   ELECTRONIC-MARKET|2011-09-15 18:30:10|          25|               50|Your shipment arr...|          4.5|
[2023-01-26T18:15:10.990+0000] {spark_submit.py:495} INFO - | MLA902808321|Procesador Gamer ...| 23699.0|          new|            MALL WEB|2016-05-26 20:28:35|         500|                1|Your shipment arr...|          4.5|
[2023-01-26T18:15:10.990+0000] {spark_submit.py:495} INFO - |MLA1272836523|Disco Duro Intern...| 16999.0|          new|        TIENDA DELTA|2011-09-15 21:48:50|         150|                1|Your shipment arr...|          4.5|
[2023-01-26T18:15:10.990+0000] {spark_submit.py:495} INFO - +-------------+--------------------+--------+-------------+--------------------+-------------------+------------+-----------------+--------------------+-------------+
[2023-01-26T18:15:10.990+0000] {spark_submit.py:495} INFO - only showing top 20 rows
[2023-01-26T18:15:10.991+0000] {spark_submit.py:495} INFO - 
[2023-01-26T18:15:37.827+0000] {spark_submit.py:495} INFO - 23/01/26 18:15:37 ERROR Executor: Exception in task 0.0 in stage 4.0 (TID 4)
[2023-01-26T18:15:37.827+0000] {spark_submit.py:495} INFO - com.mongodb.MongoTimeoutException: Timed out after 30000 ms while waiting to connect. Client view of cluster state is {type=UNKNOWN, servers=[{address=172.22.0.2:27017, type=UNKNOWN, state=CONNECTING, exception={com.mongodb.MongoSocketOpenException: Exception opening socket}, caused by {java.net.SocketTimeoutException: connect timed out}}]
[2023-01-26T18:15:37.827+0000] {spark_submit.py:495} INFO - at com.mongodb.internal.connection.BaseCluster.getDescription(BaseCluster.java:177)
[2023-01-26T18:15:37.827+0000] {spark_submit.py:495} INFO - at com.mongodb.internal.connection.SingleServerCluster.getDescription(SingleServerCluster.java:41)
[2023-01-26T18:15:37.828+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate.getConnectedClusterDescription(MongoClientDelegate.java:147)
[2023-01-26T18:15:37.828+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate.createClientSession(MongoClientDelegate.java:98)
[2023-01-26T18:15:37.828+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.getClientSession(MongoClientDelegate.java:278)
[2023-01-26T18:15:37.828+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:202)
[2023-01-26T18:15:37.828+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:441)
[2023-01-26T18:15:37.828+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:421)
[2023-01-26T18:15:37.828+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$4(MongoSpark.scala:138)
[2023-01-26T18:15:37.828+0000] {spark_submit.py:495} INFO - at scala.collection.Iterator.foreach(Iterator.scala:943)
[2023-01-26T18:15:37.828+0000] {spark_submit.py:495} INFO - at scala.collection.Iterator.foreach$(Iterator.scala:943)
[2023-01-26T18:15:37.828+0000] {spark_submit.py:495} INFO - at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
[2023-01-26T18:15:37.828+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$3(MongoSpark.scala:122)
[2023-01-26T18:15:37.828+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$3$adapted(MongoSpark.scala:121)
[2023-01-26T18:15:37.828+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.$anonfun$withCollectionDo$1(MongoConnector.scala:186)
[2023-01-26T18:15:37.829+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.$anonfun$withDatabaseDo$1(MongoConnector.scala:171)
[2023-01-26T18:15:37.829+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withMongoClientDo(MongoConnector.scala:154)
[2023-01-26T18:15:37.829+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withDatabaseDo(MongoConnector.scala:171)
[2023-01-26T18:15:37.829+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withCollectionDo(MongoConnector.scala:184)
[2023-01-26T18:15:37.829+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$2(MongoSpark.scala:121)
[2023-01-26T18:15:37.829+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$2$adapted(MongoSpark.scala:120)
[2023-01-26T18:15:37.829+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1011)
[2023-01-26T18:15:37.829+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1011)
[2023-01-26T18:15:37.829+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2268)
[2023-01-26T18:15:37.829+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
[2023-01-26T18:15:37.830+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.Task.run(Task.scala:136)
[2023-01-26T18:15:37.830+0000] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)
[2023-01-26T18:15:37.830+0000] {spark_submit.py:495} INFO - at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)
[2023-01-26T18:15:37.830+0000] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)
[2023-01-26T18:15:37.830+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[2023-01-26T18:15:37.830+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[2023-01-26T18:15:37.830+0000] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2023-01-26T18:15:37.851+0000] {spark_submit.py:495} INFO - 23/01/26 18:15:37 ERROR TaskSetManager: Task 0 in stage 4.0 failed 1 times; aborting job
[2023-01-26T18:15:37.972+0000] {spark_submit.py:495} INFO - [[34m2023-01-26 18:15:37,972[0m] {[34mconsumerSpark.py:[0m37} ERROR[0m - Error al escribir en Mongo: An error occurred while calling o142.save.
[2023-01-26T18:15:37.973+0000] {spark_submit.py:495} INFO - : org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 4.0 failed 1 times, most recent failure: Lost task 0.0 in stage 4.0 (TID 4) (f1cbb5df32e6 executor driver): com.mongodb.MongoTimeoutException: Timed out after 30000 ms while waiting to connect. Client view of cluster state is {type=UNKNOWN, servers=[{address=172.22.0.2:27017, type=UNKNOWN, state=CONNECTING, exception={com.mongodb.MongoSocketOpenException: Exception opening socket}, caused by {java.net.SocketTimeoutException: connect timed out}}]
[2023-01-26T18:15:37.973+0000] {spark_submit.py:495} INFO - at com.mongodb.internal.connection.BaseCluster.getDescription(BaseCluster.java:177)
[2023-01-26T18:15:37.973+0000] {spark_submit.py:495} INFO - at com.mongodb.internal.connection.SingleServerCluster.getDescription(SingleServerCluster.java:41)
[2023-01-26T18:15:37.973+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate.getConnectedClusterDescription(MongoClientDelegate.java:147)
[2023-01-26T18:15:37.974+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate.createClientSession(MongoClientDelegate.java:98)
[2023-01-26T18:15:37.974+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.getClientSession(MongoClientDelegate.java:278)
[2023-01-26T18:15:37.974+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:202)
[2023-01-26T18:15:37.974+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:441)
[2023-01-26T18:15:37.974+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:421)
[2023-01-26T18:15:37.974+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$4(MongoSpark.scala:138)
[2023-01-26T18:15:37.974+0000] {spark_submit.py:495} INFO - at scala.collection.Iterator.foreach(Iterator.scala:943)
[2023-01-26T18:15:37.974+0000] {spark_submit.py:495} INFO - at scala.collection.Iterator.foreach$(Iterator.scala:943)
[2023-01-26T18:15:37.974+0000] {spark_submit.py:495} INFO - at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
[2023-01-26T18:15:37.974+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$3(MongoSpark.scala:122)
[2023-01-26T18:15:37.974+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$3$adapted(MongoSpark.scala:121)
[2023-01-26T18:15:37.974+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.$anonfun$withCollectionDo$1(MongoConnector.scala:186)
[2023-01-26T18:15:37.975+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.$anonfun$withDatabaseDo$1(MongoConnector.scala:171)
[2023-01-26T18:15:37.975+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withMongoClientDo(MongoConnector.scala:154)
[2023-01-26T18:15:37.975+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withDatabaseDo(MongoConnector.scala:171)
[2023-01-26T18:15:37.975+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withCollectionDo(MongoConnector.scala:184)
[2023-01-26T18:15:37.975+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$2(MongoSpark.scala:121)
[2023-01-26T18:15:37.975+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$2$adapted(MongoSpark.scala:120)
[2023-01-26T18:15:37.975+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1011)
[2023-01-26T18:15:37.975+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1011)
[2023-01-26T18:15:37.975+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2268)
[2023-01-26T18:15:37.975+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
[2023-01-26T18:15:37.975+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.Task.run(Task.scala:136)
[2023-01-26T18:15:37.975+0000] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)
[2023-01-26T18:15:37.975+0000] {spark_submit.py:495} INFO - at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)
[2023-01-26T18:15:37.976+0000] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)
[2023-01-26T18:15:37.976+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[2023-01-26T18:15:37.976+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[2023-01-26T18:15:37.976+0000] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2023-01-26T18:15:37.976+0000] {spark_submit.py:495} INFO - 
[2023-01-26T18:15:37.976+0000] {spark_submit.py:495} INFO - Driver stacktrace:
[2023-01-26T18:15:37.977+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)
[2023-01-26T18:15:37.977+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)
[2023-01-26T18:15:37.977+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)
[2023-01-26T18:15:37.977+0000] {spark_submit.py:495} INFO - at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
[2023-01-26T18:15:37.977+0000] {spark_submit.py:495} INFO - at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
[2023-01-26T18:15:37.977+0000] {spark_submit.py:495} INFO - at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
[2023-01-26T18:15:37.977+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)
[2023-01-26T18:15:37.977+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)
[2023-01-26T18:15:37.977+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)
[2023-01-26T18:15:37.977+0000] {spark_submit.py:495} INFO - at scala.Option.foreach(Option.scala:407)
[2023-01-26T18:15:37.977+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)
[2023-01-26T18:15:37.977+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)
[2023-01-26T18:15:37.977+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)
[2023-01-26T18:15:37.978+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)
[2023-01-26T18:15:37.978+0000] {spark_submit.py:495} INFO - at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
[2023-01-26T18:15:37.978+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)
[2023-01-26T18:15:37.978+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.runJob(SparkContext.scala:2228)
[2023-01-26T18:15:37.978+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.runJob(SparkContext.scala:2249)
[2023-01-26T18:15:37.978+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.runJob(SparkContext.scala:2268)
[2023-01-26T18:15:37.978+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.runJob(SparkContext.scala:2293)
[2023-01-26T18:15:37.978+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$1(RDD.scala:1011)
[2023-01-26T18:15:37.978+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
[2023-01-26T18:15:37.978+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
[2023-01-26T18:15:37.978+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.withScope(RDD.scala:406)
[2023-01-26T18:15:37.978+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.foreachPartition(RDD.scala:1009)
[2023-01-26T18:15:37.978+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.save(MongoSpark.scala:120)
[2023-01-26T18:15:37.979+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.save(MongoSpark.scala:169)
[2023-01-26T18:15:37.979+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.sql.DefaultSource.createRelation(DefaultSource.scala:70)
[2023-01-26T18:15:37.980+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:47)
[2023-01-26T18:15:37.980+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)
[2023-01-26T18:15:37.980+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)
[2023-01-26T18:15:37.980+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)
[2023-01-26T18:15:37.980+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)
[2023-01-26T18:15:37.980+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)
[2023-01-26T18:15:37.981+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)
[2023-01-26T18:15:37.981+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)
[2023-01-26T18:15:37.981+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
[2023-01-26T18:15:37.981+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
[2023-01-26T18:15:37.981+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)
[2023-01-26T18:15:37.981+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)
[2023-01-26T18:15:37.981+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:584)
[2023-01-26T18:15:37.981+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:176)
[2023-01-26T18:15:37.981+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:584)
[2023-01-26T18:15:37.981+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)
[2023-01-26T18:15:37.981+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
[2023-01-26T18:15:37.981+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
[2023-01-26T18:15:37.981+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
[2023-01-26T18:15:37.982+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
[2023-01-26T18:15:37.982+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:560)
[2023-01-26T18:15:37.982+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)
[2023-01-26T18:15:37.982+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)
[2023-01-26T18:15:37.982+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)
[2023-01-26T18:15:37.982+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:116)
[2023-01-26T18:15:37.982+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:860)
[2023-01-26T18:15:37.982+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:390)
[2023-01-26T18:15:37.982+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:363)
[2023-01-26T18:15:37.982+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:247)
[2023-01-26T18:15:37.982+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
[2023-01-26T18:15:37.982+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
[2023-01-26T18:15:37.982+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
[2023-01-26T18:15:37.982+0000] {spark_submit.py:495} INFO - at java.base/java.lang.reflect.Method.invoke(Method.java:566)
[2023-01-26T18:15:37.983+0000] {spark_submit.py:495} INFO - at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
[2023-01-26T18:15:37.983+0000] {spark_submit.py:495} INFO - at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
[2023-01-26T18:15:37.983+0000] {spark_submit.py:495} INFO - at py4j.Gateway.invoke(Gateway.java:282)
[2023-01-26T18:15:37.983+0000] {spark_submit.py:495} INFO - at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
[2023-01-26T18:15:37.983+0000] {spark_submit.py:495} INFO - at py4j.commands.CallCommand.execute(CallCommand.java:79)
[2023-01-26T18:15:37.983+0000] {spark_submit.py:495} INFO - at py4j.ClientServerConnection.sendCommand(ClientServerConnection.java:244)
[2023-01-26T18:15:37.983+0000] {spark_submit.py:495} INFO - at py4j.CallbackClient.sendCommand(CallbackClient.java:384)
[2023-01-26T18:15:37.983+0000] {spark_submit.py:495} INFO - at py4j.CallbackClient.sendCommand(CallbackClient.java:356)
[2023-01-26T18:15:37.983+0000] {spark_submit.py:495} INFO - at py4j.reflection.PythonProxyHandler.invoke(PythonProxyHandler.java:106)
[2023-01-26T18:15:37.983+0000] {spark_submit.py:495} INFO - at com.sun.proxy.$Proxy32.call(Unknown Source)
[2023-01-26T18:15:37.983+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1(ForeachBatchSink.scala:51)
[2023-01-26T18:15:37.984+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1$adapted(ForeachBatchSink.scala:51)
[2023-01-26T18:15:37.984+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.sources.ForeachBatchSink.addBatch(ForeachBatchSink.scala:32)
[2023-01-26T18:15:37.984+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:666)
[2023-01-26T18:15:37.984+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)
[2023-01-26T18:15:37.984+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)
[2023-01-26T18:15:37.984+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)
[2023-01-26T18:15:37.991+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
[2023-01-26T18:15:37.991+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
[2023-01-26T18:15:37.991+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:664)
[2023-01-26T18:15:37.992+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:375)
[2023-01-26T18:15:37.992+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:373)
[2023-01-26T18:15:37.992+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:68)
[2023-01-26T18:15:37.992+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:664)
[2023-01-26T18:15:37.992+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:256)
[2023-01-26T18:15:37.992+0000] {spark_submit.py:495} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2023-01-26T18:15:37.993+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:375)
[2023-01-26T18:15:37.993+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:373)
[2023-01-26T18:15:37.993+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:68)
[2023-01-26T18:15:37.994+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:219)
[2023-01-26T18:15:37.994+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)
[2023-01-26T18:15:37.994+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:213)
[2023-01-26T18:15:37.994+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:307)
[2023-01-26T18:15:37.994+0000] {spark_submit.py:495} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2023-01-26T18:15:37.994+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
[2023-01-26T18:15:37.994+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:285)
[2023-01-26T18:15:37.994+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:208)
[2023-01-26T18:15:37.994+0000] {spark_submit.py:495} INFO - Caused by: com.mongodb.MongoTimeoutException: Timed out after 30000 ms while waiting to connect. Client view of cluster state is {type=UNKNOWN, servers=[{address=172.22.0.2:27017, type=UNKNOWN, state=CONNECTING, exception={com.mongodb.MongoSocketOpenException: Exception opening socket}, caused by {java.net.SocketTimeoutException: connect timed out}}]
[2023-01-26T18:15:37.994+0000] {spark_submit.py:495} INFO - at com.mongodb.internal.connection.BaseCluster.getDescription(BaseCluster.java:177)
[2023-01-26T18:15:37.994+0000] {spark_submit.py:495} INFO - at com.mongodb.internal.connection.SingleServerCluster.getDescription(SingleServerCluster.java:41)
[2023-01-26T18:15:37.994+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate.getConnectedClusterDescription(MongoClientDelegate.java:147)
[2023-01-26T18:15:37.996+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate.createClientSession(MongoClientDelegate.java:98)
[2023-01-26T18:15:37.996+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.getClientSession(MongoClientDelegate.java:278)
[2023-01-26T18:15:37.996+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:202)
[2023-01-26T18:15:37.996+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:441)
[2023-01-26T18:15:37.996+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:421)
[2023-01-26T18:15:37.996+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$4(MongoSpark.scala:138)
[2023-01-26T18:15:37.997+0000] {spark_submit.py:495} INFO - at scala.collection.Iterator.foreach(Iterator.scala:943)
[2023-01-26T18:15:37.997+0000] {spark_submit.py:495} INFO - at scala.collection.Iterator.foreach$(Iterator.scala:943)
[2023-01-26T18:15:37.998+0000] {spark_submit.py:495} INFO - at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
[2023-01-26T18:15:37.998+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$3(MongoSpark.scala:122)
[2023-01-26T18:15:37.998+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$3$adapted(MongoSpark.scala:121)
[2023-01-26T18:15:37.998+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.$anonfun$withCollectionDo$1(MongoConnector.scala:186)
[2023-01-26T18:15:37.998+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.$anonfun$withDatabaseDo$1(MongoConnector.scala:171)
[2023-01-26T18:15:37.998+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withMongoClientDo(MongoConnector.scala:154)
[2023-01-26T18:15:37.998+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withDatabaseDo(MongoConnector.scala:171)
[2023-01-26T18:15:37.998+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withCollectionDo(MongoConnector.scala:184)
[2023-01-26T18:15:37.998+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$2(MongoSpark.scala:121)
[2023-01-26T18:15:37.998+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$2$adapted(MongoSpark.scala:120)
[2023-01-26T18:15:37.998+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1011)
[2023-01-26T18:15:37.998+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1011)
[2023-01-26T18:15:37.998+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2268)
[2023-01-26T18:15:37.999+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
[2023-01-26T18:15:37.999+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.Task.run(Task.scala:136)
[2023-01-26T18:15:37.999+0000] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)
[2023-01-26T18:15:37.999+0000] {spark_submit.py:495} INFO - at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)
[2023-01-26T18:15:37.999+0000] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)
[2023-01-26T18:15:37.999+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[2023-01-26T18:15:37.999+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[2023-01-26T18:15:37.999+0000] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2023-01-26T18:15:37.999+0000] {spark_submit.py:495} INFO - [0m
[2023-01-26T18:15:38.257+0000] {spark_submit.py:495} INFO - [[34m2023-01-26 18:15:38,257[0m] {[34mclientserver.py:[0m577} INFO[0m - Received command c on object id p0[0m
[2023-01-26T18:22:57.971+0000] {spark_submit.py:495} INFO - [[34m2023-01-26 18:22:57,971[0m] {[34mclientserver.py:[0m577} INFO[0m - Received command c on object id p0[0m
[2023-01-26T18:23:01.020+0000] {spark_submit.py:495} INFO - -------------------------------------------
[2023-01-26T18:23:01.020+0000] {spark_submit.py:495} INFO - Batch: 3
[2023-01-26T18:23:01.021+0000] {spark_submit.py:495} INFO - -------------------------------------------
[2023-01-26T18:23:01.065+0000] {spark_submit.py:495} INFO - +-------------+--------------------+--------+-------------+--------------------+-------------------+------------+-----------------+--------------------+-------------+
[2023-01-26T18:23:01.065+0000] {spark_submit.py:495} INFO - |           id|       nameOfProduct|   price|ProductStatus|          SellerName|   RegistrationDate|QuantitySold|QuantityAvailable|      Free shipping?|Store ratings|
[2023-01-26T18:23:01.065+0000] {spark_submit.py:495} INFO - +-------------+--------------------+--------+-------------+--------------------+-------------------+------------+-----------------+--------------------+-------------+
[2023-01-26T18:23:01.065+0000] {spark_submit.py:495} INFO - |MLA1221192372|Disco Duro Extern...| 26299.0|          new|    OVERHARD DIGITAL|2015-01-08 12:19:41|         250|              100|Your shipment arr...|          4.5|
[2023-01-26T18:23:01.065+0000] {spark_submit.py:495} INFO - |MLA1157512489|Placa De Video Nv...|124999.0|          new|      PCREGISTRADAOK|2020-11-02 13:17:33|          50|                1|Your shipment arr...|          4.5|
[2023-01-26T18:23:01.065+0000] {spark_submit.py:495} INFO - |MLA1115295971|Procesador Amd Ry...| 93999.0|          new|      PCREGISTRADAOK|2020-11-02 13:17:33|          25|                1|Your shipment arr...|          4.5|
[2023-01-26T18:23:01.065+0000] {spark_submit.py:495} INFO - |MLA1160198557|Memoria Ram Fury ...| 26699.0|          new|    OVERHARD DIGITAL|2015-01-08 12:19:41|         500|               50|Your shipment arr...|          4.5|
[2023-01-26T18:23:01.066+0000] {spark_submit.py:495} INFO - |MLA1148481665|Microprocesador P...| 58899.0|          new|            MALL WEB|2016-05-26 20:28:35|         150|                1|Your shipment arr...|          4.5|
[2023-01-26T18:23:01.066+0000] {spark_submit.py:495} INFO - |MLA1318244331|Disco Solido Ssd ...| 28799.0|          new|    OVERHARD DIGITAL|2015-01-08 12:19:41|           5|              150|Your shipment arr...|          4.5|
[2023-01-26T18:23:01.066+0000] {spark_submit.py:495} INFO - | MLA923637669|Placa De Video Ge...| 28699.0|          new|         INDIGOTRADE|2018-01-05 19:21:16|         500|               50|Your shipment arr...|          4.5|
[2023-01-26T18:23:01.066+0000] {spark_submit.py:495} INFO - |MLA1133481354|Disco Sólido Inte...| 13799.0|          new|           DALECLÍCK|2015-03-16 12:07:23|         500|               50|You have to pay t...|          4.5|
[2023-01-26T18:23:01.066+0000] {spark_submit.py:495} INFO - |MLA1139583771|Disco Sólido Inte...|  8339.0|          new|DIAMONDSYSTEMPADUAML|2015-07-01 12:32:22|         250|              500|You have to pay t...|          4.5|
[2023-01-26T18:23:01.066+0000] {spark_submit.py:495} INFO - | MLA882277177|Capturadora Video...|  2699.0|          new|         BSK-IMPORTS|2019-12-30 23:07:04|         500|               50|You have to pay t...|          3.0|
[2023-01-26T18:23:01.066+0000] {spark_submit.py:495} INFO - |MLA1283127595|Procesador Intel ...| 51379.0|          new|      PCREGISTRADAOK|2020-11-02 13:17:33|           1|                1|Your shipment arr...|          4.5|
[2023-01-26T18:23:01.066+0000] {spark_submit.py:495} INFO - |MLA1119996182|Disco Sólido Inte...| 25499.0|          new|            MALL WEB|2016-05-26 20:28:35|         500|                1|Your shipment arr...|          4.5|
[2023-01-26T18:23:01.066+0000] {spark_submit.py:495} INFO - |MLA1183925519|Placa De Video Nv...|299999.0|          new|ESTUDIOSMPSRLESTU...|2020-11-29 18:36:32|          50|                1|Your shipment arr...|          4.5|
[2023-01-26T18:23:01.066+0000] {spark_submit.py:495} INFO - | MLA844489972|Disco Duro Intern...| 11599.0|          new|           DALECLÍCK|2015-03-16 12:07:23|        5000|              500|You have to pay t...|          5.0|
[2023-01-26T18:23:01.066+0000] {spark_submit.py:495} INFO - | MLA833246699|Disco Sólido Inte...| 26999.0|          new|            MALL WEB|2016-05-26 20:28:35|         500|                1|Your shipment arr...|          4.5|
[2023-01-26T18:23:01.066+0000] {spark_submit.py:495} INFO - |MLA1163233412|Placa De Video Nv...| 34659.0|          new|      PCREGISTRADAOK|2020-11-02 13:17:33|           1|                1|Your shipment arr...|          4.5|
[2023-01-26T18:23:01.066+0000] {spark_submit.py:495} INFO - |MLA1314590445|Disco Sólido Inte...|  7999.0|          new|       ELECTRO-SHOWS|2010-03-26 01:08:16|         250|              250|You have to pay t...|          4.5|
[2023-01-26T18:23:01.067+0000] {spark_submit.py:495} INFO - |MLA1302136118|Disco Duro Intern...| 18271.0|          new|DIAMONDSYSTEMSANJ...|2016-04-08 17:21:06|         100|               50|Your shipment arr...|          4.5|
[2023-01-26T18:23:01.067+0000] {spark_submit.py:495} INFO - | MLA902808321|Procesador Gamer ...| 23699.0|          new|            MALL WEB|2016-05-26 20:28:35|         500|                1|Your shipment arr...|          4.5|
[2023-01-26T18:23:01.067+0000] {spark_submit.py:495} INFO - |MLA1157644025|Disco Duro Extern...| 38790.0|          new|   ELECTRONIC-MARKET|2011-09-15 18:30:10|          25|               50|Your shipment arr...|          4.5|
[2023-01-26T18:23:01.067+0000] {spark_submit.py:495} INFO - +-------------+--------------------+--------+-------------+--------------------+-------------------+------------+-----------------+--------------------+-------------+
[2023-01-26T18:23:01.067+0000] {spark_submit.py:495} INFO - only showing top 20 rows
[2023-01-26T18:23:01.067+0000] {spark_submit.py:495} INFO - 
[2023-01-26T18:23:28.704+0000] {spark_submit.py:495} INFO - 23/01/26 18:23:28 ERROR Executor: Exception in task 0.0 in stage 7.0 (TID 7)
[2023-01-26T18:23:28.704+0000] {spark_submit.py:495} INFO - com.mongodb.MongoTimeoutException: Timed out after 30000 ms while waiting to connect. Client view of cluster state is {type=UNKNOWN, servers=[{address=172.22.0.2:27017, type=UNKNOWN, state=CONNECTING, exception={com.mongodb.MongoSocketOpenException: Exception opening socket}, caused by {java.net.SocketTimeoutException: connect timed out}}]
[2023-01-26T18:23:28.704+0000] {spark_submit.py:495} INFO - at com.mongodb.internal.connection.BaseCluster.getDescription(BaseCluster.java:177)
[2023-01-26T18:23:28.704+0000] {spark_submit.py:495} INFO - at com.mongodb.internal.connection.SingleServerCluster.getDescription(SingleServerCluster.java:41)
[2023-01-26T18:23:28.704+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate.getConnectedClusterDescription(MongoClientDelegate.java:147)
[2023-01-26T18:23:28.705+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate.createClientSession(MongoClientDelegate.java:98)
[2023-01-26T18:23:28.705+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.getClientSession(MongoClientDelegate.java:278)
[2023-01-26T18:23:28.705+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:202)
[2023-01-26T18:23:28.705+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:441)
[2023-01-26T18:23:28.705+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:421)
[2023-01-26T18:23:28.705+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$4(MongoSpark.scala:138)
[2023-01-26T18:23:28.705+0000] {spark_submit.py:495} INFO - at scala.collection.Iterator.foreach(Iterator.scala:943)
[2023-01-26T18:23:28.705+0000] {spark_submit.py:495} INFO - at scala.collection.Iterator.foreach$(Iterator.scala:943)
[2023-01-26T18:23:28.705+0000] {spark_submit.py:495} INFO - at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
[2023-01-26T18:23:28.705+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$3(MongoSpark.scala:122)
[2023-01-26T18:23:28.705+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$3$adapted(MongoSpark.scala:121)
[2023-01-26T18:23:28.706+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.$anonfun$withCollectionDo$1(MongoConnector.scala:186)
[2023-01-26T18:23:28.706+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.$anonfun$withDatabaseDo$1(MongoConnector.scala:171)
[2023-01-26T18:23:28.706+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withMongoClientDo(MongoConnector.scala:154)
[2023-01-26T18:23:28.706+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withDatabaseDo(MongoConnector.scala:171)
[2023-01-26T18:23:28.706+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withCollectionDo(MongoConnector.scala:184)
[2023-01-26T18:23:28.706+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$2(MongoSpark.scala:121)
[2023-01-26T18:23:28.706+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$2$adapted(MongoSpark.scala:120)
[2023-01-26T18:23:28.706+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1011)
[2023-01-26T18:23:28.706+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1011)
[2023-01-26T18:23:28.706+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2268)
[2023-01-26T18:23:28.706+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
[2023-01-26T18:23:28.706+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.Task.run(Task.scala:136)
[2023-01-26T18:23:28.706+0000] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)
[2023-01-26T18:23:28.707+0000] {spark_submit.py:495} INFO - at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)
[2023-01-26T18:23:28.707+0000] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)
[2023-01-26T18:23:28.707+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[2023-01-26T18:23:28.707+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[2023-01-26T18:23:28.707+0000] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2023-01-26T18:23:28.707+0000] {spark_submit.py:495} INFO - 23/01/26 18:23:28 ERROR TaskSetManager: Task 0 in stage 7.0 failed 1 times; aborting job
[2023-01-26T18:23:28.845+0000] {spark_submit.py:495} INFO - [[34m2023-01-26 18:23:28,844[0m] {[34mconsumerSpark.py:[0m37} ERROR[0m - Error al escribir en Mongo: An error occurred while calling o158.save.
[2023-01-26T18:23:28.845+0000] {spark_submit.py:495} INFO - : org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 7.0 failed 1 times, most recent failure: Lost task 0.0 in stage 7.0 (TID 7) (f1cbb5df32e6 executor driver): com.mongodb.MongoTimeoutException: Timed out after 30000 ms while waiting to connect. Client view of cluster state is {type=UNKNOWN, servers=[{address=172.22.0.2:27017, type=UNKNOWN, state=CONNECTING, exception={com.mongodb.MongoSocketOpenException: Exception opening socket}, caused by {java.net.SocketTimeoutException: connect timed out}}]
[2023-01-26T18:23:28.845+0000] {spark_submit.py:495} INFO - at com.mongodb.internal.connection.BaseCluster.getDescription(BaseCluster.java:177)
[2023-01-26T18:23:28.845+0000] {spark_submit.py:495} INFO - at com.mongodb.internal.connection.SingleServerCluster.getDescription(SingleServerCluster.java:41)
[2023-01-26T18:23:28.846+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate.getConnectedClusterDescription(MongoClientDelegate.java:147)
[2023-01-26T18:23:28.846+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate.createClientSession(MongoClientDelegate.java:98)
[2023-01-26T18:23:28.846+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.getClientSession(MongoClientDelegate.java:278)
[2023-01-26T18:23:28.846+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:202)
[2023-01-26T18:23:28.846+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:441)
[2023-01-26T18:23:28.846+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:421)
[2023-01-26T18:23:28.847+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$4(MongoSpark.scala:138)
[2023-01-26T18:23:28.847+0000] {spark_submit.py:495} INFO - at scala.collection.Iterator.foreach(Iterator.scala:943)
[2023-01-26T18:23:28.847+0000] {spark_submit.py:495} INFO - at scala.collection.Iterator.foreach$(Iterator.scala:943)
[2023-01-26T18:23:28.847+0000] {spark_submit.py:495} INFO - at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
[2023-01-26T18:23:28.847+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$3(MongoSpark.scala:122)
[2023-01-26T18:23:28.847+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$3$adapted(MongoSpark.scala:121)
[2023-01-26T18:23:28.848+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.$anonfun$withCollectionDo$1(MongoConnector.scala:186)
[2023-01-26T18:23:28.848+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.$anonfun$withDatabaseDo$1(MongoConnector.scala:171)
[2023-01-26T18:23:28.848+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withMongoClientDo(MongoConnector.scala:154)
[2023-01-26T18:23:28.848+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withDatabaseDo(MongoConnector.scala:171)
[2023-01-26T18:23:28.848+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withCollectionDo(MongoConnector.scala:184)
[2023-01-26T18:23:28.849+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$2(MongoSpark.scala:121)
[2023-01-26T18:23:28.849+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$2$adapted(MongoSpark.scala:120)
[2023-01-26T18:23:28.849+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1011)
[2023-01-26T18:23:28.849+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1011)
[2023-01-26T18:23:28.849+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2268)
[2023-01-26T18:23:28.849+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
[2023-01-26T18:23:28.850+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.Task.run(Task.scala:136)
[2023-01-26T18:23:28.850+0000] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)
[2023-01-26T18:23:28.850+0000] {spark_submit.py:495} INFO - at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)
[2023-01-26T18:23:28.850+0000] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)
[2023-01-26T18:23:28.850+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[2023-01-26T18:23:28.850+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[2023-01-26T18:23:28.851+0000] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2023-01-26T18:23:28.851+0000] {spark_submit.py:495} INFO - 
[2023-01-26T18:23:28.851+0000] {spark_submit.py:495} INFO - Driver stacktrace:
[2023-01-26T18:23:28.851+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)
[2023-01-26T18:23:28.851+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)
[2023-01-26T18:23:28.851+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)
[2023-01-26T18:23:28.852+0000] {spark_submit.py:495} INFO - at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
[2023-01-26T18:23:28.852+0000] {spark_submit.py:495} INFO - at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
[2023-01-26T18:23:28.852+0000] {spark_submit.py:495} INFO - at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
[2023-01-26T18:23:28.852+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)
[2023-01-26T18:23:28.852+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)
[2023-01-26T18:23:28.853+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)
[2023-01-26T18:23:28.853+0000] {spark_submit.py:495} INFO - at scala.Option.foreach(Option.scala:407)
[2023-01-26T18:23:28.853+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)
[2023-01-26T18:23:28.853+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)
[2023-01-26T18:23:28.853+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)
[2023-01-26T18:23:28.853+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)
[2023-01-26T18:23:28.854+0000] {spark_submit.py:495} INFO - at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
[2023-01-26T18:23:28.854+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)
[2023-01-26T18:23:28.854+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.runJob(SparkContext.scala:2228)
[2023-01-26T18:23:28.854+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.runJob(SparkContext.scala:2249)
[2023-01-26T18:23:28.854+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.runJob(SparkContext.scala:2268)
[2023-01-26T18:23:28.854+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.runJob(SparkContext.scala:2293)
[2023-01-26T18:23:28.855+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$1(RDD.scala:1011)
[2023-01-26T18:23:28.855+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
[2023-01-26T18:23:28.855+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
[2023-01-26T18:23:28.855+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.withScope(RDD.scala:406)
[2023-01-26T18:23:28.855+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.foreachPartition(RDD.scala:1009)
[2023-01-26T18:23:28.855+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.save(MongoSpark.scala:120)
[2023-01-26T18:23:28.856+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.save(MongoSpark.scala:169)
[2023-01-26T18:23:28.856+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.sql.DefaultSource.createRelation(DefaultSource.scala:70)
[2023-01-26T18:23:28.856+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:47)
[2023-01-26T18:23:28.856+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)
[2023-01-26T18:23:28.856+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)
[2023-01-26T18:23:28.856+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)
[2023-01-26T18:23:28.857+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)
[2023-01-26T18:23:28.857+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)
[2023-01-26T18:23:28.857+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)
[2023-01-26T18:23:28.857+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)
[2023-01-26T18:23:28.857+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
[2023-01-26T18:23:28.858+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
[2023-01-26T18:23:28.858+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)
[2023-01-26T18:23:28.858+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)
[2023-01-26T18:23:28.858+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:584)
[2023-01-26T18:23:28.858+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:176)
[2023-01-26T18:23:28.858+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:584)
[2023-01-26T18:23:28.859+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)
[2023-01-26T18:23:28.859+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
[2023-01-26T18:23:28.859+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
[2023-01-26T18:23:28.859+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
[2023-01-26T18:23:28.859+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
[2023-01-26T18:23:28.860+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:560)
[2023-01-26T18:23:28.860+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)
[2023-01-26T18:23:28.860+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)
[2023-01-26T18:23:28.860+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)
[2023-01-26T18:23:28.860+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:116)
[2023-01-26T18:23:28.860+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:860)
[2023-01-26T18:23:28.861+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:390)
[2023-01-26T18:23:28.861+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:363)
[2023-01-26T18:23:28.861+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:247)
[2023-01-26T18:23:28.861+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
[2023-01-26T18:23:28.861+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
[2023-01-26T18:23:28.861+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
[2023-01-26T18:23:28.862+0000] {spark_submit.py:495} INFO - at java.base/java.lang.reflect.Method.invoke(Method.java:566)
[2023-01-26T18:23:28.862+0000] {spark_submit.py:495} INFO - at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
[2023-01-26T18:23:28.862+0000] {spark_submit.py:495} INFO - at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
[2023-01-26T18:23:28.862+0000] {spark_submit.py:495} INFO - at py4j.Gateway.invoke(Gateway.java:282)
[2023-01-26T18:23:28.862+0000] {spark_submit.py:495} INFO - at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
[2023-01-26T18:23:28.862+0000] {spark_submit.py:495} INFO - at py4j.commands.CallCommand.execute(CallCommand.java:79)
[2023-01-26T18:23:28.863+0000] {spark_submit.py:495} INFO - at py4j.ClientServerConnection.sendCommand(ClientServerConnection.java:244)
[2023-01-26T18:23:28.863+0000] {spark_submit.py:495} INFO - at py4j.CallbackClient.sendCommand(CallbackClient.java:384)
[2023-01-26T18:23:28.863+0000] {spark_submit.py:495} INFO - at py4j.CallbackClient.sendCommand(CallbackClient.java:356)
[2023-01-26T18:23:28.863+0000] {spark_submit.py:495} INFO - at py4j.reflection.PythonProxyHandler.invoke(PythonProxyHandler.java:106)
[2023-01-26T18:23:28.863+0000] {spark_submit.py:495} INFO - at com.sun.proxy.$Proxy32.call(Unknown Source)
[2023-01-26T18:23:28.864+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1(ForeachBatchSink.scala:51)
[2023-01-26T18:23:28.864+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1$adapted(ForeachBatchSink.scala:51)
[2023-01-26T18:23:28.864+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.sources.ForeachBatchSink.addBatch(ForeachBatchSink.scala:32)
[2023-01-26T18:23:28.864+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:666)
[2023-01-26T18:23:28.864+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)
[2023-01-26T18:23:28.864+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)
[2023-01-26T18:23:28.865+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)
[2023-01-26T18:23:28.865+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
[2023-01-26T18:23:28.865+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
[2023-01-26T18:23:28.866+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:664)
[2023-01-26T18:23:28.866+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:375)
[2023-01-26T18:23:28.866+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:373)
[2023-01-26T18:23:28.866+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:68)
[2023-01-26T18:23:28.866+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:664)
[2023-01-26T18:23:28.866+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:256)
[2023-01-26T18:23:28.867+0000] {spark_submit.py:495} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2023-01-26T18:23:28.867+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:375)
[2023-01-26T18:23:28.867+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:373)
[2023-01-26T18:23:28.867+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:68)
[2023-01-26T18:23:28.867+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:219)
[2023-01-26T18:23:28.868+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)
[2023-01-26T18:23:28.868+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:213)
[2023-01-26T18:23:28.868+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:307)
[2023-01-26T18:23:28.868+0000] {spark_submit.py:495} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2023-01-26T18:23:28.868+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
[2023-01-26T18:23:28.869+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:285)
[2023-01-26T18:23:28.869+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:208)
[2023-01-26T18:23:28.869+0000] {spark_submit.py:495} INFO - Caused by: com.mongodb.MongoTimeoutException: Timed out after 30000 ms while waiting to connect. Client view of cluster state is {type=UNKNOWN, servers=[{address=172.22.0.2:27017, type=UNKNOWN, state=CONNECTING, exception={com.mongodb.MongoSocketOpenException: Exception opening socket}, caused by {java.net.SocketTimeoutException: connect timed out}}]
[2023-01-26T18:23:28.869+0000] {spark_submit.py:495} INFO - at com.mongodb.internal.connection.BaseCluster.getDescription(BaseCluster.java:177)
[2023-01-26T18:23:28.869+0000] {spark_submit.py:495} INFO - at com.mongodb.internal.connection.SingleServerCluster.getDescription(SingleServerCluster.java:41)
[2023-01-26T18:23:28.869+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate.getConnectedClusterDescription(MongoClientDelegate.java:147)
[2023-01-26T18:23:28.870+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate.createClientSession(MongoClientDelegate.java:98)
[2023-01-26T18:23:28.870+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.getClientSession(MongoClientDelegate.java:278)
[2023-01-26T18:23:28.870+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:202)
[2023-01-26T18:23:28.870+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:441)
[2023-01-26T18:23:28.870+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:421)
[2023-01-26T18:23:28.870+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$4(MongoSpark.scala:138)
[2023-01-26T18:23:28.871+0000] {spark_submit.py:495} INFO - at scala.collection.Iterator.foreach(Iterator.scala:943)
[2023-01-26T18:23:28.871+0000] {spark_submit.py:495} INFO - at scala.collection.Iterator.foreach$(Iterator.scala:943)
[2023-01-26T18:23:28.871+0000] {spark_submit.py:495} INFO - at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
[2023-01-26T18:23:28.871+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$3(MongoSpark.scala:122)
[2023-01-26T18:23:28.871+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$3$adapted(MongoSpark.scala:121)
[2023-01-26T18:23:28.871+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.$anonfun$withCollectionDo$1(MongoConnector.scala:186)
[2023-01-26T18:23:28.872+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.$anonfun$withDatabaseDo$1(MongoConnector.scala:171)
[2023-01-26T18:23:28.872+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withMongoClientDo(MongoConnector.scala:154)
[2023-01-26T18:23:28.872+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withDatabaseDo(MongoConnector.scala:171)
[2023-01-26T18:23:28.872+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withCollectionDo(MongoConnector.scala:184)
[2023-01-26T18:23:28.872+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$2(MongoSpark.scala:121)
[2023-01-26T18:23:28.873+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$2$adapted(MongoSpark.scala:120)
[2023-01-26T18:23:28.873+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1011)
[2023-01-26T18:23:28.873+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1011)
[2023-01-26T18:23:28.873+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2268)
[2023-01-26T18:23:28.873+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
[2023-01-26T18:23:28.873+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.Task.run(Task.scala:136)
[2023-01-26T18:23:28.874+0000] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)
[2023-01-26T18:23:28.874+0000] {spark_submit.py:495} INFO - at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)
[2023-01-26T18:23:28.875+0000] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)
[2023-01-26T18:23:28.875+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[2023-01-26T18:23:28.875+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[2023-01-26T18:23:28.875+0000] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2023-01-26T18:23:28.875+0000] {spark_submit.py:495} INFO - [0m
[2023-01-26T18:23:28.977+0000] {spark_submit.py:495} INFO - [[34m2023-01-26 18:23:28,977[0m] {[34mclientserver.py:[0m577} INFO[0m - Received command c on object id p0[0m
[2023-01-26T18:23:59.059+0000] {spark_submit.py:495} INFO - 23/01/26 18:23:59 ERROR Executor: Exception in task 0.0 in stage 9.0 (TID 9)
[2023-01-26T18:23:59.059+0000] {spark_submit.py:495} INFO - com.mongodb.MongoTimeoutException: Timed out after 30000 ms while waiting to connect. Client view of cluster state is {type=UNKNOWN, servers=[{address=172.22.0.2:27017, type=UNKNOWN, state=CONNECTING, exception={com.mongodb.MongoSocketOpenException: Exception opening socket}, caused by {java.net.SocketTimeoutException: connect timed out}}]
[2023-01-26T18:23:59.059+0000] {spark_submit.py:495} INFO - at com.mongodb.internal.connection.BaseCluster.getDescription(BaseCluster.java:177)
[2023-01-26T18:23:59.059+0000] {spark_submit.py:495} INFO - at com.mongodb.internal.connection.SingleServerCluster.getDescription(SingleServerCluster.java:41)
[2023-01-26T18:23:59.059+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate.getConnectedClusterDescription(MongoClientDelegate.java:147)
[2023-01-26T18:23:59.060+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate.createClientSession(MongoClientDelegate.java:98)
[2023-01-26T18:23:59.060+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.getClientSession(MongoClientDelegate.java:278)
[2023-01-26T18:23:59.060+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:202)
[2023-01-26T18:23:59.060+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:441)
[2023-01-26T18:23:59.060+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:421)
[2023-01-26T18:23:59.060+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$4(MongoSpark.scala:138)
[2023-01-26T18:23:59.060+0000] {spark_submit.py:495} INFO - at scala.collection.Iterator.foreach(Iterator.scala:943)
[2023-01-26T18:23:59.060+0000] {spark_submit.py:495} INFO - at scala.collection.Iterator.foreach$(Iterator.scala:943)
[2023-01-26T18:23:59.061+0000] {spark_submit.py:495} INFO - at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
[2023-01-26T18:23:59.061+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$3(MongoSpark.scala:122)
[2023-01-26T18:23:59.061+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$3$adapted(MongoSpark.scala:121)
[2023-01-26T18:23:59.061+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.$anonfun$withCollectionDo$1(MongoConnector.scala:186)
[2023-01-26T18:23:59.061+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.$anonfun$withDatabaseDo$1(MongoConnector.scala:171)
[2023-01-26T18:23:59.061+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withMongoClientDo(MongoConnector.scala:154)
[2023-01-26T18:23:59.061+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withDatabaseDo(MongoConnector.scala:171)
[2023-01-26T18:23:59.062+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withCollectionDo(MongoConnector.scala:184)
[2023-01-26T18:23:59.062+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$2(MongoSpark.scala:121)
[2023-01-26T18:23:59.062+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$2$adapted(MongoSpark.scala:120)
[2023-01-26T18:23:59.062+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1011)
[2023-01-26T18:23:59.062+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1011)
[2023-01-26T18:23:59.062+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2268)
[2023-01-26T18:23:59.062+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
[2023-01-26T18:23:59.062+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.Task.run(Task.scala:136)
[2023-01-26T18:23:59.063+0000] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)
[2023-01-26T18:23:59.063+0000] {spark_submit.py:495} INFO - at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)
[2023-01-26T18:23:59.063+0000] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)
[2023-01-26T18:23:59.063+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[2023-01-26T18:23:59.063+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[2023-01-26T18:23:59.063+0000] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2023-01-26T18:23:59.064+0000] {spark_submit.py:495} INFO - 23/01/26 18:23:59 ERROR TaskSetManager: Task 0 in stage 9.0 failed 1 times; aborting job
[2023-01-26T18:23:59.196+0000] {spark_submit.py:495} INFO - [[34m2023-01-26 18:23:59,196[0m] {[34mconsumerSpark.py:[0m37} ERROR[0m - Error al escribir en Mongo: An error occurred while calling o167.save.
[2023-01-26T18:23:59.197+0000] {spark_submit.py:495} INFO - : org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 9.0 failed 1 times, most recent failure: Lost task 0.0 in stage 9.0 (TID 9) (f1cbb5df32e6 executor driver): com.mongodb.MongoTimeoutException: Timed out after 30000 ms while waiting to connect. Client view of cluster state is {type=UNKNOWN, servers=[{address=172.22.0.2:27017, type=UNKNOWN, state=CONNECTING, exception={com.mongodb.MongoSocketOpenException: Exception opening socket}, caused by {java.net.SocketTimeoutException: connect timed out}}]
[2023-01-26T18:23:59.197+0000] {spark_submit.py:495} INFO - at com.mongodb.internal.connection.BaseCluster.getDescription(BaseCluster.java:177)
[2023-01-26T18:23:59.197+0000] {spark_submit.py:495} INFO - at com.mongodb.internal.connection.SingleServerCluster.getDescription(SingleServerCluster.java:41)
[2023-01-26T18:23:59.197+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate.getConnectedClusterDescription(MongoClientDelegate.java:147)
[2023-01-26T18:23:59.197+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate.createClientSession(MongoClientDelegate.java:98)
[2023-01-26T18:23:59.197+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.getClientSession(MongoClientDelegate.java:278)
[2023-01-26T18:23:59.197+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:202)
[2023-01-26T18:23:59.197+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:441)
[2023-01-26T18:23:59.197+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:421)
[2023-01-26T18:23:59.197+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$4(MongoSpark.scala:138)
[2023-01-26T18:23:59.198+0000] {spark_submit.py:495} INFO - at scala.collection.Iterator.foreach(Iterator.scala:943)
[2023-01-26T18:23:59.198+0000] {spark_submit.py:495} INFO - at scala.collection.Iterator.foreach$(Iterator.scala:943)
[2023-01-26T18:23:59.198+0000] {spark_submit.py:495} INFO - at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
[2023-01-26T18:23:59.198+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$3(MongoSpark.scala:122)
[2023-01-26T18:23:59.198+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$3$adapted(MongoSpark.scala:121)
[2023-01-26T18:23:59.198+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.$anonfun$withCollectionDo$1(MongoConnector.scala:186)
[2023-01-26T18:23:59.198+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.$anonfun$withDatabaseDo$1(MongoConnector.scala:171)
[2023-01-26T18:23:59.198+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withMongoClientDo(MongoConnector.scala:154)
[2023-01-26T18:23:59.198+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withDatabaseDo(MongoConnector.scala:171)
[2023-01-26T18:23:59.198+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withCollectionDo(MongoConnector.scala:184)
[2023-01-26T18:23:59.198+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$2(MongoSpark.scala:121)
[2023-01-26T18:23:59.198+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$2$adapted(MongoSpark.scala:120)
[2023-01-26T18:23:59.198+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1011)
[2023-01-26T18:23:59.199+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1011)
[2023-01-26T18:23:59.199+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2268)
[2023-01-26T18:23:59.199+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
[2023-01-26T18:23:59.199+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.Task.run(Task.scala:136)
[2023-01-26T18:23:59.199+0000] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)
[2023-01-26T18:23:59.199+0000] {spark_submit.py:495} INFO - at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)
[2023-01-26T18:23:59.199+0000] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)
[2023-01-26T18:23:59.199+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[2023-01-26T18:23:59.199+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[2023-01-26T18:23:59.199+0000] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2023-01-26T18:23:59.199+0000] {spark_submit.py:495} INFO - 
[2023-01-26T18:23:59.199+0000] {spark_submit.py:495} INFO - Driver stacktrace:
[2023-01-26T18:23:59.199+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)
[2023-01-26T18:23:59.200+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)
[2023-01-26T18:23:59.200+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)
[2023-01-26T18:23:59.200+0000] {spark_submit.py:495} INFO - at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
[2023-01-26T18:23:59.200+0000] {spark_submit.py:495} INFO - at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
[2023-01-26T18:23:59.200+0000] {spark_submit.py:495} INFO - at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
[2023-01-26T18:23:59.200+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)
[2023-01-26T18:23:59.200+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)
[2023-01-26T18:23:59.200+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)
[2023-01-26T18:23:59.200+0000] {spark_submit.py:495} INFO - at scala.Option.foreach(Option.scala:407)
[2023-01-26T18:23:59.200+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)
[2023-01-26T18:23:59.200+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)
[2023-01-26T18:23:59.201+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)
[2023-01-26T18:23:59.201+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)
[2023-01-26T18:23:59.201+0000] {spark_submit.py:495} INFO - at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
[2023-01-26T18:23:59.201+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)
[2023-01-26T18:23:59.201+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.runJob(SparkContext.scala:2228)
[2023-01-26T18:23:59.201+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.runJob(SparkContext.scala:2249)
[2023-01-26T18:23:59.201+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.runJob(SparkContext.scala:2268)
[2023-01-26T18:23:59.201+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.runJob(SparkContext.scala:2293)
[2023-01-26T18:23:59.201+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$1(RDD.scala:1011)
[2023-01-26T18:23:59.201+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
[2023-01-26T18:23:59.201+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
[2023-01-26T18:23:59.202+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.withScope(RDD.scala:406)
[2023-01-26T18:23:59.202+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.foreachPartition(RDD.scala:1009)
[2023-01-26T18:23:59.202+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.save(MongoSpark.scala:120)
[2023-01-26T18:23:59.202+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.save(MongoSpark.scala:169)
[2023-01-26T18:23:59.202+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.sql.DefaultSource.createRelation(DefaultSource.scala:70)
[2023-01-26T18:23:59.202+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:47)
[2023-01-26T18:23:59.202+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)
[2023-01-26T18:23:59.202+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)
[2023-01-26T18:23:59.202+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)
[2023-01-26T18:23:59.202+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)
[2023-01-26T18:23:59.203+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)
[2023-01-26T18:23:59.203+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)
[2023-01-26T18:23:59.203+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)
[2023-01-26T18:23:59.203+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
[2023-01-26T18:23:59.203+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
[2023-01-26T18:23:59.203+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)
[2023-01-26T18:23:59.203+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)
[2023-01-26T18:23:59.203+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:584)
[2023-01-26T18:23:59.203+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:176)
[2023-01-26T18:23:59.203+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:584)
[2023-01-26T18:23:59.204+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)
[2023-01-26T18:23:59.204+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
[2023-01-26T18:23:59.204+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
[2023-01-26T18:23:59.204+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
[2023-01-26T18:23:59.204+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
[2023-01-26T18:23:59.204+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:560)
[2023-01-26T18:23:59.204+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)
[2023-01-26T18:23:59.204+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)
[2023-01-26T18:23:59.204+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)
[2023-01-26T18:23:59.204+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:116)
[2023-01-26T18:23:59.204+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:860)
[2023-01-26T18:23:59.205+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:390)
[2023-01-26T18:23:59.205+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:363)
[2023-01-26T18:23:59.205+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:247)
[2023-01-26T18:23:59.205+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
[2023-01-26T18:23:59.205+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
[2023-01-26T18:23:59.205+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
[2023-01-26T18:23:59.205+0000] {spark_submit.py:495} INFO - at java.base/java.lang.reflect.Method.invoke(Method.java:566)
[2023-01-26T18:23:59.205+0000] {spark_submit.py:495} INFO - at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
[2023-01-26T18:23:59.205+0000] {spark_submit.py:495} INFO - at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
[2023-01-26T18:23:59.205+0000] {spark_submit.py:495} INFO - at py4j.Gateway.invoke(Gateway.java:282)
[2023-01-26T18:23:59.206+0000] {spark_submit.py:495} INFO - at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
[2023-01-26T18:23:59.206+0000] {spark_submit.py:495} INFO - at py4j.commands.CallCommand.execute(CallCommand.java:79)
[2023-01-26T18:23:59.206+0000] {spark_submit.py:495} INFO - at py4j.ClientServerConnection.sendCommand(ClientServerConnection.java:244)
[2023-01-26T18:23:59.206+0000] {spark_submit.py:495} INFO - at py4j.CallbackClient.sendCommand(CallbackClient.java:384)
[2023-01-26T18:23:59.206+0000] {spark_submit.py:495} INFO - at py4j.CallbackClient.sendCommand(CallbackClient.java:356)
[2023-01-26T18:23:59.206+0000] {spark_submit.py:495} INFO - at py4j.reflection.PythonProxyHandler.invoke(PythonProxyHandler.java:106)
[2023-01-26T18:23:59.206+0000] {spark_submit.py:495} INFO - at com.sun.proxy.$Proxy32.call(Unknown Source)
[2023-01-26T18:23:59.206+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1(ForeachBatchSink.scala:51)
[2023-01-26T18:23:59.206+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1$adapted(ForeachBatchSink.scala:51)
[2023-01-26T18:23:59.207+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.sources.ForeachBatchSink.addBatch(ForeachBatchSink.scala:32)
[2023-01-26T18:23:59.207+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:666)
[2023-01-26T18:23:59.207+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)
[2023-01-26T18:23:59.207+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)
[2023-01-26T18:23:59.207+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)
[2023-01-26T18:23:59.207+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
[2023-01-26T18:23:59.207+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
[2023-01-26T18:23:59.207+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:664)
[2023-01-26T18:23:59.208+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:375)
[2023-01-26T18:23:59.208+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:373)
[2023-01-26T18:23:59.208+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:68)
[2023-01-26T18:23:59.208+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:664)
[2023-01-26T18:23:59.208+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:256)
[2023-01-26T18:23:59.208+0000] {spark_submit.py:495} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2023-01-26T18:23:59.208+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:375)
[2023-01-26T18:23:59.208+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:373)
[2023-01-26T18:23:59.208+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:68)
[2023-01-26T18:23:59.208+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:219)
[2023-01-26T18:23:59.209+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)
[2023-01-26T18:23:59.209+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:213)
[2023-01-26T18:23:59.209+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:307)
[2023-01-26T18:23:59.209+0000] {spark_submit.py:495} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2023-01-26T18:23:59.209+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
[2023-01-26T18:23:59.209+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:285)
[2023-01-26T18:23:59.209+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:208)
[2023-01-26T18:23:59.209+0000] {spark_submit.py:495} INFO - Caused by: com.mongodb.MongoTimeoutException: Timed out after 30000 ms while waiting to connect. Client view of cluster state is {type=UNKNOWN, servers=[{address=172.22.0.2:27017, type=UNKNOWN, state=CONNECTING, exception={com.mongodb.MongoSocketOpenException: Exception opening socket}, caused by {java.net.SocketTimeoutException: connect timed out}}]
[2023-01-26T18:23:59.209+0000] {spark_submit.py:495} INFO - at com.mongodb.internal.connection.BaseCluster.getDescription(BaseCluster.java:177)
[2023-01-26T18:23:59.209+0000] {spark_submit.py:495} INFO - at com.mongodb.internal.connection.SingleServerCluster.getDescription(SingleServerCluster.java:41)
[2023-01-26T18:23:59.210+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate.getConnectedClusterDescription(MongoClientDelegate.java:147)
[2023-01-26T18:23:59.210+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate.createClientSession(MongoClientDelegate.java:98)
[2023-01-26T18:23:59.210+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.getClientSession(MongoClientDelegate.java:278)
[2023-01-26T18:23:59.210+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:202)
[2023-01-26T18:23:59.210+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:441)
[2023-01-26T18:23:59.210+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:421)
[2023-01-26T18:23:59.210+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$4(MongoSpark.scala:138)
[2023-01-26T18:23:59.210+0000] {spark_submit.py:495} INFO - at scala.collection.Iterator.foreach(Iterator.scala:943)
[2023-01-26T18:23:59.210+0000] {spark_submit.py:495} INFO - at scala.collection.Iterator.foreach$(Iterator.scala:943)
[2023-01-26T18:23:59.210+0000] {spark_submit.py:495} INFO - at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
[2023-01-26T18:23:59.211+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$3(MongoSpark.scala:122)
[2023-01-26T18:23:59.211+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$3$adapted(MongoSpark.scala:121)
[2023-01-26T18:23:59.211+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.$anonfun$withCollectionDo$1(MongoConnector.scala:186)
[2023-01-26T18:23:59.211+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.$anonfun$withDatabaseDo$1(MongoConnector.scala:171)
[2023-01-26T18:23:59.211+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withMongoClientDo(MongoConnector.scala:154)
[2023-01-26T18:23:59.211+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withDatabaseDo(MongoConnector.scala:171)
[2023-01-26T18:23:59.211+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withCollectionDo(MongoConnector.scala:184)
[2023-01-26T18:23:59.211+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$2(MongoSpark.scala:121)
[2023-01-26T18:23:59.211+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$2$adapted(MongoSpark.scala:120)
[2023-01-26T18:23:59.211+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1011)
[2023-01-26T18:23:59.212+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1011)
[2023-01-26T18:23:59.212+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2268)
[2023-01-26T18:23:59.212+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
[2023-01-26T18:23:59.212+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.Task.run(Task.scala:136)
[2023-01-26T18:23:59.212+0000] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)
[2023-01-26T18:23:59.212+0000] {spark_submit.py:495} INFO - at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)
[2023-01-26T18:23:59.212+0000] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)
[2023-01-26T18:23:59.214+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[2023-01-26T18:23:59.214+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[2023-01-26T18:23:59.214+0000] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2023-01-26T18:23:59.214+0000] {spark_submit.py:495} INFO - [0m
[2023-01-26T19:44:22.268+0000] {spark_submit.py:495} INFO - [[34m2023-01-26 19:44:22,267[0m] {[34mclientserver.py:[0m577} INFO[0m - Received command c on object id p0[0m
[2023-01-26T19:44:25.683+0000] {spark_submit.py:495} INFO - -------------------------------------------
[2023-01-26T19:44:25.683+0000] {spark_submit.py:495} INFO - Batch: 4
[2023-01-26T19:44:25.683+0000] {spark_submit.py:495} INFO - -------------------------------------------
[2023-01-26T19:44:25.705+0000] {spark_submit.py:495} INFO - +-------------+--------------------+--------+-------------+--------------------+-------------------+------------+-----------------+--------------------+-------------+
[2023-01-26T19:44:25.705+0000] {spark_submit.py:495} INFO - |           id|       nameOfProduct|   price|ProductStatus|          SellerName|   RegistrationDate|QuantitySold|QuantityAvailable|      Free shipping?|Store ratings|
[2023-01-26T19:44:25.705+0000] {spark_submit.py:495} INFO - +-------------+--------------------+--------+-------------+--------------------+-------------------+------------+-----------------+--------------------+-------------+
[2023-01-26T19:44:25.705+0000] {spark_submit.py:495} INFO - |MLA1221192372|Disco Duro Extern...| 26299.0|          new|    OVERHARD DIGITAL|2015-01-08 12:19:41|         250|              100|Your shipment arr...|          4.5|
[2023-01-26T19:44:25.705+0000] {spark_submit.py:495} INFO - |MLA1157512489|Placa De Video Nv...|124999.0|          new|      PCREGISTRADAOK|2020-11-02 13:17:33|          50|                1|Your shipment arr...|          4.5|
[2023-01-26T19:44:25.705+0000] {spark_submit.py:495} INFO - |MLA1115295971|Procesador Amd Ry...| 93999.0|          new|      PCREGISTRADAOK|2020-11-02 13:17:33|          25|                1|Your shipment arr...|          4.5|
[2023-01-26T19:44:25.705+0000] {spark_submit.py:495} INFO - |MLA1160198557|Memoria Ram Fury ...| 26699.0|          new|    OVERHARD DIGITAL|2015-01-08 12:19:41|         500|               50|Your shipment arr...|          4.5|
[2023-01-26T19:44:25.706+0000] {spark_submit.py:495} INFO - |MLA1148481665|Microprocesador P...| 58899.0|          new|            MALL WEB|2016-05-26 20:28:35|         150|                1|Your shipment arr...|          4.5|
[2023-01-26T19:44:25.706+0000] {spark_submit.py:495} INFO - |MLA1160204806|Memoria Ram Fury ...| 15499.0|          new|    OVERHARD DIGITAL|2015-01-08 12:19:41|         500|              100|Your shipment arr...|          4.5|
[2023-01-26T19:44:25.706+0000] {spark_submit.py:495} INFO - | MLA923637669|Placa De Video Ge...| 28699.0|          new|         INDIGOTRADE|2018-01-05 19:21:16|         500|               50|Your shipment arr...|          4.5|
[2023-01-26T19:44:25.706+0000] {spark_submit.py:495} INFO - |MLA1318244331|Disco Solido Ssd ...| 28799.0|          new|    OVERHARD DIGITAL|2015-01-08 12:19:41|           5|              150|Your shipment arr...|          4.5|
[2023-01-26T19:44:25.706+0000] {spark_submit.py:495} INFO - |MLA1133481354|Disco Sólido Inte...| 13799.0|          new|           DALECLÍCK|2015-03-16 12:07:23|         500|                1|You have to pay t...|          4.5|
[2023-01-26T19:44:25.706+0000] {spark_submit.py:495} INFO - | MLA882277177|Capturadora Video...|  2699.0|          new|         BSK-IMPORTS|2019-12-30 23:07:04|         500|               50|You have to pay t...|          3.0|
[2023-01-26T19:44:25.706+0000] {spark_submit.py:495} INFO - |MLA1283127595|Procesador Intel ...| 51379.0|          new|      PCREGISTRADAOK|2020-11-02 13:17:33|           1|                1|Your shipment arr...|          4.5|
[2023-01-26T19:44:25.706+0000] {spark_submit.py:495} INFO - |MLA1139583771|Disco Sólido Inte...|  8339.0|          new|DIAMONDSYSTEMPADUAML|2015-07-01 12:32:22|         250|              500|You have to pay t...|          4.5|
[2023-01-26T19:44:25.706+0000] {spark_submit.py:495} INFO - |MLA1119996182|Disco Sólido Inte...| 25499.0|          new|            MALL WEB|2016-05-26 20:28:35|         500|                1|Your shipment arr...|          4.5|
[2023-01-26T19:44:25.706+0000] {spark_submit.py:495} INFO - |MLA1183925519|Placa De Video Nv...|299999.0|          new|ESTUDIOSMPSRLESTU...|2020-11-29 18:36:32|          50|                1|Your shipment arr...|          4.5|
[2023-01-26T19:44:25.706+0000] {spark_submit.py:495} INFO - | MLA844489972|Disco Duro Intern...| 11599.0|          new|           DALECLÍCK|2015-03-16 12:07:23|        5000|              500|You have to pay t...|          5.0|
[2023-01-26T19:44:25.706+0000] {spark_submit.py:495} INFO - | MLA833246699|Disco Sólido Inte...| 26999.0|          new|            MALL WEB|2016-05-26 20:28:35|         500|                1|Your shipment arr...|          4.5|
[2023-01-26T19:44:25.706+0000] {spark_submit.py:495} INFO - |MLA1163233412|Placa De Video Nv...| 34659.0|          new|      PCREGISTRADAOK|2020-11-02 13:17:33|           1|                1|Your shipment arr...|          4.5|
[2023-01-26T19:44:25.707+0000] {spark_submit.py:495} INFO - |MLA1314590445|Disco Sólido Inte...|  7999.0|          new|       ELECTRO-SHOWS|2010-03-26 01:08:16|         250|              250|You have to pay t...|          4.5|
[2023-01-26T19:44:25.707+0000] {spark_submit.py:495} INFO - |MLA1302136118|Disco Duro Intern...| 18271.0|          new|DIAMONDSYSTEMSANJ...|2016-04-08 17:21:06|         100|               50|Your shipment arr...|          4.5|
[2023-01-26T19:44:25.707+0000] {spark_submit.py:495} INFO - | MLA902808321|Procesador Gamer ...| 23699.0|          new|            MALL WEB|2016-05-26 20:28:35|         500|                1|Your shipment arr...|          4.5|
[2023-01-26T19:44:25.708+0000] {spark_submit.py:495} INFO - +-------------+--------------------+--------+-------------+--------------------+-------------------+------------+-----------------+--------------------+-------------+
[2023-01-26T19:44:25.708+0000] {spark_submit.py:495} INFO - only showing top 20 rows
[2023-01-26T19:44:25.708+0000] {spark_submit.py:495} INFO - 
[2023-01-26T19:44:53.523+0000] {spark_submit.py:495} INFO - 23/01/26 19:44:53 ERROR Executor: Exception in task 0.0 in stage 10.0 (TID 10)
[2023-01-26T19:44:53.523+0000] {spark_submit.py:495} INFO - com.mongodb.MongoTimeoutException: Timed out after 30000 ms while waiting to connect. Client view of cluster state is {type=UNKNOWN, servers=[{address=172.22.0.2:27017, type=UNKNOWN, state=CONNECTING, exception={com.mongodb.MongoSocketOpenException: Exception opening socket}, caused by {java.net.SocketTimeoutException: connect timed out}}]
[2023-01-26T19:44:53.523+0000] {spark_submit.py:495} INFO - at com.mongodb.internal.connection.BaseCluster.getDescription(BaseCluster.java:177)
[2023-01-26T19:44:53.523+0000] {spark_submit.py:495} INFO - at com.mongodb.internal.connection.SingleServerCluster.getDescription(SingleServerCluster.java:41)
[2023-01-26T19:44:53.523+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate.getConnectedClusterDescription(MongoClientDelegate.java:147)
[2023-01-26T19:44:53.523+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate.createClientSession(MongoClientDelegate.java:98)
[2023-01-26T19:44:53.523+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.getClientSession(MongoClientDelegate.java:278)
[2023-01-26T19:44:53.523+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:202)
[2023-01-26T19:44:53.523+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:441)
[2023-01-26T19:44:53.524+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:421)
[2023-01-26T19:44:53.524+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$4(MongoSpark.scala:138)
[2023-01-26T19:44:53.524+0000] {spark_submit.py:495} INFO - at scala.collection.Iterator.foreach(Iterator.scala:943)
[2023-01-26T19:44:53.524+0000] {spark_submit.py:495} INFO - at scala.collection.Iterator.foreach$(Iterator.scala:943)
[2023-01-26T19:44:53.524+0000] {spark_submit.py:495} INFO - at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
[2023-01-26T19:44:53.524+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$3(MongoSpark.scala:122)
[2023-01-26T19:44:53.524+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$3$adapted(MongoSpark.scala:121)
[2023-01-26T19:44:53.524+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.$anonfun$withCollectionDo$1(MongoConnector.scala:186)
[2023-01-26T19:44:53.524+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.$anonfun$withDatabaseDo$1(MongoConnector.scala:171)
[2023-01-26T19:44:53.524+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withMongoClientDo(MongoConnector.scala:154)
[2023-01-26T19:44:53.524+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withDatabaseDo(MongoConnector.scala:171)
[2023-01-26T19:44:53.524+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withCollectionDo(MongoConnector.scala:184)
[2023-01-26T19:44:53.524+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$2(MongoSpark.scala:121)
[2023-01-26T19:44:53.524+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$2$adapted(MongoSpark.scala:120)
[2023-01-26T19:44:53.525+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1011)
[2023-01-26T19:44:53.525+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1011)
[2023-01-26T19:44:53.525+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2268)
[2023-01-26T19:44:53.525+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
[2023-01-26T19:44:53.525+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.Task.run(Task.scala:136)
[2023-01-26T19:44:53.525+0000] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)
[2023-01-26T19:44:53.525+0000] {spark_submit.py:495} INFO - at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)
[2023-01-26T19:44:53.525+0000] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)
[2023-01-26T19:44:53.525+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[2023-01-26T19:44:53.525+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[2023-01-26T19:44:53.525+0000] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2023-01-26T19:44:53.529+0000] {spark_submit.py:495} INFO - 23/01/26 19:44:53 ERROR TaskSetManager: Task 0 in stage 10.0 failed 1 times; aborting job
[2023-01-26T19:44:53.680+0000] {spark_submit.py:495} INFO - [[34m2023-01-26 19:44:53,679[0m] {[34mconsumerSpark.py:[0m37} ERROR[0m - Error al escribir en Mongo: An error occurred while calling o176.save.
[2023-01-26T19:44:53.680+0000] {spark_submit.py:495} INFO - : org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 10.0 failed 1 times, most recent failure: Lost task 0.0 in stage 10.0 (TID 10) (f1cbb5df32e6 executor driver): com.mongodb.MongoTimeoutException: Timed out after 30000 ms while waiting to connect. Client view of cluster state is {type=UNKNOWN, servers=[{address=172.22.0.2:27017, type=UNKNOWN, state=CONNECTING, exception={com.mongodb.MongoSocketOpenException: Exception opening socket}, caused by {java.net.SocketTimeoutException: connect timed out}}]
[2023-01-26T19:44:53.681+0000] {spark_submit.py:495} INFO - at com.mongodb.internal.connection.BaseCluster.getDescription(BaseCluster.java:177)
[2023-01-26T19:44:53.681+0000] {spark_submit.py:495} INFO - at com.mongodb.internal.connection.SingleServerCluster.getDescription(SingleServerCluster.java:41)
[2023-01-26T19:44:53.681+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate.getConnectedClusterDescription(MongoClientDelegate.java:147)
[2023-01-26T19:44:53.681+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate.createClientSession(MongoClientDelegate.java:98)
[2023-01-26T19:44:53.681+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.getClientSession(MongoClientDelegate.java:278)
[2023-01-26T19:44:53.681+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:202)
[2023-01-26T19:44:53.682+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:441)
[2023-01-26T19:44:53.682+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:421)
[2023-01-26T19:44:53.682+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$4(MongoSpark.scala:138)
[2023-01-26T19:44:53.682+0000] {spark_submit.py:495} INFO - at scala.collection.Iterator.foreach(Iterator.scala:943)
[2023-01-26T19:44:53.682+0000] {spark_submit.py:495} INFO - at scala.collection.Iterator.foreach$(Iterator.scala:943)
[2023-01-26T19:44:53.682+0000] {spark_submit.py:495} INFO - at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
[2023-01-26T19:44:53.682+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$3(MongoSpark.scala:122)
[2023-01-26T19:44:53.682+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$3$adapted(MongoSpark.scala:121)
[2023-01-26T19:44:53.682+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.$anonfun$withCollectionDo$1(MongoConnector.scala:186)
[2023-01-26T19:44:53.682+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.$anonfun$withDatabaseDo$1(MongoConnector.scala:171)
[2023-01-26T19:44:53.682+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withMongoClientDo(MongoConnector.scala:154)
[2023-01-26T19:44:53.682+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withDatabaseDo(MongoConnector.scala:171)
[2023-01-26T19:44:53.682+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withCollectionDo(MongoConnector.scala:184)
[2023-01-26T19:44:53.683+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$2(MongoSpark.scala:121)
[2023-01-26T19:44:53.683+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$2$adapted(MongoSpark.scala:120)
[2023-01-26T19:44:53.683+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1011)
[2023-01-26T19:44:53.683+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1011)
[2023-01-26T19:44:53.683+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2268)
[2023-01-26T19:44:53.683+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
[2023-01-26T19:44:53.683+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.Task.run(Task.scala:136)
[2023-01-26T19:44:53.683+0000] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)
[2023-01-26T19:44:53.683+0000] {spark_submit.py:495} INFO - at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)
[2023-01-26T19:44:53.683+0000] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)
[2023-01-26T19:44:53.683+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[2023-01-26T19:44:53.683+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[2023-01-26T19:44:53.683+0000] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2023-01-26T19:44:53.684+0000] {spark_submit.py:495} INFO - 
[2023-01-26T19:44:53.684+0000] {spark_submit.py:495} INFO - Driver stacktrace:
[2023-01-26T19:44:53.684+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)
[2023-01-26T19:44:53.685+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)
[2023-01-26T19:44:53.685+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)
[2023-01-26T19:44:53.685+0000] {spark_submit.py:495} INFO - at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
[2023-01-26T19:44:53.685+0000] {spark_submit.py:495} INFO - at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
[2023-01-26T19:44:53.686+0000] {spark_submit.py:495} INFO - at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
[2023-01-26T19:44:53.686+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)
[2023-01-26T19:44:53.686+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)
[2023-01-26T19:44:53.686+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)
[2023-01-26T19:44:53.686+0000] {spark_submit.py:495} INFO - at scala.Option.foreach(Option.scala:407)
[2023-01-26T19:44:53.686+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)
[2023-01-26T19:44:53.686+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)
[2023-01-26T19:44:53.686+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)
[2023-01-26T19:44:53.686+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)
[2023-01-26T19:44:53.686+0000] {spark_submit.py:495} INFO - at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
[2023-01-26T19:44:53.686+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)
[2023-01-26T19:44:53.687+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.runJob(SparkContext.scala:2228)
[2023-01-26T19:44:53.687+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.runJob(SparkContext.scala:2249)
[2023-01-26T19:44:53.687+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.runJob(SparkContext.scala:2268)
[2023-01-26T19:44:53.687+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.runJob(SparkContext.scala:2293)
[2023-01-26T19:44:53.687+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$1(RDD.scala:1011)
[2023-01-26T19:44:53.687+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
[2023-01-26T19:44:53.687+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
[2023-01-26T19:44:53.687+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.withScope(RDD.scala:406)
[2023-01-26T19:44:53.687+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.foreachPartition(RDD.scala:1009)
[2023-01-26T19:44:53.687+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.save(MongoSpark.scala:120)
[2023-01-26T19:44:53.687+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.save(MongoSpark.scala:169)
[2023-01-26T19:44:53.688+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.sql.DefaultSource.createRelation(DefaultSource.scala:70)
[2023-01-26T19:44:53.688+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:47)
[2023-01-26T19:44:53.688+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)
[2023-01-26T19:44:53.688+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)
[2023-01-26T19:44:53.688+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)
[2023-01-26T19:44:53.688+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)
[2023-01-26T19:44:53.688+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)
[2023-01-26T19:44:53.688+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)
[2023-01-26T19:44:53.688+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)
[2023-01-26T19:44:53.688+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
[2023-01-26T19:44:53.688+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
[2023-01-26T19:44:53.688+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)
[2023-01-26T19:44:53.688+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)
[2023-01-26T19:44:53.688+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:584)
[2023-01-26T19:44:53.689+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:176)
[2023-01-26T19:44:53.689+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:584)
[2023-01-26T19:44:53.689+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)
[2023-01-26T19:44:53.689+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
[2023-01-26T19:44:53.689+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
[2023-01-26T19:44:53.689+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
[2023-01-26T19:44:53.689+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
[2023-01-26T19:44:53.689+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:560)
[2023-01-26T19:44:53.689+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)
[2023-01-26T19:44:53.689+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)
[2023-01-26T19:44:53.690+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)
[2023-01-26T19:44:53.690+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:116)
[2023-01-26T19:44:53.690+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:860)
[2023-01-26T19:44:53.690+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:390)
[2023-01-26T19:44:53.690+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:363)
[2023-01-26T19:44:53.690+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:247)
[2023-01-26T19:44:53.690+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
[2023-01-26T19:44:53.690+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
[2023-01-26T19:44:53.690+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
[2023-01-26T19:44:53.690+0000] {spark_submit.py:495} INFO - at java.base/java.lang.reflect.Method.invoke(Method.java:566)
[2023-01-26T19:44:53.690+0000] {spark_submit.py:495} INFO - at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
[2023-01-26T19:44:53.690+0000] {spark_submit.py:495} INFO - at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
[2023-01-26T19:44:53.690+0000] {spark_submit.py:495} INFO - at py4j.Gateway.invoke(Gateway.java:282)
[2023-01-26T19:44:53.690+0000] {spark_submit.py:495} INFO - at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
[2023-01-26T19:44:53.691+0000] {spark_submit.py:495} INFO - at py4j.commands.CallCommand.execute(CallCommand.java:79)
[2023-01-26T19:44:53.691+0000] {spark_submit.py:495} INFO - at py4j.ClientServerConnection.sendCommand(ClientServerConnection.java:244)
[2023-01-26T19:44:53.691+0000] {spark_submit.py:495} INFO - at py4j.CallbackClient.sendCommand(CallbackClient.java:384)
[2023-01-26T19:44:53.691+0000] {spark_submit.py:495} INFO - at py4j.CallbackClient.sendCommand(CallbackClient.java:356)
[2023-01-26T19:44:53.691+0000] {spark_submit.py:495} INFO - at py4j.reflection.PythonProxyHandler.invoke(PythonProxyHandler.java:106)
[2023-01-26T19:44:53.691+0000] {spark_submit.py:495} INFO - at com.sun.proxy.$Proxy32.call(Unknown Source)
[2023-01-26T19:44:53.691+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1(ForeachBatchSink.scala:51)
[2023-01-26T19:44:53.691+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1$adapted(ForeachBatchSink.scala:51)
[2023-01-26T19:44:53.691+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.sources.ForeachBatchSink.addBatch(ForeachBatchSink.scala:32)
[2023-01-26T19:44:53.691+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:666)
[2023-01-26T19:44:53.691+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)
[2023-01-26T19:44:53.691+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)
[2023-01-26T19:44:53.691+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)
[2023-01-26T19:44:53.691+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
[2023-01-26T19:44:53.692+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
[2023-01-26T19:44:53.692+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:664)
[2023-01-26T19:44:53.692+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:375)
[2023-01-26T19:44:53.692+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:373)
[2023-01-26T19:44:53.692+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:68)
[2023-01-26T19:44:53.692+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:664)
[2023-01-26T19:44:53.692+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:256)
[2023-01-26T19:44:53.692+0000] {spark_submit.py:495} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2023-01-26T19:44:53.692+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:375)
[2023-01-26T19:44:53.692+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:373)
[2023-01-26T19:44:53.692+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:68)
[2023-01-26T19:44:53.692+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:219)
[2023-01-26T19:44:53.692+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)
[2023-01-26T19:44:53.692+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:213)
[2023-01-26T19:44:53.693+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:307)
[2023-01-26T19:44:53.693+0000] {spark_submit.py:495} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2023-01-26T19:44:53.693+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
[2023-01-26T19:44:53.693+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:285)
[2023-01-26T19:44:53.693+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:208)
[2023-01-26T19:44:53.693+0000] {spark_submit.py:495} INFO - Caused by: com.mongodb.MongoTimeoutException: Timed out after 30000 ms while waiting to connect. Client view of cluster state is {type=UNKNOWN, servers=[{address=172.22.0.2:27017, type=UNKNOWN, state=CONNECTING, exception={com.mongodb.MongoSocketOpenException: Exception opening socket}, caused by {java.net.SocketTimeoutException: connect timed out}}]
[2023-01-26T19:44:53.693+0000] {spark_submit.py:495} INFO - at com.mongodb.internal.connection.BaseCluster.getDescription(BaseCluster.java:177)
[2023-01-26T19:44:53.693+0000] {spark_submit.py:495} INFO - at com.mongodb.internal.connection.SingleServerCluster.getDescription(SingleServerCluster.java:41)
[2023-01-26T19:44:53.693+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate.getConnectedClusterDescription(MongoClientDelegate.java:147)
[2023-01-26T19:44:53.693+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate.createClientSession(MongoClientDelegate.java:98)
[2023-01-26T19:44:53.693+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.getClientSession(MongoClientDelegate.java:278)
[2023-01-26T19:44:53.693+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:202)
[2023-01-26T19:44:53.693+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:441)
[2023-01-26T19:44:53.693+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:421)
[2023-01-26T19:44:53.693+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$4(MongoSpark.scala:138)
[2023-01-26T19:44:53.694+0000] {spark_submit.py:495} INFO - at scala.collection.Iterator.foreach(Iterator.scala:943)
[2023-01-26T19:44:53.694+0000] {spark_submit.py:495} INFO - at scala.collection.Iterator.foreach$(Iterator.scala:943)
[2023-01-26T19:44:53.694+0000] {spark_submit.py:495} INFO - at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
[2023-01-26T19:44:53.694+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$3(MongoSpark.scala:122)
[2023-01-26T19:44:53.694+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$3$adapted(MongoSpark.scala:121)
[2023-01-26T19:44:53.694+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.$anonfun$withCollectionDo$1(MongoConnector.scala:186)
[2023-01-26T19:44:53.694+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.$anonfun$withDatabaseDo$1(MongoConnector.scala:171)
[2023-01-26T19:44:53.694+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withMongoClientDo(MongoConnector.scala:154)
[2023-01-26T19:44:53.694+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withDatabaseDo(MongoConnector.scala:171)
[2023-01-26T19:44:53.694+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withCollectionDo(MongoConnector.scala:184)
[2023-01-26T19:44:53.694+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$2(MongoSpark.scala:121)
[2023-01-26T19:44:53.694+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$2$adapted(MongoSpark.scala:120)
[2023-01-26T19:44:53.694+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1011)
[2023-01-26T19:44:53.694+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1011)
[2023-01-26T19:44:53.695+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2268)
[2023-01-26T19:44:53.695+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
[2023-01-26T19:44:53.695+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.Task.run(Task.scala:136)
[2023-01-26T19:44:53.695+0000] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)
[2023-01-26T19:44:53.695+0000] {spark_submit.py:495} INFO - at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)
[2023-01-26T19:44:53.695+0000] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)
[2023-01-26T19:44:53.695+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[2023-01-26T19:44:53.695+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[2023-01-26T19:44:53.695+0000] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2023-01-26T19:44:53.695+0000] {spark_submit.py:495} INFO - [0m
[2023-01-26T19:44:53.814+0000] {spark_submit.py:495} INFO - [[34m2023-01-26 19:44:53,814[0m] {[34mclientserver.py:[0m577} INFO[0m - Received command c on object id p0[0m
[2023-01-26T19:45:23.885+0000] {spark_submit.py:495} INFO - 23/01/26 19:45:23 ERROR Executor: Exception in task 0.0 in stage 12.0 (TID 12)
[2023-01-26T19:45:23.885+0000] {spark_submit.py:495} INFO - com.mongodb.MongoTimeoutException: Timed out after 30000 ms while waiting to connect. Client view of cluster state is {type=UNKNOWN, servers=[{address=172.22.0.2:27017, type=UNKNOWN, state=CONNECTING, exception={com.mongodb.MongoSocketOpenException: Exception opening socket}, caused by {java.net.SocketTimeoutException: connect timed out}}]
[2023-01-26T19:45:23.886+0000] {spark_submit.py:495} INFO - at com.mongodb.internal.connection.BaseCluster.getDescription(BaseCluster.java:177)
[2023-01-26T19:45:23.886+0000] {spark_submit.py:495} INFO - at com.mongodb.internal.connection.SingleServerCluster.getDescription(SingleServerCluster.java:41)
[2023-01-26T19:45:23.886+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate.getConnectedClusterDescription(MongoClientDelegate.java:147)
[2023-01-26T19:45:23.886+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate.createClientSession(MongoClientDelegate.java:98)
[2023-01-26T19:45:23.886+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.getClientSession(MongoClientDelegate.java:278)
[2023-01-26T19:45:23.886+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:202)
[2023-01-26T19:45:23.886+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:441)
[2023-01-26T19:45:23.886+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:421)
[2023-01-26T19:45:23.886+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$4(MongoSpark.scala:138)
[2023-01-26T19:45:23.886+0000] {spark_submit.py:495} INFO - at scala.collection.Iterator.foreach(Iterator.scala:943)
[2023-01-26T19:45:23.886+0000] {spark_submit.py:495} INFO - at scala.collection.Iterator.foreach$(Iterator.scala:943)
[2023-01-26T19:45:23.886+0000] {spark_submit.py:495} INFO - at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
[2023-01-26T19:45:23.886+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$3(MongoSpark.scala:122)
[2023-01-26T19:45:23.886+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$3$adapted(MongoSpark.scala:121)
[2023-01-26T19:45:23.887+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.$anonfun$withCollectionDo$1(MongoConnector.scala:186)
[2023-01-26T19:45:23.887+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.$anonfun$withDatabaseDo$1(MongoConnector.scala:171)
[2023-01-26T19:45:23.887+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withMongoClientDo(MongoConnector.scala:154)
[2023-01-26T19:45:23.887+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withDatabaseDo(MongoConnector.scala:171)
[2023-01-26T19:45:23.887+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withCollectionDo(MongoConnector.scala:184)
[2023-01-26T19:45:23.887+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$2(MongoSpark.scala:121)
[2023-01-26T19:45:23.887+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$2$adapted(MongoSpark.scala:120)
[2023-01-26T19:45:23.887+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1011)
[2023-01-26T19:45:23.887+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1011)
[2023-01-26T19:45:23.887+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2268)
[2023-01-26T19:45:23.887+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
[2023-01-26T19:45:23.887+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.Task.run(Task.scala:136)
[2023-01-26T19:45:23.887+0000] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)
[2023-01-26T19:45:23.887+0000] {spark_submit.py:495} INFO - at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)
[2023-01-26T19:45:23.887+0000] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)
[2023-01-26T19:45:23.888+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[2023-01-26T19:45:23.888+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[2023-01-26T19:45:23.888+0000] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2023-01-26T19:45:23.897+0000] {spark_submit.py:495} INFO - 23/01/26 19:45:23 ERROR TaskSetManager: Task 0 in stage 12.0 failed 1 times; aborting job
[2023-01-26T19:45:24.052+0000] {spark_submit.py:495} INFO - [[34m2023-01-26 19:45:24,052[0m] {[34mconsumerSpark.py:[0m37} ERROR[0m - Error al escribir en Mongo: An error occurred while calling o185.save.
[2023-01-26T19:45:24.053+0000] {spark_submit.py:495} INFO - : org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 12.0 failed 1 times, most recent failure: Lost task 0.0 in stage 12.0 (TID 12) (f1cbb5df32e6 executor driver): com.mongodb.MongoTimeoutException: Timed out after 30000 ms while waiting to connect. Client view of cluster state is {type=UNKNOWN, servers=[{address=172.22.0.2:27017, type=UNKNOWN, state=CONNECTING, exception={com.mongodb.MongoSocketOpenException: Exception opening socket}, caused by {java.net.SocketTimeoutException: connect timed out}}]
[2023-01-26T19:45:24.053+0000] {spark_submit.py:495} INFO - at com.mongodb.internal.connection.BaseCluster.getDescription(BaseCluster.java:177)
[2023-01-26T19:45:24.053+0000] {spark_submit.py:495} INFO - at com.mongodb.internal.connection.SingleServerCluster.getDescription(SingleServerCluster.java:41)
[2023-01-26T19:45:24.054+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate.getConnectedClusterDescription(MongoClientDelegate.java:147)
[2023-01-26T19:45:24.054+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate.createClientSession(MongoClientDelegate.java:98)
[2023-01-26T19:45:24.054+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.getClientSession(MongoClientDelegate.java:278)
[2023-01-26T19:45:24.054+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:202)
[2023-01-26T19:45:24.054+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:441)
[2023-01-26T19:45:24.054+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:421)
[2023-01-26T19:45:24.054+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$4(MongoSpark.scala:138)
[2023-01-26T19:45:24.054+0000] {spark_submit.py:495} INFO - at scala.collection.Iterator.foreach(Iterator.scala:943)
[2023-01-26T19:45:24.054+0000] {spark_submit.py:495} INFO - at scala.collection.Iterator.foreach$(Iterator.scala:943)
[2023-01-26T19:45:24.054+0000] {spark_submit.py:495} INFO - at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
[2023-01-26T19:45:24.054+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$3(MongoSpark.scala:122)
[2023-01-26T19:45:24.054+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$3$adapted(MongoSpark.scala:121)
[2023-01-26T19:45:24.054+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.$anonfun$withCollectionDo$1(MongoConnector.scala:186)
[2023-01-26T19:45:24.055+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.$anonfun$withDatabaseDo$1(MongoConnector.scala:171)
[2023-01-26T19:45:24.055+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withMongoClientDo(MongoConnector.scala:154)
[2023-01-26T19:45:24.055+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withDatabaseDo(MongoConnector.scala:171)
[2023-01-26T19:45:24.055+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withCollectionDo(MongoConnector.scala:184)
[2023-01-26T19:45:24.055+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$2(MongoSpark.scala:121)
[2023-01-26T19:45:24.055+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$2$adapted(MongoSpark.scala:120)
[2023-01-26T19:45:24.055+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1011)
[2023-01-26T19:45:24.055+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1011)
[2023-01-26T19:45:24.055+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2268)
[2023-01-26T19:45:24.055+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
[2023-01-26T19:45:24.055+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.Task.run(Task.scala:136)
[2023-01-26T19:45:24.055+0000] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)
[2023-01-26T19:45:24.055+0000] {spark_submit.py:495} INFO - at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)
[2023-01-26T19:45:24.056+0000] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)
[2023-01-26T19:45:24.056+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[2023-01-26T19:45:24.056+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[2023-01-26T19:45:24.056+0000] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2023-01-26T19:45:24.056+0000] {spark_submit.py:495} INFO - 
[2023-01-26T19:45:24.056+0000] {spark_submit.py:495} INFO - Driver stacktrace:
[2023-01-26T19:45:24.056+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)
[2023-01-26T19:45:24.056+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)
[2023-01-26T19:45:24.056+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)
[2023-01-26T19:45:24.056+0000] {spark_submit.py:495} INFO - at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
[2023-01-26T19:45:24.056+0000] {spark_submit.py:495} INFO - at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
[2023-01-26T19:45:24.056+0000] {spark_submit.py:495} INFO - at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
[2023-01-26T19:45:24.056+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)
[2023-01-26T19:45:24.057+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)
[2023-01-26T19:45:24.057+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)
[2023-01-26T19:45:24.057+0000] {spark_submit.py:495} INFO - at scala.Option.foreach(Option.scala:407)
[2023-01-26T19:45:24.057+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)
[2023-01-26T19:45:24.057+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)
[2023-01-26T19:45:24.057+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)
[2023-01-26T19:45:24.057+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)
[2023-01-26T19:45:24.057+0000] {spark_submit.py:495} INFO - at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
[2023-01-26T19:45:24.057+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)
[2023-01-26T19:45:24.057+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.runJob(SparkContext.scala:2228)
[2023-01-26T19:45:24.057+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.runJob(SparkContext.scala:2249)
[2023-01-26T19:45:24.057+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.runJob(SparkContext.scala:2268)
[2023-01-26T19:45:24.057+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.runJob(SparkContext.scala:2293)
[2023-01-26T19:45:24.058+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$1(RDD.scala:1011)
[2023-01-26T19:45:24.058+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
[2023-01-26T19:45:24.058+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
[2023-01-26T19:45:24.058+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.withScope(RDD.scala:406)
[2023-01-26T19:45:24.058+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.foreachPartition(RDD.scala:1009)
[2023-01-26T19:45:24.058+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.save(MongoSpark.scala:120)
[2023-01-26T19:45:24.058+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.save(MongoSpark.scala:169)
[2023-01-26T19:45:24.058+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.sql.DefaultSource.createRelation(DefaultSource.scala:70)
[2023-01-26T19:45:24.058+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:47)
[2023-01-26T19:45:24.059+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)
[2023-01-26T19:45:24.059+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)
[2023-01-26T19:45:24.059+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)
[2023-01-26T19:45:24.059+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)
[2023-01-26T19:45:24.059+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)
[2023-01-26T19:45:24.059+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)
[2023-01-26T19:45:24.059+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)
[2023-01-26T19:45:24.059+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
[2023-01-26T19:45:24.059+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
[2023-01-26T19:45:24.059+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)
[2023-01-26T19:45:24.059+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)
[2023-01-26T19:45:24.059+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:584)
[2023-01-26T19:45:24.059+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:176)
[2023-01-26T19:45:24.059+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:584)
[2023-01-26T19:45:24.060+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)
[2023-01-26T19:45:24.060+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
[2023-01-26T19:45:24.060+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
[2023-01-26T19:45:24.060+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
[2023-01-26T19:45:24.060+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
[2023-01-26T19:45:24.060+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:560)
[2023-01-26T19:45:24.060+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)
[2023-01-26T19:45:24.060+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)
[2023-01-26T19:45:24.061+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)
[2023-01-26T19:45:24.061+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:116)
[2023-01-26T19:45:24.061+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:860)
[2023-01-26T19:45:24.061+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:390)
[2023-01-26T19:45:24.061+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:363)
[2023-01-26T19:45:24.061+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:247)
[2023-01-26T19:45:24.061+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
[2023-01-26T19:45:24.061+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
[2023-01-26T19:45:24.061+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
[2023-01-26T19:45:24.062+0000] {spark_submit.py:495} INFO - at java.base/java.lang.reflect.Method.invoke(Method.java:566)
[2023-01-26T19:45:24.062+0000] {spark_submit.py:495} INFO - at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
[2023-01-26T19:45:24.062+0000] {spark_submit.py:495} INFO - at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
[2023-01-26T19:45:24.062+0000] {spark_submit.py:495} INFO - at py4j.Gateway.invoke(Gateway.java:282)
[2023-01-26T19:45:24.062+0000] {spark_submit.py:495} INFO - at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
[2023-01-26T19:45:24.062+0000] {spark_submit.py:495} INFO - at py4j.commands.CallCommand.execute(CallCommand.java:79)
[2023-01-26T19:45:24.062+0000] {spark_submit.py:495} INFO - at py4j.ClientServerConnection.sendCommand(ClientServerConnection.java:244)
[2023-01-26T19:45:24.062+0000] {spark_submit.py:495} INFO - at py4j.CallbackClient.sendCommand(CallbackClient.java:384)
[2023-01-26T19:45:24.062+0000] {spark_submit.py:495} INFO - at py4j.CallbackClient.sendCommand(CallbackClient.java:356)
[2023-01-26T19:45:24.062+0000] {spark_submit.py:495} INFO - at py4j.reflection.PythonProxyHandler.invoke(PythonProxyHandler.java:106)
[2023-01-26T19:45:24.062+0000] {spark_submit.py:495} INFO - at com.sun.proxy.$Proxy32.call(Unknown Source)
[2023-01-26T19:45:24.062+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1(ForeachBatchSink.scala:51)
[2023-01-26T19:45:24.062+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1$adapted(ForeachBatchSink.scala:51)
[2023-01-26T19:45:24.063+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.sources.ForeachBatchSink.addBatch(ForeachBatchSink.scala:32)
[2023-01-26T19:45:24.063+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:666)
[2023-01-26T19:45:24.063+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)
[2023-01-26T19:45:24.063+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)
[2023-01-26T19:45:24.063+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)
[2023-01-26T19:45:24.063+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
[2023-01-26T19:45:24.063+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
[2023-01-26T19:45:24.063+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:664)
[2023-01-26T19:45:24.063+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:375)
[2023-01-26T19:45:24.063+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:373)
[2023-01-26T19:45:24.063+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:68)
[2023-01-26T19:45:24.063+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:664)
[2023-01-26T19:45:24.063+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:256)
[2023-01-26T19:45:24.063+0000] {spark_submit.py:495} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2023-01-26T19:45:24.063+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:375)
[2023-01-26T19:45:24.064+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:373)
[2023-01-26T19:45:24.064+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:68)
[2023-01-26T19:45:24.064+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:219)
[2023-01-26T19:45:24.064+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)
[2023-01-26T19:45:24.064+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:213)
[2023-01-26T19:45:24.064+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:307)
[2023-01-26T19:45:24.064+0000] {spark_submit.py:495} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2023-01-26T19:45:24.064+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
[2023-01-26T19:45:24.064+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:285)
[2023-01-26T19:45:24.064+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:208)
[2023-01-26T19:45:24.064+0000] {spark_submit.py:495} INFO - Caused by: com.mongodb.MongoTimeoutException: Timed out after 30000 ms while waiting to connect. Client view of cluster state is {type=UNKNOWN, servers=[{address=172.22.0.2:27017, type=UNKNOWN, state=CONNECTING, exception={com.mongodb.MongoSocketOpenException: Exception opening socket}, caused by {java.net.SocketTimeoutException: connect timed out}}]
[2023-01-26T19:45:24.064+0000] {spark_submit.py:495} INFO - at com.mongodb.internal.connection.BaseCluster.getDescription(BaseCluster.java:177)
[2023-01-26T19:45:24.064+0000] {spark_submit.py:495} INFO - at com.mongodb.internal.connection.SingleServerCluster.getDescription(SingleServerCluster.java:41)
[2023-01-26T19:45:24.064+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate.getConnectedClusterDescription(MongoClientDelegate.java:147)
[2023-01-26T19:45:24.065+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate.createClientSession(MongoClientDelegate.java:98)
[2023-01-26T19:45:24.065+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.getClientSession(MongoClientDelegate.java:278)
[2023-01-26T19:45:24.065+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:202)
[2023-01-26T19:45:24.065+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:441)
[2023-01-26T19:45:24.065+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:421)
[2023-01-26T19:45:24.065+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$4(MongoSpark.scala:138)
[2023-01-26T19:45:24.065+0000] {spark_submit.py:495} INFO - at scala.collection.Iterator.foreach(Iterator.scala:943)
[2023-01-26T19:45:24.065+0000] {spark_submit.py:495} INFO - at scala.collection.Iterator.foreach$(Iterator.scala:943)
[2023-01-26T19:45:24.065+0000] {spark_submit.py:495} INFO - at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
[2023-01-26T19:45:24.065+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$3(MongoSpark.scala:122)
[2023-01-26T19:45:24.065+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$3$adapted(MongoSpark.scala:121)
[2023-01-26T19:45:24.065+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.$anonfun$withCollectionDo$1(MongoConnector.scala:186)
[2023-01-26T19:45:24.066+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.$anonfun$withDatabaseDo$1(MongoConnector.scala:171)
[2023-01-26T19:45:24.066+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withMongoClientDo(MongoConnector.scala:154)
[2023-01-26T19:45:24.066+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withDatabaseDo(MongoConnector.scala:171)
[2023-01-26T19:45:24.066+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withCollectionDo(MongoConnector.scala:184)
[2023-01-26T19:45:24.066+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$2(MongoSpark.scala:121)
[2023-01-26T19:45:24.066+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$2$adapted(MongoSpark.scala:120)
[2023-01-26T19:45:24.066+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1011)
[2023-01-26T19:45:24.066+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1011)
[2023-01-26T19:45:24.066+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2268)
[2023-01-26T19:45:24.066+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
[2023-01-26T19:45:24.066+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.Task.run(Task.scala:136)
[2023-01-26T19:45:24.066+0000] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)
[2023-01-26T19:45:24.066+0000] {spark_submit.py:495} INFO - at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)
[2023-01-26T19:45:24.066+0000] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)
[2023-01-26T19:45:24.067+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[2023-01-26T19:45:24.067+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[2023-01-26T19:45:24.067+0000] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2023-01-26T19:45:24.070+0000] {spark_submit.py:495} INFO - [0m
[2023-01-26T19:47:30.098+0000] {spark_submit.py:495} INFO - [[34m2023-01-26 19:47:30,098[0m] {[34mclientserver.py:[0m577} INFO[0m - Received command c on object id p0[0m
[2023-01-26T19:47:30.883+0000] {spark_submit.py:495} INFO - -------------------------------------------
[2023-01-26T19:47:30.883+0000] {spark_submit.py:495} INFO - Batch: 5
[2023-01-26T19:47:30.883+0000] {spark_submit.py:495} INFO - -------------------------------------------
[2023-01-26T19:47:30.925+0000] {spark_submit.py:495} INFO - +-------------+--------------------+--------+-------------+--------------------+-------------------+------------+-----------------+--------------------+-------------+
[2023-01-26T19:47:30.925+0000] {spark_submit.py:495} INFO - |           id|       nameOfProduct|   price|ProductStatus|          SellerName|   RegistrationDate|QuantitySold|QuantityAvailable|      Free shipping?|Store ratings|
[2023-01-26T19:47:30.925+0000] {spark_submit.py:495} INFO - +-------------+--------------------+--------+-------------+--------------------+-------------------+------------+-----------------+--------------------+-------------+
[2023-01-26T19:47:30.925+0000] {spark_submit.py:495} INFO - |MLA1221192372|Disco Duro Extern...| 26299.0|          new|    OVERHARD DIGITAL|2015-01-08 12:19:41|         250|              100|Your shipment arr...|          4.5|
[2023-01-26T19:47:30.925+0000] {spark_submit.py:495} INFO - |MLA1157512489|Placa De Video Nv...|124999.0|          new|      PCREGISTRADAOK|2020-11-02 13:17:33|          50|                1|Your shipment arr...|          4.5|
[2023-01-26T19:47:30.926+0000] {spark_submit.py:495} INFO - |MLA1115295971|Procesador Amd Ry...| 93999.0|          new|      PCREGISTRADAOK|2020-11-02 13:17:33|          25|                1|Your shipment arr...|          4.5|
[2023-01-26T19:47:30.926+0000] {spark_submit.py:495} INFO - |MLA1160198557|Memoria Ram Fury ...| 26699.0|          new|    OVERHARD DIGITAL|2015-01-08 12:19:41|         500|               50|Your shipment arr...|          4.5|
[2023-01-26T19:47:30.926+0000] {spark_submit.py:495} INFO - |MLA1148481665|Microprocesador P...| 58899.0|          new|            MALL WEB|2016-05-26 20:28:35|         150|                1|Your shipment arr...|          4.5|
[2023-01-26T19:47:30.926+0000] {spark_submit.py:495} INFO - |MLA1160204806|Memoria Ram Fury ...| 15499.0|          new|    OVERHARD DIGITAL|2015-01-08 12:19:41|         500|              100|Your shipment arr...|          4.5|
[2023-01-26T19:47:30.926+0000] {spark_submit.py:495} INFO - | MLA923637669|Placa De Video Ge...| 28699.0|          new|         INDIGOTRADE|2018-01-05 19:21:16|         500|               50|Your shipment arr...|          4.5|
[2023-01-26T19:47:30.927+0000] {spark_submit.py:495} INFO - |MLA1318244331|Disco Solido Ssd ...| 28799.0|          new|    OVERHARD DIGITAL|2015-01-08 12:19:41|           5|              150|Your shipment arr...|          4.5|
[2023-01-26T19:47:30.927+0000] {spark_submit.py:495} INFO - |MLA1133481354|Disco Sólido Inte...| 13799.0|          new|           DALECLÍCK|2015-03-16 12:07:23|         500|                1|You have to pay t...|          4.5|
[2023-01-26T19:47:30.927+0000] {spark_submit.py:495} INFO - | MLA882277177|Capturadora Video...|  2699.0|          new|         BSK-IMPORTS|2019-12-30 23:07:04|         500|               50|You have to pay t...|          3.0|
[2023-01-26T19:47:30.927+0000] {spark_submit.py:495} INFO - |MLA1283127595|Procesador Intel ...| 51379.0|          new|      PCREGISTRADAOK|2020-11-02 13:17:33|           1|                1|Your shipment arr...|          4.5|
[2023-01-26T19:47:30.927+0000] {spark_submit.py:495} INFO - |MLA1139583771|Disco Sólido Inte...|  8339.0|          new|DIAMONDSYSTEMPADUAML|2015-07-01 12:32:22|         250|              500|You have to pay t...|          4.5|
[2023-01-26T19:47:30.927+0000] {spark_submit.py:495} INFO - |MLA1119996182|Disco Sólido Inte...| 25499.0|          new|            MALL WEB|2016-05-26 20:28:35|         500|                1|Your shipment arr...|          4.5|
[2023-01-26T19:47:30.927+0000] {spark_submit.py:495} INFO - |MLA1183925519|Placa De Video Nv...|299999.0|          new|ESTUDIOSMPSRLESTU...|2020-11-29 18:36:32|          50|                1|Your shipment arr...|          4.5|
[2023-01-26T19:47:30.927+0000] {spark_submit.py:495} INFO - | MLA844489972|Disco Duro Intern...| 11599.0|          new|           DALECLÍCK|2015-03-16 12:07:23|        5000|              500|You have to pay t...|          5.0|
[2023-01-26T19:47:30.927+0000] {spark_submit.py:495} INFO - | MLA833246699|Disco Sólido Inte...| 26999.0|          new|            MALL WEB|2016-05-26 20:28:35|         500|                1|Your shipment arr...|          4.5|
[2023-01-26T19:47:30.927+0000] {spark_submit.py:495} INFO - |MLA1163233412|Placa De Video Nv...| 34659.0|          new|      PCREGISTRADAOK|2020-11-02 13:17:33|           1|                1|Your shipment arr...|          4.5|
[2023-01-26T19:47:30.927+0000] {spark_submit.py:495} INFO - |MLA1314590445|Disco Sólido Inte...|  7999.0|          new|       ELECTRO-SHOWS|2010-03-26 01:08:16|         250|              250|You have to pay t...|          4.5|
[2023-01-26T19:47:30.927+0000] {spark_submit.py:495} INFO - |MLA1302136118|Disco Duro Intern...| 18271.0|          new|DIAMONDSYSTEMSANJ...|2016-04-08 17:21:06|         100|               50|Your shipment arr...|          4.5|
[2023-01-26T19:47:30.927+0000] {spark_submit.py:495} INFO - | MLA902808321|Procesador Gamer ...| 23699.0|          new|            MALL WEB|2016-05-26 20:28:35|         500|                1|Your shipment arr...|          4.5|
[2023-01-26T19:47:30.928+0000] {spark_submit.py:495} INFO - +-------------+--------------------+--------+-------------+--------------------+-------------------+------------+-----------------+--------------------+-------------+
[2023-01-26T19:47:30.928+0000] {spark_submit.py:495} INFO - only showing top 20 rows
[2023-01-26T19:47:30.928+0000] {spark_submit.py:495} INFO - 
[2023-01-26T19:48:00.789+0000] {spark_submit.py:495} INFO - 23/01/26 19:48:00 ERROR Executor: Exception in task 0.0 in stage 13.0 (TID 13)
[2023-01-26T19:48:00.789+0000] {spark_submit.py:495} INFO - com.mongodb.MongoTimeoutException: Timed out after 30000 ms while waiting to connect. Client view of cluster state is {type=UNKNOWN, servers=[{address=172.22.0.2:27017, type=UNKNOWN, state=CONNECTING, exception={com.mongodb.MongoSocketOpenException: Exception opening socket}, caused by {java.net.SocketTimeoutException: connect timed out}}]
[2023-01-26T19:48:00.790+0000] {spark_submit.py:495} INFO - at com.mongodb.internal.connection.BaseCluster.getDescription(BaseCluster.java:177)
[2023-01-26T19:48:00.790+0000] {spark_submit.py:495} INFO - at com.mongodb.internal.connection.SingleServerCluster.getDescription(SingleServerCluster.java:41)
[2023-01-26T19:48:00.790+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate.getConnectedClusterDescription(MongoClientDelegate.java:147)
[2023-01-26T19:48:00.791+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate.createClientSession(MongoClientDelegate.java:98)
[2023-01-26T19:48:00.791+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.getClientSession(MongoClientDelegate.java:278)
[2023-01-26T19:48:00.792+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:202)
[2023-01-26T19:48:00.792+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:441)
[2023-01-26T19:48:00.793+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:421)
[2023-01-26T19:48:00.793+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$4(MongoSpark.scala:138)
[2023-01-26T19:48:00.793+0000] {spark_submit.py:495} INFO - at scala.collection.Iterator.foreach(Iterator.scala:943)
[2023-01-26T19:48:00.794+0000] {spark_submit.py:495} INFO - at scala.collection.Iterator.foreach$(Iterator.scala:943)
[2023-01-26T19:48:00.794+0000] {spark_submit.py:495} INFO - at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
[2023-01-26T19:48:00.795+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$3(MongoSpark.scala:122)
[2023-01-26T19:48:00.795+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$3$adapted(MongoSpark.scala:121)
[2023-01-26T19:48:00.795+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.$anonfun$withCollectionDo$1(MongoConnector.scala:186)
[2023-01-26T19:48:00.796+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.$anonfun$withDatabaseDo$1(MongoConnector.scala:171)
[2023-01-26T19:48:00.796+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withMongoClientDo(MongoConnector.scala:154)
[2023-01-26T19:48:00.796+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withDatabaseDo(MongoConnector.scala:171)
[2023-01-26T19:48:00.797+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withCollectionDo(MongoConnector.scala:184)
[2023-01-26T19:48:00.797+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$2(MongoSpark.scala:121)
[2023-01-26T19:48:00.797+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$2$adapted(MongoSpark.scala:120)
[2023-01-26T19:48:00.798+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1011)
[2023-01-26T19:48:00.798+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1011)
[2023-01-26T19:48:00.798+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2268)
[2023-01-26T19:48:00.799+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
[2023-01-26T19:48:00.799+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.Task.run(Task.scala:136)
[2023-01-26T19:48:00.799+0000] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)
[2023-01-26T19:48:00.800+0000] {spark_submit.py:495} INFO - at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)
[2023-01-26T19:48:00.800+0000] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)
[2023-01-26T19:48:00.801+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[2023-01-26T19:48:00.807+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[2023-01-26T19:48:00.807+0000] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2023-01-26T19:48:00.808+0000] {spark_submit.py:495} INFO - 23/01/26 19:48:00 ERROR TaskSetManager: Task 0 in stage 13.0 failed 1 times; aborting job
[2023-01-26T19:48:00.941+0000] {spark_submit.py:495} INFO - [[34m2023-01-26 19:48:00,941[0m] {[34mconsumerSpark.py:[0m37} ERROR[0m - Error al escribir en Mongo: An error occurred while calling o194.save.
[2023-01-26T19:48:00.942+0000] {spark_submit.py:495} INFO - : org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 13.0 failed 1 times, most recent failure: Lost task 0.0 in stage 13.0 (TID 13) (f1cbb5df32e6 executor driver): com.mongodb.MongoTimeoutException: Timed out after 30000 ms while waiting to connect. Client view of cluster state is {type=UNKNOWN, servers=[{address=172.22.0.2:27017, type=UNKNOWN, state=CONNECTING, exception={com.mongodb.MongoSocketOpenException: Exception opening socket}, caused by {java.net.SocketTimeoutException: connect timed out}}]
[2023-01-26T19:48:00.942+0000] {spark_submit.py:495} INFO - at com.mongodb.internal.connection.BaseCluster.getDescription(BaseCluster.java:177)
[2023-01-26T19:48:00.943+0000] {spark_submit.py:495} INFO - at com.mongodb.internal.connection.SingleServerCluster.getDescription(SingleServerCluster.java:41)
[2023-01-26T19:48:00.943+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate.getConnectedClusterDescription(MongoClientDelegate.java:147)
[2023-01-26T19:48:00.944+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate.createClientSession(MongoClientDelegate.java:98)
[2023-01-26T19:48:00.944+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.getClientSession(MongoClientDelegate.java:278)
[2023-01-26T19:48:00.944+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:202)
[2023-01-26T19:48:00.945+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:441)
[2023-01-26T19:48:00.945+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:421)
[2023-01-26T19:48:00.945+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$4(MongoSpark.scala:138)
[2023-01-26T19:48:00.946+0000] {spark_submit.py:495} INFO - at scala.collection.Iterator.foreach(Iterator.scala:943)
[2023-01-26T19:48:00.946+0000] {spark_submit.py:495} INFO - at scala.collection.Iterator.foreach$(Iterator.scala:943)
[2023-01-26T19:48:00.947+0000] {spark_submit.py:495} INFO - at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
[2023-01-26T19:48:00.947+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$3(MongoSpark.scala:122)
[2023-01-26T19:48:00.947+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$3$adapted(MongoSpark.scala:121)
[2023-01-26T19:48:00.948+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.$anonfun$withCollectionDo$1(MongoConnector.scala:186)
[2023-01-26T19:48:00.948+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.$anonfun$withDatabaseDo$1(MongoConnector.scala:171)
[2023-01-26T19:48:00.948+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withMongoClientDo(MongoConnector.scala:154)
[2023-01-26T19:48:00.949+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withDatabaseDo(MongoConnector.scala:171)
[2023-01-26T19:48:00.949+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withCollectionDo(MongoConnector.scala:184)
[2023-01-26T19:48:00.949+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$2(MongoSpark.scala:121)
[2023-01-26T19:48:00.950+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$2$adapted(MongoSpark.scala:120)
[2023-01-26T19:48:00.950+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1011)
[2023-01-26T19:48:00.950+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1011)
[2023-01-26T19:48:00.951+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2268)
[2023-01-26T19:48:00.951+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
[2023-01-26T19:48:00.951+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.Task.run(Task.scala:136)
[2023-01-26T19:48:00.952+0000] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)
[2023-01-26T19:48:00.952+0000] {spark_submit.py:495} INFO - at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)
[2023-01-26T19:48:00.953+0000] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)
[2023-01-26T19:48:00.953+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[2023-01-26T19:48:00.953+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[2023-01-26T19:48:00.954+0000] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2023-01-26T19:48:00.954+0000] {spark_submit.py:495} INFO - 
[2023-01-26T19:48:00.955+0000] {spark_submit.py:495} INFO - Driver stacktrace:
[2023-01-26T19:48:00.955+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)
[2023-01-26T19:48:00.955+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)
[2023-01-26T19:48:00.956+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)
[2023-01-26T19:48:00.956+0000] {spark_submit.py:495} INFO - at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
[2023-01-26T19:48:00.956+0000] {spark_submit.py:495} INFO - at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
[2023-01-26T19:48:00.957+0000] {spark_submit.py:495} INFO - at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
[2023-01-26T19:48:00.957+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)
[2023-01-26T19:48:00.957+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)
[2023-01-26T19:48:00.958+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)
[2023-01-26T19:48:00.958+0000] {spark_submit.py:495} INFO - at scala.Option.foreach(Option.scala:407)
[2023-01-26T19:48:00.958+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)
[2023-01-26T19:48:00.959+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)
[2023-01-26T19:48:00.959+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)
[2023-01-26T19:48:00.959+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)
[2023-01-26T19:48:00.960+0000] {spark_submit.py:495} INFO - at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
[2023-01-26T19:48:00.960+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)
[2023-01-26T19:48:00.961+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.runJob(SparkContext.scala:2228)
[2023-01-26T19:48:00.961+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.runJob(SparkContext.scala:2249)
[2023-01-26T19:48:00.961+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.runJob(SparkContext.scala:2268)
[2023-01-26T19:48:00.962+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.runJob(SparkContext.scala:2293)
[2023-01-26T19:48:00.962+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$1(RDD.scala:1011)
[2023-01-26T19:48:00.962+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
[2023-01-26T19:48:00.963+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
[2023-01-26T19:48:00.963+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.withScope(RDD.scala:406)
[2023-01-26T19:48:00.963+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.foreachPartition(RDD.scala:1009)
[2023-01-26T19:48:00.964+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.save(MongoSpark.scala:120)
[2023-01-26T19:48:00.964+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.save(MongoSpark.scala:169)
[2023-01-26T19:48:00.965+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.sql.DefaultSource.createRelation(DefaultSource.scala:70)
[2023-01-26T19:48:00.965+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:47)
[2023-01-26T19:48:00.965+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)
[2023-01-26T19:48:00.966+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)
[2023-01-26T19:48:00.966+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)
[2023-01-26T19:48:00.966+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)
[2023-01-26T19:48:00.967+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)
[2023-01-26T19:48:00.967+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)
[2023-01-26T19:48:00.968+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)
[2023-01-26T19:48:00.968+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
[2023-01-26T19:48:00.968+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
[2023-01-26T19:48:00.969+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)
[2023-01-26T19:48:00.969+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)
[2023-01-26T19:48:00.969+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:584)
[2023-01-26T19:48:00.970+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:176)
[2023-01-26T19:48:00.970+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:584)
[2023-01-26T19:48:00.970+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)
[2023-01-26T19:48:00.971+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
[2023-01-26T19:48:00.971+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
[2023-01-26T19:48:00.971+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
[2023-01-26T19:48:00.972+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
[2023-01-26T19:48:00.972+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:560)
[2023-01-26T19:48:00.972+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)
[2023-01-26T19:48:00.973+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)
[2023-01-26T19:48:00.973+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)
[2023-01-26T19:48:00.973+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:116)
[2023-01-26T19:48:00.973+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:860)
[2023-01-26T19:48:00.974+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:390)
[2023-01-26T19:48:00.974+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:363)
[2023-01-26T19:48:00.974+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:247)
[2023-01-26T19:48:00.974+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
[2023-01-26T19:48:00.975+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
[2023-01-26T19:48:00.975+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
[2023-01-26T19:48:00.975+0000] {spark_submit.py:495} INFO - at java.base/java.lang.reflect.Method.invoke(Method.java:566)
[2023-01-26T19:48:00.975+0000] {spark_submit.py:495} INFO - at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
[2023-01-26T19:48:00.976+0000] {spark_submit.py:495} INFO - at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
[2023-01-26T19:48:00.976+0000] {spark_submit.py:495} INFO - at py4j.Gateway.invoke(Gateway.java:282)
[2023-01-26T19:48:00.976+0000] {spark_submit.py:495} INFO - at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
[2023-01-26T19:48:00.976+0000] {spark_submit.py:495} INFO - at py4j.commands.CallCommand.execute(CallCommand.java:79)
[2023-01-26T19:48:00.976+0000] {spark_submit.py:495} INFO - at py4j.ClientServerConnection.sendCommand(ClientServerConnection.java:244)
[2023-01-26T19:48:00.977+0000] {spark_submit.py:495} INFO - at py4j.CallbackClient.sendCommand(CallbackClient.java:384)
[2023-01-26T19:48:00.977+0000] {spark_submit.py:495} INFO - at py4j.CallbackClient.sendCommand(CallbackClient.java:356)
[2023-01-26T19:48:00.977+0000] {spark_submit.py:495} INFO - at py4j.reflection.PythonProxyHandler.invoke(PythonProxyHandler.java:106)
[2023-01-26T19:48:00.977+0000] {spark_submit.py:495} INFO - at com.sun.proxy.$Proxy32.call(Unknown Source)
[2023-01-26T19:48:00.978+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1(ForeachBatchSink.scala:51)
[2023-01-26T19:48:00.978+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1$adapted(ForeachBatchSink.scala:51)
[2023-01-26T19:48:00.978+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.sources.ForeachBatchSink.addBatch(ForeachBatchSink.scala:32)
[2023-01-26T19:48:00.978+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:666)
[2023-01-26T19:48:00.978+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)
[2023-01-26T19:48:00.979+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)
[2023-01-26T19:48:00.979+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)
[2023-01-26T19:48:00.979+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
[2023-01-26T19:48:00.979+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
[2023-01-26T19:48:00.979+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:664)
[2023-01-26T19:48:00.979+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:375)
[2023-01-26T19:48:00.980+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:373)
[2023-01-26T19:48:00.980+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:68)
[2023-01-26T19:48:00.980+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:664)
[2023-01-26T19:48:00.980+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:256)
[2023-01-26T19:48:00.980+0000] {spark_submit.py:495} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2023-01-26T19:48:00.980+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:375)
[2023-01-26T19:48:00.981+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:373)
[2023-01-26T19:48:00.981+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:68)
[2023-01-26T19:48:00.981+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:219)
[2023-01-26T19:48:00.981+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)
[2023-01-26T19:48:00.981+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:213)
[2023-01-26T19:48:00.981+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:307)
[2023-01-26T19:48:00.982+0000] {spark_submit.py:495} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2023-01-26T19:48:00.982+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
[2023-01-26T19:48:00.982+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:285)
[2023-01-26T19:48:00.982+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:208)
[2023-01-26T19:48:00.982+0000] {spark_submit.py:495} INFO - Caused by: com.mongodb.MongoTimeoutException: Timed out after 30000 ms while waiting to connect. Client view of cluster state is {type=UNKNOWN, servers=[{address=172.22.0.2:27017, type=UNKNOWN, state=CONNECTING, exception={com.mongodb.MongoSocketOpenException: Exception opening socket}, caused by {java.net.SocketTimeoutException: connect timed out}}]
[2023-01-26T19:48:00.982+0000] {spark_submit.py:495} INFO - at com.mongodb.internal.connection.BaseCluster.getDescription(BaseCluster.java:177)
[2023-01-26T19:48:00.983+0000] {spark_submit.py:495} INFO - at com.mongodb.internal.connection.SingleServerCluster.getDescription(SingleServerCluster.java:41)
[2023-01-26T19:48:00.983+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate.getConnectedClusterDescription(MongoClientDelegate.java:147)
[2023-01-26T19:48:00.983+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate.createClientSession(MongoClientDelegate.java:98)
[2023-01-26T19:48:00.983+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.getClientSession(MongoClientDelegate.java:278)
[2023-01-26T19:48:00.983+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:202)
[2023-01-26T19:48:00.983+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:441)
[2023-01-26T19:48:00.983+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:421)
[2023-01-26T19:48:00.984+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$4(MongoSpark.scala:138)
[2023-01-26T19:48:00.984+0000] {spark_submit.py:495} INFO - at scala.collection.Iterator.foreach(Iterator.scala:943)
[2023-01-26T19:48:00.984+0000] {spark_submit.py:495} INFO - at scala.collection.Iterator.foreach$(Iterator.scala:943)
[2023-01-26T19:48:00.984+0000] {spark_submit.py:495} INFO - at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
[2023-01-26T19:48:00.984+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$3(MongoSpark.scala:122)
[2023-01-26T19:48:00.984+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$3$adapted(MongoSpark.scala:121)
[2023-01-26T19:48:00.984+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.$anonfun$withCollectionDo$1(MongoConnector.scala:186)
[2023-01-26T19:48:00.985+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.$anonfun$withDatabaseDo$1(MongoConnector.scala:171)
[2023-01-26T19:48:00.985+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withMongoClientDo(MongoConnector.scala:154)
[2023-01-26T19:48:00.985+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withDatabaseDo(MongoConnector.scala:171)
[2023-01-26T19:48:00.985+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withCollectionDo(MongoConnector.scala:184)
[2023-01-26T19:48:00.985+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$2(MongoSpark.scala:121)
[2023-01-26T19:48:00.985+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$2$adapted(MongoSpark.scala:120)
[2023-01-26T19:48:00.985+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1011)
[2023-01-26T19:48:00.985+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1011)
[2023-01-26T19:48:00.986+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2268)
[2023-01-26T19:48:00.986+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
[2023-01-26T19:48:00.986+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.Task.run(Task.scala:136)
[2023-01-26T19:48:00.986+0000] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)
[2023-01-26T19:48:00.986+0000] {spark_submit.py:495} INFO - at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)
[2023-01-26T19:48:00.986+0000] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)
[2023-01-26T19:48:00.986+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[2023-01-26T19:48:00.987+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[2023-01-26T19:48:00.987+0000] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2023-01-26T19:48:00.987+0000] {spark_submit.py:495} INFO - [0m
[2023-01-26T19:48:01.067+0000] {spark_submit.py:495} INFO - [[34m2023-01-26 19:48:01,067[0m] {[34mclientserver.py:[0m577} INFO[0m - Received command c on object id p0[0m
[2023-01-26T19:48:31.127+0000] {spark_submit.py:495} INFO - 23/01/26 19:48:31 ERROR Executor: Exception in task 0.0 in stage 15.0 (TID 15)
[2023-01-26T19:48:31.128+0000] {spark_submit.py:495} INFO - com.mongodb.MongoTimeoutException: Timed out after 30000 ms while waiting to connect. Client view of cluster state is {type=UNKNOWN, servers=[{address=172.22.0.2:27017, type=UNKNOWN, state=CONNECTING, exception={com.mongodb.MongoSocketOpenException: Exception opening socket}, caused by {java.net.SocketTimeoutException: connect timed out}}]
[2023-01-26T19:48:31.128+0000] {spark_submit.py:495} INFO - at com.mongodb.internal.connection.BaseCluster.getDescription(BaseCluster.java:177)
[2023-01-26T19:48:31.129+0000] {spark_submit.py:495} INFO - at com.mongodb.internal.connection.SingleServerCluster.getDescription(SingleServerCluster.java:41)
[2023-01-26T19:48:31.129+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate.getConnectedClusterDescription(MongoClientDelegate.java:147)
[2023-01-26T19:48:31.130+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate.createClientSession(MongoClientDelegate.java:98)
[2023-01-26T19:48:31.130+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.getClientSession(MongoClientDelegate.java:278)
[2023-01-26T19:48:31.130+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:202)
[2023-01-26T19:48:31.131+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:441)
[2023-01-26T19:48:31.131+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:421)
[2023-01-26T19:48:31.131+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$4(MongoSpark.scala:138)
[2023-01-26T19:48:31.132+0000] {spark_submit.py:495} INFO - at scala.collection.Iterator.foreach(Iterator.scala:943)
[2023-01-26T19:48:31.132+0000] {spark_submit.py:495} INFO - at scala.collection.Iterator.foreach$(Iterator.scala:943)
[2023-01-26T19:48:31.132+0000] {spark_submit.py:495} INFO - at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
[2023-01-26T19:48:31.133+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$3(MongoSpark.scala:122)
[2023-01-26T19:48:31.133+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$3$adapted(MongoSpark.scala:121)
[2023-01-26T19:48:31.133+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.$anonfun$withCollectionDo$1(MongoConnector.scala:186)
[2023-01-26T19:48:31.134+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.$anonfun$withDatabaseDo$1(MongoConnector.scala:171)
[2023-01-26T19:48:31.134+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withMongoClientDo(MongoConnector.scala:154)
[2023-01-26T19:48:31.134+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withDatabaseDo(MongoConnector.scala:171)
[2023-01-26T19:48:31.135+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withCollectionDo(MongoConnector.scala:184)
[2023-01-26T19:48:31.135+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$2(MongoSpark.scala:121)
[2023-01-26T19:48:31.135+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$2$adapted(MongoSpark.scala:120)
[2023-01-26T19:48:31.136+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1011)
[2023-01-26T19:48:31.136+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1011)
[2023-01-26T19:48:31.136+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2268)
[2023-01-26T19:48:31.137+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
[2023-01-26T19:48:31.137+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.Task.run(Task.scala:136)
[2023-01-26T19:48:31.138+0000] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)
[2023-01-26T19:48:31.138+0000] {spark_submit.py:495} INFO - at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)
[2023-01-26T19:48:31.138+0000] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)
[2023-01-26T19:48:31.139+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[2023-01-26T19:48:31.139+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[2023-01-26T19:48:31.140+0000] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2023-01-26T19:48:31.144+0000] {spark_submit.py:495} INFO - 23/01/26 19:48:31 ERROR TaskSetManager: Task 0 in stage 15.0 failed 1 times; aborting job
[2023-01-26T19:48:31.293+0000] {spark_submit.py:495} INFO - [[34m2023-01-26 19:48:31,292[0m] {[34mconsumerSpark.py:[0m37} ERROR[0m - Error al escribir en Mongo: An error occurred while calling o203.save.
[2023-01-26T19:48:31.293+0000] {spark_submit.py:495} INFO - : org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 15.0 failed 1 times, most recent failure: Lost task 0.0 in stage 15.0 (TID 15) (f1cbb5df32e6 executor driver): com.mongodb.MongoTimeoutException: Timed out after 30000 ms while waiting to connect. Client view of cluster state is {type=UNKNOWN, servers=[{address=172.22.0.2:27017, type=UNKNOWN, state=CONNECTING, exception={com.mongodb.MongoSocketOpenException: Exception opening socket}, caused by {java.net.SocketTimeoutException: connect timed out}}]
[2023-01-26T19:48:31.293+0000] {spark_submit.py:495} INFO - at com.mongodb.internal.connection.BaseCluster.getDescription(BaseCluster.java:177)
[2023-01-26T19:48:31.293+0000] {spark_submit.py:495} INFO - at com.mongodb.internal.connection.SingleServerCluster.getDescription(SingleServerCluster.java:41)
[2023-01-26T19:48:31.294+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate.getConnectedClusterDescription(MongoClientDelegate.java:147)
[2023-01-26T19:48:31.294+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate.createClientSession(MongoClientDelegate.java:98)
[2023-01-26T19:48:31.294+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.getClientSession(MongoClientDelegate.java:278)
[2023-01-26T19:48:31.294+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:202)
[2023-01-26T19:48:31.294+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:441)
[2023-01-26T19:48:31.294+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:421)
[2023-01-26T19:48:31.294+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$4(MongoSpark.scala:138)
[2023-01-26T19:48:31.295+0000] {spark_submit.py:495} INFO - at scala.collection.Iterator.foreach(Iterator.scala:943)
[2023-01-26T19:48:31.295+0000] {spark_submit.py:495} INFO - at scala.collection.Iterator.foreach$(Iterator.scala:943)
[2023-01-26T19:48:31.295+0000] {spark_submit.py:495} INFO - at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
[2023-01-26T19:48:31.295+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$3(MongoSpark.scala:122)
[2023-01-26T19:48:31.295+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$3$adapted(MongoSpark.scala:121)
[2023-01-26T19:48:31.296+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.$anonfun$withCollectionDo$1(MongoConnector.scala:186)
[2023-01-26T19:48:31.296+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.$anonfun$withDatabaseDo$1(MongoConnector.scala:171)
[2023-01-26T19:48:31.296+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withMongoClientDo(MongoConnector.scala:154)
[2023-01-26T19:48:31.296+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withDatabaseDo(MongoConnector.scala:171)
[2023-01-26T19:48:31.296+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withCollectionDo(MongoConnector.scala:184)
[2023-01-26T19:48:31.296+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$2(MongoSpark.scala:121)
[2023-01-26T19:48:31.297+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$2$adapted(MongoSpark.scala:120)
[2023-01-26T19:48:31.297+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1011)
[2023-01-26T19:48:31.297+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1011)
[2023-01-26T19:48:31.297+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2268)
[2023-01-26T19:48:31.297+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
[2023-01-26T19:48:31.297+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.Task.run(Task.scala:136)
[2023-01-26T19:48:31.297+0000] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)
[2023-01-26T19:48:31.297+0000] {spark_submit.py:495} INFO - at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)
[2023-01-26T19:48:31.298+0000] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)
[2023-01-26T19:48:31.298+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[2023-01-26T19:48:31.298+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[2023-01-26T19:48:31.298+0000] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2023-01-26T19:48:31.298+0000] {spark_submit.py:495} INFO - 
[2023-01-26T19:48:31.298+0000] {spark_submit.py:495} INFO - Driver stacktrace:
[2023-01-26T19:48:31.298+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)
[2023-01-26T19:48:31.299+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)
[2023-01-26T19:48:31.299+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)
[2023-01-26T19:48:31.299+0000] {spark_submit.py:495} INFO - at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
[2023-01-26T19:48:31.299+0000] {spark_submit.py:495} INFO - at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
[2023-01-26T19:48:31.299+0000] {spark_submit.py:495} INFO - at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
[2023-01-26T19:48:31.299+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)
[2023-01-26T19:48:31.299+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)
[2023-01-26T19:48:31.300+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)
[2023-01-26T19:48:31.300+0000] {spark_submit.py:495} INFO - at scala.Option.foreach(Option.scala:407)
[2023-01-26T19:48:31.300+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)
[2023-01-26T19:48:31.300+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)
[2023-01-26T19:48:31.300+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)
[2023-01-26T19:48:31.300+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)
[2023-01-26T19:48:31.300+0000] {spark_submit.py:495} INFO - at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
[2023-01-26T19:48:31.301+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)
[2023-01-26T19:48:31.301+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.runJob(SparkContext.scala:2228)
[2023-01-26T19:48:31.301+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.runJob(SparkContext.scala:2249)
[2023-01-26T19:48:31.301+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.runJob(SparkContext.scala:2268)
[2023-01-26T19:48:31.301+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.runJob(SparkContext.scala:2293)
[2023-01-26T19:48:31.301+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$1(RDD.scala:1011)
[2023-01-26T19:48:31.301+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
[2023-01-26T19:48:31.301+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
[2023-01-26T19:48:31.302+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.withScope(RDD.scala:406)
[2023-01-26T19:48:31.302+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.foreachPartition(RDD.scala:1009)
[2023-01-26T19:48:31.302+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.save(MongoSpark.scala:120)
[2023-01-26T19:48:31.302+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.save(MongoSpark.scala:169)
[2023-01-26T19:48:31.302+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.sql.DefaultSource.createRelation(DefaultSource.scala:70)
[2023-01-26T19:48:31.302+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:47)
[2023-01-26T19:48:31.302+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)
[2023-01-26T19:48:31.303+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)
[2023-01-26T19:48:31.303+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)
[2023-01-26T19:48:31.303+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)
[2023-01-26T19:48:31.303+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)
[2023-01-26T19:48:31.303+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)
[2023-01-26T19:48:31.303+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)
[2023-01-26T19:48:31.303+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
[2023-01-26T19:48:31.303+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
[2023-01-26T19:48:31.304+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)
[2023-01-26T19:48:31.304+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)
[2023-01-26T19:48:31.304+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:584)
[2023-01-26T19:48:31.304+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:176)
[2023-01-26T19:48:31.304+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:584)
[2023-01-26T19:48:31.304+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)
[2023-01-26T19:48:31.304+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
[2023-01-26T19:48:31.305+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
[2023-01-26T19:48:31.305+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
[2023-01-26T19:48:31.305+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
[2023-01-26T19:48:31.305+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:560)
[2023-01-26T19:48:31.305+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)
[2023-01-26T19:48:31.305+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)
[2023-01-26T19:48:31.305+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)
[2023-01-26T19:48:31.306+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:116)
[2023-01-26T19:48:31.306+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:860)
[2023-01-26T19:48:31.306+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:390)
[2023-01-26T19:48:31.306+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:363)
[2023-01-26T19:48:31.306+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:247)
[2023-01-26T19:48:31.306+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
[2023-01-26T19:48:31.306+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
[2023-01-26T19:48:31.306+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
[2023-01-26T19:48:31.307+0000] {spark_submit.py:495} INFO - at java.base/java.lang.reflect.Method.invoke(Method.java:566)
[2023-01-26T19:48:31.307+0000] {spark_submit.py:495} INFO - at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
[2023-01-26T19:48:31.307+0000] {spark_submit.py:495} INFO - at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
[2023-01-26T19:48:31.307+0000] {spark_submit.py:495} INFO - at py4j.Gateway.invoke(Gateway.java:282)
[2023-01-26T19:48:31.307+0000] {spark_submit.py:495} INFO - at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
[2023-01-26T19:48:31.307+0000] {spark_submit.py:495} INFO - at py4j.commands.CallCommand.execute(CallCommand.java:79)
[2023-01-26T19:48:31.307+0000] {spark_submit.py:495} INFO - at py4j.ClientServerConnection.sendCommand(ClientServerConnection.java:244)
[2023-01-26T19:48:31.308+0000] {spark_submit.py:495} INFO - at py4j.CallbackClient.sendCommand(CallbackClient.java:384)
[2023-01-26T19:48:31.308+0000] {spark_submit.py:495} INFO - at py4j.CallbackClient.sendCommand(CallbackClient.java:356)
[2023-01-26T19:48:31.308+0000] {spark_submit.py:495} INFO - at py4j.reflection.PythonProxyHandler.invoke(PythonProxyHandler.java:106)
[2023-01-26T19:48:31.308+0000] {spark_submit.py:495} INFO - at com.sun.proxy.$Proxy32.call(Unknown Source)
[2023-01-26T19:48:31.308+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1(ForeachBatchSink.scala:51)
[2023-01-26T19:48:31.308+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1$adapted(ForeachBatchSink.scala:51)
[2023-01-26T19:48:31.308+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.sources.ForeachBatchSink.addBatch(ForeachBatchSink.scala:32)
[2023-01-26T19:48:31.309+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:666)
[2023-01-26T19:48:31.309+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)
[2023-01-26T19:48:31.309+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)
[2023-01-26T19:48:31.309+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)
[2023-01-26T19:48:31.309+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
[2023-01-26T19:48:31.309+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
[2023-01-26T19:48:31.309+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:664)
[2023-01-26T19:48:31.309+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:375)
[2023-01-26T19:48:31.310+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:373)
[2023-01-26T19:48:31.310+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:68)
[2023-01-26T19:48:31.310+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:664)
[2023-01-26T19:48:31.310+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:256)
[2023-01-26T19:48:31.310+0000] {spark_submit.py:495} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2023-01-26T19:48:31.310+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:375)
[2023-01-26T19:48:31.310+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:373)
[2023-01-26T19:48:31.310+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:68)
[2023-01-26T19:48:31.310+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:219)
[2023-01-26T19:48:31.311+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)
[2023-01-26T19:48:31.311+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:213)
[2023-01-26T19:48:31.311+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:307)
[2023-01-26T19:48:31.311+0000] {spark_submit.py:495} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2023-01-26T19:48:31.311+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
[2023-01-26T19:48:31.311+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:285)
[2023-01-26T19:48:31.311+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:208)
[2023-01-26T19:48:31.311+0000] {spark_submit.py:495} INFO - Caused by: com.mongodb.MongoTimeoutException: Timed out after 30000 ms while waiting to connect. Client view of cluster state is {type=UNKNOWN, servers=[{address=172.22.0.2:27017, type=UNKNOWN, state=CONNECTING, exception={com.mongodb.MongoSocketOpenException: Exception opening socket}, caused by {java.net.SocketTimeoutException: connect timed out}}]
[2023-01-26T19:48:31.311+0000] {spark_submit.py:495} INFO - at com.mongodb.internal.connection.BaseCluster.getDescription(BaseCluster.java:177)
[2023-01-26T19:48:31.312+0000] {spark_submit.py:495} INFO - at com.mongodb.internal.connection.SingleServerCluster.getDescription(SingleServerCluster.java:41)
[2023-01-26T19:48:31.312+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate.getConnectedClusterDescription(MongoClientDelegate.java:147)
[2023-01-26T19:48:31.312+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate.createClientSession(MongoClientDelegate.java:98)
[2023-01-26T19:48:31.312+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.getClientSession(MongoClientDelegate.java:278)
[2023-01-26T19:48:31.312+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:202)
[2023-01-26T19:48:31.312+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:441)
[2023-01-26T19:48:31.312+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:421)
[2023-01-26T19:48:31.312+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$4(MongoSpark.scala:138)
[2023-01-26T19:48:31.312+0000] {spark_submit.py:495} INFO - at scala.collection.Iterator.foreach(Iterator.scala:943)
[2023-01-26T19:48:31.313+0000] {spark_submit.py:495} INFO - at scala.collection.Iterator.foreach$(Iterator.scala:943)
[2023-01-26T19:48:31.313+0000] {spark_submit.py:495} INFO - at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
[2023-01-26T19:48:31.313+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$3(MongoSpark.scala:122)
[2023-01-26T19:48:31.313+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$3$adapted(MongoSpark.scala:121)
[2023-01-26T19:48:31.313+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.$anonfun$withCollectionDo$1(MongoConnector.scala:186)
[2023-01-26T19:48:31.313+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.$anonfun$withDatabaseDo$1(MongoConnector.scala:171)
[2023-01-26T19:48:31.313+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withMongoClientDo(MongoConnector.scala:154)
[2023-01-26T19:48:31.313+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withDatabaseDo(MongoConnector.scala:171)
[2023-01-26T19:48:31.313+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withCollectionDo(MongoConnector.scala:184)
[2023-01-26T19:48:31.314+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$2(MongoSpark.scala:121)
[2023-01-26T19:48:31.314+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$2$adapted(MongoSpark.scala:120)
[2023-01-26T19:48:31.314+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1011)
[2023-01-26T19:48:31.314+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1011)
[2023-01-26T19:48:31.314+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2268)
[2023-01-26T19:48:31.314+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
[2023-01-26T19:48:31.314+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.Task.run(Task.scala:136)
[2023-01-26T19:48:31.314+0000] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)
[2023-01-26T19:48:31.314+0000] {spark_submit.py:495} INFO - at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)
[2023-01-26T19:48:31.315+0000] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)
[2023-01-26T19:48:31.315+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[2023-01-26T19:48:31.315+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[2023-01-26T19:48:31.315+0000] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2023-01-26T19:48:31.315+0000] {spark_submit.py:495} INFO - [0m
[2023-01-26T20:24:07.860+0000] {spark_submit.py:495} INFO - [[34m2023-01-26 20:24:07,860[0m] {[34mclientserver.py:[0m577} INFO[0m - Received command c on object id p0[0m
[2023-01-26T20:24:10.825+0000] {spark_submit.py:495} INFO - -------------------------------------------
[2023-01-26T20:24:10.826+0000] {spark_submit.py:495} INFO - Batch: 6
[2023-01-26T20:24:10.826+0000] {spark_submit.py:495} INFO - -------------------------------------------
[2023-01-26T20:24:10.861+0000] {spark_submit.py:495} INFO - +-------------+--------------------+--------+-------------+--------------------+-------------------+------------+-----------------+--------------------+-------------+
[2023-01-26T20:24:10.861+0000] {spark_submit.py:495} INFO - |           id|       nameOfProduct|   price|ProductStatus|          SellerName|   RegistrationDate|QuantitySold|QuantityAvailable|      Free shipping?|Store ratings|
[2023-01-26T20:24:10.861+0000] {spark_submit.py:495} INFO - +-------------+--------------------+--------+-------------+--------------------+-------------------+------------+-----------------+--------------------+-------------+
[2023-01-26T20:24:10.861+0000] {spark_submit.py:495} INFO - |MLA1221192372|Disco Duro Extern...| 26299.0|          new|    OVERHARD DIGITAL|2015-01-08 12:19:41|         250|              100|Your shipment arr...|          4.5|
[2023-01-26T20:24:10.861+0000] {spark_submit.py:495} INFO - |MLA1157512489|Placa De Video Nv...|124999.0|          new|      PCREGISTRADAOK|2020-11-02 13:17:33|          50|                1|Your shipment arr...|          4.5|
[2023-01-26T20:24:10.861+0000] {spark_submit.py:495} INFO - |MLA1115295971|Procesador Amd Ry...| 93999.0|          new|      PCREGISTRADAOK|2020-11-02 13:17:33|          25|                1|Your shipment arr...|          4.5|
[2023-01-26T20:24:10.861+0000] {spark_submit.py:495} INFO - |MLA1160198557|Memoria Ram Fury ...| 26699.0|          new|    OVERHARD DIGITAL|2015-01-08 12:19:41|         500|               50|Your shipment arr...|          4.5|
[2023-01-26T20:24:10.861+0000] {spark_submit.py:495} INFO - |MLA1148481665|Microprocesador P...| 58899.0|          new|            MALL WEB|2016-05-26 20:28:35|         150|                1|Your shipment arr...|          4.5|
[2023-01-26T20:24:10.862+0000] {spark_submit.py:495} INFO - |MLA1160204806|Memoria Ram Fury ...| 15499.0|          new|    OVERHARD DIGITAL|2015-01-08 12:19:41|         500|              100|Your shipment arr...|          4.5|
[2023-01-26T20:24:10.862+0000] {spark_submit.py:495} INFO - | MLA923637669|Placa De Video Ge...| 28699.0|          new|         INDIGOTRADE|2018-01-05 19:21:16|         500|               50|Your shipment arr...|          4.5|
[2023-01-26T20:24:10.862+0000] {spark_submit.py:495} INFO - |MLA1318244331|Disco Solido Ssd ...| 28799.0|          new|    OVERHARD DIGITAL|2015-01-08 12:19:41|           5|              150|Your shipment arr...|          4.5|
[2023-01-26T20:24:10.862+0000] {spark_submit.py:495} INFO - |MLA1133481354|Disco Sólido Inte...| 13799.0|          new|           DALECLÍCK|2015-03-16 12:07:23|         500|                1|You have to pay t...|          4.5|
[2023-01-26T20:24:10.862+0000] {spark_submit.py:495} INFO - | MLA882277177|Capturadora Video...|  2699.0|          new|         BSK-IMPORTS|2019-12-30 23:07:04|         500|               50|You have to pay t...|          3.0|
[2023-01-26T20:24:10.862+0000] {spark_submit.py:495} INFO - |MLA1283127595|Procesador Intel ...| 51379.0|          new|      PCREGISTRADAOK|2020-11-02 13:17:33|           1|                1|Your shipment arr...|          4.5|
[2023-01-26T20:24:10.862+0000] {spark_submit.py:495} INFO - |MLA1139583771|Disco Sólido Inte...|  8339.0|          new|DIAMONDSYSTEMPADUAML|2015-07-01 12:32:22|         250|              500|You have to pay t...|          4.5|
[2023-01-26T20:24:10.862+0000] {spark_submit.py:495} INFO - |MLA1119996182|Disco Sólido Inte...| 25499.0|          new|            MALL WEB|2016-05-26 20:28:35|         500|                1|Your shipment arr...|          4.5|
[2023-01-26T20:24:10.862+0000] {spark_submit.py:495} INFO - |MLA1183925519|Placa De Video Nv...|299999.0|          new|ESTUDIOSMPSRLESTU...|2020-11-29 18:36:32|          50|                1|Your shipment arr...|          4.5|
[2023-01-26T20:24:10.862+0000] {spark_submit.py:495} INFO - | MLA844489972|Disco Duro Intern...| 11599.0|          new|           DALECLÍCK|2015-03-16 12:07:23|        5000|              500|You have to pay t...|          5.0|
[2023-01-26T20:24:10.862+0000] {spark_submit.py:495} INFO - | MLA833246699|Disco Sólido Inte...| 26999.0|          new|            MALL WEB|2016-05-26 20:28:35|         500|                1|Your shipment arr...|          4.5|
[2023-01-26T20:24:10.862+0000] {spark_submit.py:495} INFO - |MLA1163233412|Placa De Video Nv...| 34659.0|          new|      PCREGISTRADAOK|2020-11-02 13:17:33|           1|                1|Your shipment arr...|          4.5|
[2023-01-26T20:24:10.862+0000] {spark_submit.py:495} INFO - |MLA1314590445|Disco Sólido Inte...|  7999.0|          new|       ELECTRO-SHOWS|2010-03-26 01:08:16|         250|              250|You have to pay t...|          4.5|
[2023-01-26T20:24:10.863+0000] {spark_submit.py:495} INFO - |MLA1302136118|Disco Duro Intern...| 18271.0|          new|DIAMONDSYSTEMSANJ...|2016-04-08 17:21:06|         100|               50|Your shipment arr...|          4.5|
[2023-01-26T20:24:10.863+0000] {spark_submit.py:495} INFO - | MLA902808321|Procesador Gamer ...| 23699.0|          new|            MALL WEB|2016-05-26 20:28:35|         500|                1|Your shipment arr...|          4.5|
[2023-01-26T20:24:10.863+0000] {spark_submit.py:495} INFO - +-------------+--------------------+--------+-------------+--------------------+-------------------+------------+-----------------+--------------------+-------------+
[2023-01-26T20:24:10.863+0000] {spark_submit.py:495} INFO - only showing top 20 rows
[2023-01-26T20:24:10.863+0000] {spark_submit.py:495} INFO - 
[2023-01-26T20:24:38.679+0000] {spark_submit.py:495} INFO - 23/01/26 20:24:38 ERROR Executor: Exception in task 0.0 in stage 16.0 (TID 16)
[2023-01-26T20:24:38.679+0000] {spark_submit.py:495} INFO - com.mongodb.MongoTimeoutException: Timed out after 30000 ms while waiting to connect. Client view of cluster state is {type=UNKNOWN, servers=[{address=172.22.0.2:27017, type=UNKNOWN, state=CONNECTING, exception={com.mongodb.MongoSocketOpenException: Exception opening socket}, caused by {java.net.SocketTimeoutException: connect timed out}}]
[2023-01-26T20:24:38.679+0000] {spark_submit.py:495} INFO - at com.mongodb.internal.connection.BaseCluster.getDescription(BaseCluster.java:177)
[2023-01-26T20:24:38.679+0000] {spark_submit.py:495} INFO - at com.mongodb.internal.connection.SingleServerCluster.getDescription(SingleServerCluster.java:41)
[2023-01-26T20:24:38.680+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate.getConnectedClusterDescription(MongoClientDelegate.java:147)
[2023-01-26T20:24:38.680+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate.createClientSession(MongoClientDelegate.java:98)
[2023-01-26T20:24:38.680+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.getClientSession(MongoClientDelegate.java:278)
[2023-01-26T20:24:38.680+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:202)
[2023-01-26T20:24:38.680+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:441)
[2023-01-26T20:24:38.680+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:421)
[2023-01-26T20:24:38.681+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$4(MongoSpark.scala:138)
[2023-01-26T20:24:38.681+0000] {spark_submit.py:495} INFO - at scala.collection.Iterator.foreach(Iterator.scala:943)
[2023-01-26T20:24:38.681+0000] {spark_submit.py:495} INFO - at scala.collection.Iterator.foreach$(Iterator.scala:943)
[2023-01-26T20:24:38.681+0000] {spark_submit.py:495} INFO - at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
[2023-01-26T20:24:38.681+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$3(MongoSpark.scala:122)
[2023-01-26T20:24:38.681+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$3$adapted(MongoSpark.scala:121)
[2023-01-26T20:24:38.682+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.$anonfun$withCollectionDo$1(MongoConnector.scala:186)
[2023-01-26T20:24:38.682+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.$anonfun$withDatabaseDo$1(MongoConnector.scala:171)
[2023-01-26T20:24:38.682+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withMongoClientDo(MongoConnector.scala:154)
[2023-01-26T20:24:38.682+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withDatabaseDo(MongoConnector.scala:171)
[2023-01-26T20:24:38.682+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withCollectionDo(MongoConnector.scala:184)
[2023-01-26T20:24:38.682+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$2(MongoSpark.scala:121)
[2023-01-26T20:24:38.683+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$2$adapted(MongoSpark.scala:120)
[2023-01-26T20:24:38.683+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1011)
[2023-01-26T20:24:38.683+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1011)
[2023-01-26T20:24:38.683+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2268)
[2023-01-26T20:24:38.683+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
[2023-01-26T20:24:38.683+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.Task.run(Task.scala:136)
[2023-01-26T20:24:38.684+0000] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)
[2023-01-26T20:24:38.684+0000] {spark_submit.py:495} INFO - at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)
[2023-01-26T20:24:38.684+0000] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)
[2023-01-26T20:24:38.684+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[2023-01-26T20:24:38.684+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[2023-01-26T20:24:38.684+0000] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2023-01-26T20:24:38.688+0000] {spark_submit.py:495} INFO - 23/01/26 20:24:38 ERROR TaskSetManager: Task 0 in stage 16.0 failed 1 times; aborting job
[2023-01-26T20:24:38.801+0000] {spark_submit.py:495} INFO - [[34m2023-01-26 20:24:38,801[0m] {[34mconsumerSpark.py:[0m37} ERROR[0m - Error al escribir en Mongo: An error occurred while calling o212.save.
[2023-01-26T20:24:38.802+0000] {spark_submit.py:495} INFO - : org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 16.0 failed 1 times, most recent failure: Lost task 0.0 in stage 16.0 (TID 16) (f1cbb5df32e6 executor driver): com.mongodb.MongoTimeoutException: Timed out after 30000 ms while waiting to connect. Client view of cluster state is {type=UNKNOWN, servers=[{address=172.22.0.2:27017, type=UNKNOWN, state=CONNECTING, exception={com.mongodb.MongoSocketOpenException: Exception opening socket}, caused by {java.net.SocketTimeoutException: connect timed out}}]
[2023-01-26T20:24:38.802+0000] {spark_submit.py:495} INFO - at com.mongodb.internal.connection.BaseCluster.getDescription(BaseCluster.java:177)
[2023-01-26T20:24:38.803+0000] {spark_submit.py:495} INFO - at com.mongodb.internal.connection.SingleServerCluster.getDescription(SingleServerCluster.java:41)
[2023-01-26T20:24:38.803+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate.getConnectedClusterDescription(MongoClientDelegate.java:147)
[2023-01-26T20:24:38.804+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate.createClientSession(MongoClientDelegate.java:98)
[2023-01-26T20:24:38.804+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.getClientSession(MongoClientDelegate.java:278)
[2023-01-26T20:24:38.804+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:202)
[2023-01-26T20:24:38.805+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:441)
[2023-01-26T20:24:38.805+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:421)
[2023-01-26T20:24:38.805+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$4(MongoSpark.scala:138)
[2023-01-26T20:24:38.806+0000] {spark_submit.py:495} INFO - at scala.collection.Iterator.foreach(Iterator.scala:943)
[2023-01-26T20:24:38.806+0000] {spark_submit.py:495} INFO - at scala.collection.Iterator.foreach$(Iterator.scala:943)
[2023-01-26T20:24:38.806+0000] {spark_submit.py:495} INFO - at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
[2023-01-26T20:24:38.807+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$3(MongoSpark.scala:122)
[2023-01-26T20:24:38.807+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$3$adapted(MongoSpark.scala:121)
[2023-01-26T20:24:38.808+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.$anonfun$withCollectionDo$1(MongoConnector.scala:186)
[2023-01-26T20:24:38.808+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.$anonfun$withDatabaseDo$1(MongoConnector.scala:171)
[2023-01-26T20:24:38.808+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withMongoClientDo(MongoConnector.scala:154)
[2023-01-26T20:24:38.809+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withDatabaseDo(MongoConnector.scala:171)
[2023-01-26T20:24:38.809+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withCollectionDo(MongoConnector.scala:184)
[2023-01-26T20:24:38.809+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$2(MongoSpark.scala:121)
[2023-01-26T20:24:38.810+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$2$adapted(MongoSpark.scala:120)
[2023-01-26T20:24:38.810+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1011)
[2023-01-26T20:24:38.810+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1011)
[2023-01-26T20:24:38.811+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2268)
[2023-01-26T20:24:38.811+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
[2023-01-26T20:24:38.811+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.Task.run(Task.scala:136)
[2023-01-26T20:24:38.812+0000] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)
[2023-01-26T20:24:38.812+0000] {spark_submit.py:495} INFO - at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)
[2023-01-26T20:24:38.812+0000] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)
[2023-01-26T20:24:38.813+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[2023-01-26T20:24:38.813+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[2023-01-26T20:24:38.813+0000] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2023-01-26T20:24:38.814+0000] {spark_submit.py:495} INFO - 
[2023-01-26T20:24:38.814+0000] {spark_submit.py:495} INFO - Driver stacktrace:
[2023-01-26T20:24:38.814+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)
[2023-01-26T20:24:38.815+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)
[2023-01-26T20:24:38.815+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)
[2023-01-26T20:24:38.815+0000] {spark_submit.py:495} INFO - at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
[2023-01-26T20:24:38.816+0000] {spark_submit.py:495} INFO - at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
[2023-01-26T20:24:38.816+0000] {spark_submit.py:495} INFO - at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
[2023-01-26T20:24:38.816+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)
[2023-01-26T20:24:38.817+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)
[2023-01-26T20:24:38.817+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)
[2023-01-26T20:24:38.817+0000] {spark_submit.py:495} INFO - at scala.Option.foreach(Option.scala:407)
[2023-01-26T20:24:38.818+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)
[2023-01-26T20:24:38.818+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)
[2023-01-26T20:24:38.818+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)
[2023-01-26T20:24:38.819+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)
[2023-01-26T20:24:38.819+0000] {spark_submit.py:495} INFO - at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
[2023-01-26T20:24:38.819+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)
[2023-01-26T20:24:38.820+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.runJob(SparkContext.scala:2228)
[2023-01-26T20:24:38.820+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.runJob(SparkContext.scala:2249)
[2023-01-26T20:24:38.820+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.runJob(SparkContext.scala:2268)
[2023-01-26T20:24:38.821+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.runJob(SparkContext.scala:2293)
[2023-01-26T20:24:38.821+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$1(RDD.scala:1011)
[2023-01-26T20:24:38.821+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
[2023-01-26T20:24:38.822+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
[2023-01-26T20:24:38.822+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.withScope(RDD.scala:406)
[2023-01-26T20:24:38.822+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.foreachPartition(RDD.scala:1009)
[2023-01-26T20:24:38.822+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.save(MongoSpark.scala:120)
[2023-01-26T20:24:38.823+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.save(MongoSpark.scala:169)
[2023-01-26T20:24:38.823+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.sql.DefaultSource.createRelation(DefaultSource.scala:70)
[2023-01-26T20:24:38.824+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:47)
[2023-01-26T20:24:38.824+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)
[2023-01-26T20:24:38.824+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)
[2023-01-26T20:24:38.825+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)
[2023-01-26T20:24:38.825+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)
[2023-01-26T20:24:38.825+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)
[2023-01-26T20:24:38.826+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)
[2023-01-26T20:24:38.826+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)
[2023-01-26T20:24:38.826+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
[2023-01-26T20:24:38.827+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
[2023-01-26T20:24:38.827+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)
[2023-01-26T20:24:38.827+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)
[2023-01-26T20:24:38.828+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:584)
[2023-01-26T20:24:38.828+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:176)
[2023-01-26T20:24:38.828+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:584)
[2023-01-26T20:24:38.829+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)
[2023-01-26T20:24:38.829+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
[2023-01-26T20:24:38.829+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
[2023-01-26T20:24:38.830+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
[2023-01-26T20:24:38.830+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
[2023-01-26T20:24:38.830+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:560)
[2023-01-26T20:24:38.830+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)
[2023-01-26T20:24:38.830+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)
[2023-01-26T20:24:38.831+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)
[2023-01-26T20:24:38.831+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:116)
[2023-01-26T20:24:38.831+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:860)
[2023-01-26T20:24:38.831+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:390)
[2023-01-26T20:24:38.832+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:363)
[2023-01-26T20:24:38.832+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:247)
[2023-01-26T20:24:38.832+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
[2023-01-26T20:24:38.832+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
[2023-01-26T20:24:38.833+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
[2023-01-26T20:24:38.833+0000] {spark_submit.py:495} INFO - at java.base/java.lang.reflect.Method.invoke(Method.java:566)
[2023-01-26T20:24:38.833+0000] {spark_submit.py:495} INFO - at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
[2023-01-26T20:24:38.833+0000] {spark_submit.py:495} INFO - at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
[2023-01-26T20:24:38.833+0000] {spark_submit.py:495} INFO - at py4j.Gateway.invoke(Gateway.java:282)
[2023-01-26T20:24:38.834+0000] {spark_submit.py:495} INFO - at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
[2023-01-26T20:24:38.834+0000] {spark_submit.py:495} INFO - at py4j.commands.CallCommand.execute(CallCommand.java:79)
[2023-01-26T20:24:38.834+0000] {spark_submit.py:495} INFO - at py4j.ClientServerConnection.sendCommand(ClientServerConnection.java:244)
[2023-01-26T20:24:38.834+0000] {spark_submit.py:495} INFO - at py4j.CallbackClient.sendCommand(CallbackClient.java:384)
[2023-01-26T20:24:38.835+0000] {spark_submit.py:495} INFO - at py4j.CallbackClient.sendCommand(CallbackClient.java:356)
[2023-01-26T20:24:38.835+0000] {spark_submit.py:495} INFO - at py4j.reflection.PythonProxyHandler.invoke(PythonProxyHandler.java:106)
[2023-01-26T20:24:38.835+0000] {spark_submit.py:495} INFO - at com.sun.proxy.$Proxy32.call(Unknown Source)
[2023-01-26T20:24:38.835+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1(ForeachBatchSink.scala:51)
[2023-01-26T20:24:38.835+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1$adapted(ForeachBatchSink.scala:51)
[2023-01-26T20:24:38.835+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.sources.ForeachBatchSink.addBatch(ForeachBatchSink.scala:32)
[2023-01-26T20:24:38.836+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:666)
[2023-01-26T20:24:38.836+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)
[2023-01-26T20:24:38.836+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)
[2023-01-26T20:24:38.836+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)
[2023-01-26T20:24:38.836+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
[2023-01-26T20:24:38.836+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
[2023-01-26T20:24:38.837+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:664)
[2023-01-26T20:24:38.837+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:375)
[2023-01-26T20:24:38.837+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:373)
[2023-01-26T20:24:38.837+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:68)
[2023-01-26T20:24:38.837+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:664)
[2023-01-26T20:24:38.838+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:256)
[2023-01-26T20:24:38.838+0000] {spark_submit.py:495} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2023-01-26T20:24:38.838+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:375)
[2023-01-26T20:24:38.838+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:373)
[2023-01-26T20:24:38.838+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:68)
[2023-01-26T20:24:38.838+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:219)
[2023-01-26T20:24:38.839+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)
[2023-01-26T20:24:38.839+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:213)
[2023-01-26T20:24:38.839+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:307)
[2023-01-26T20:24:38.839+0000] {spark_submit.py:495} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2023-01-26T20:24:38.839+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
[2023-01-26T20:24:38.840+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:285)
[2023-01-26T20:24:38.840+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:208)
[2023-01-26T20:24:38.840+0000] {spark_submit.py:495} INFO - Caused by: com.mongodb.MongoTimeoutException: Timed out after 30000 ms while waiting to connect. Client view of cluster state is {type=UNKNOWN, servers=[{address=172.22.0.2:27017, type=UNKNOWN, state=CONNECTING, exception={com.mongodb.MongoSocketOpenException: Exception opening socket}, caused by {java.net.SocketTimeoutException: connect timed out}}]
[2023-01-26T20:24:38.840+0000] {spark_submit.py:495} INFO - at com.mongodb.internal.connection.BaseCluster.getDescription(BaseCluster.java:177)
[2023-01-26T20:24:38.840+0000] {spark_submit.py:495} INFO - at com.mongodb.internal.connection.SingleServerCluster.getDescription(SingleServerCluster.java:41)
[2023-01-26T20:24:38.841+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate.getConnectedClusterDescription(MongoClientDelegate.java:147)
[2023-01-26T20:24:38.841+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate.createClientSession(MongoClientDelegate.java:98)
[2023-01-26T20:24:38.841+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.getClientSession(MongoClientDelegate.java:278)
[2023-01-26T20:24:38.841+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:202)
[2023-01-26T20:24:38.841+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:441)
[2023-01-26T20:24:38.841+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:421)
[2023-01-26T20:24:38.841+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$4(MongoSpark.scala:138)
[2023-01-26T20:24:38.841+0000] {spark_submit.py:495} INFO - at scala.collection.Iterator.foreach(Iterator.scala:943)
[2023-01-26T20:24:38.842+0000] {spark_submit.py:495} INFO - at scala.collection.Iterator.foreach$(Iterator.scala:943)
[2023-01-26T20:24:38.842+0000] {spark_submit.py:495} INFO - at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
[2023-01-26T20:24:38.842+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$3(MongoSpark.scala:122)
[2023-01-26T20:24:38.842+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$3$adapted(MongoSpark.scala:121)
[2023-01-26T20:24:38.842+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.$anonfun$withCollectionDo$1(MongoConnector.scala:186)
[2023-01-26T20:24:38.842+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.$anonfun$withDatabaseDo$1(MongoConnector.scala:171)
[2023-01-26T20:24:38.842+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withMongoClientDo(MongoConnector.scala:154)
[2023-01-26T20:24:38.843+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withDatabaseDo(MongoConnector.scala:171)
[2023-01-26T20:24:38.843+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withCollectionDo(MongoConnector.scala:184)
[2023-01-26T20:24:38.843+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$2(MongoSpark.scala:121)
[2023-01-26T20:24:38.843+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$2$adapted(MongoSpark.scala:120)
[2023-01-26T20:24:38.843+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1011)
[2023-01-26T20:24:38.843+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1011)
[2023-01-26T20:24:38.843+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2268)
[2023-01-26T20:24:38.843+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
[2023-01-26T20:24:38.844+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.Task.run(Task.scala:136)
[2023-01-26T20:24:38.844+0000] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)
[2023-01-26T20:24:38.844+0000] {spark_submit.py:495} INFO - at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)
[2023-01-26T20:24:38.844+0000] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)
[2023-01-26T20:24:38.844+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[2023-01-26T20:24:38.844+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[2023-01-26T20:24:38.844+0000] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2023-01-26T20:24:38.845+0000] {spark_submit.py:495} INFO - [0m
[2023-01-26T20:24:38.919+0000] {spark_submit.py:495} INFO - [[34m2023-01-26 20:24:38,919[0m] {[34mclientserver.py:[0m577} INFO[0m - Received command c on object id p0[0m
[2023-01-26T20:25:08.978+0000] {spark_submit.py:495} INFO - 23/01/26 20:25:08 ERROR Executor: Exception in task 0.0 in stage 18.0 (TID 18)
[2023-01-26T20:25:08.978+0000] {spark_submit.py:495} INFO - com.mongodb.MongoTimeoutException: Timed out after 30000 ms while waiting to connect. Client view of cluster state is {type=UNKNOWN, servers=[{address=172.22.0.2:27017, type=UNKNOWN, state=CONNECTING, exception={com.mongodb.MongoSocketOpenException: Exception opening socket}, caused by {java.net.SocketTimeoutException: connect timed out}}]
[2023-01-26T20:25:08.979+0000] {spark_submit.py:495} INFO - at com.mongodb.internal.connection.BaseCluster.getDescription(BaseCluster.java:177)
[2023-01-26T20:25:08.979+0000] {spark_submit.py:495} INFO - at com.mongodb.internal.connection.SingleServerCluster.getDescription(SingleServerCluster.java:41)
[2023-01-26T20:25:08.979+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate.getConnectedClusterDescription(MongoClientDelegate.java:147)
[2023-01-26T20:25:08.980+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate.createClientSession(MongoClientDelegate.java:98)
[2023-01-26T20:25:08.980+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.getClientSession(MongoClientDelegate.java:278)
[2023-01-26T20:25:08.980+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:202)
[2023-01-26T20:25:08.980+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:441)
[2023-01-26T20:25:08.981+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:421)
[2023-01-26T20:25:08.981+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$4(MongoSpark.scala:138)
[2023-01-26T20:25:08.981+0000] {spark_submit.py:495} INFO - at scala.collection.Iterator.foreach(Iterator.scala:943)
[2023-01-26T20:25:08.982+0000] {spark_submit.py:495} INFO - at scala.collection.Iterator.foreach$(Iterator.scala:943)
[2023-01-26T20:25:08.982+0000] {spark_submit.py:495} INFO - at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
[2023-01-26T20:25:08.982+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$3(MongoSpark.scala:122)
[2023-01-26T20:25:08.984+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$3$adapted(MongoSpark.scala:121)
[2023-01-26T20:25:08.984+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.$anonfun$withCollectionDo$1(MongoConnector.scala:186)
[2023-01-26T20:25:08.985+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.$anonfun$withDatabaseDo$1(MongoConnector.scala:171)
[2023-01-26T20:25:08.985+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withMongoClientDo(MongoConnector.scala:154)
[2023-01-26T20:25:08.985+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withDatabaseDo(MongoConnector.scala:171)
[2023-01-26T20:25:08.985+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withCollectionDo(MongoConnector.scala:184)
[2023-01-26T20:25:08.985+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$2(MongoSpark.scala:121)
[2023-01-26T20:25:08.986+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$2$adapted(MongoSpark.scala:120)
[2023-01-26T20:25:08.986+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1011)
[2023-01-26T20:25:08.986+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1011)
[2023-01-26T20:25:08.986+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2268)
[2023-01-26T20:25:08.986+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
[2023-01-26T20:25:08.987+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.Task.run(Task.scala:136)
[2023-01-26T20:25:08.987+0000] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)
[2023-01-26T20:25:08.987+0000] {spark_submit.py:495} INFO - at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)
[2023-01-26T20:25:08.987+0000] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)
[2023-01-26T20:25:08.987+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[2023-01-26T20:25:08.988+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[2023-01-26T20:25:08.988+0000] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2023-01-26T20:25:08.988+0000] {spark_submit.py:495} INFO - 23/01/26 20:25:08 ERROR TaskSetManager: Task 0 in stage 18.0 failed 1 times; aborting job
[2023-01-26T20:25:09.145+0000] {spark_submit.py:495} INFO - [[34m2023-01-26 20:25:09,145[0m] {[34mconsumerSpark.py:[0m37} ERROR[0m - Error al escribir en Mongo: An error occurred while calling o221.save.
[2023-01-26T20:25:09.146+0000] {spark_submit.py:495} INFO - : org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 18.0 failed 1 times, most recent failure: Lost task 0.0 in stage 18.0 (TID 18) (f1cbb5df32e6 executor driver): com.mongodb.MongoTimeoutException: Timed out after 30000 ms while waiting to connect. Client view of cluster state is {type=UNKNOWN, servers=[{address=172.22.0.2:27017, type=UNKNOWN, state=CONNECTING, exception={com.mongodb.MongoSocketOpenException: Exception opening socket}, caused by {java.net.SocketTimeoutException: connect timed out}}]
[2023-01-26T20:25:09.146+0000] {spark_submit.py:495} INFO - at com.mongodb.internal.connection.BaseCluster.getDescription(BaseCluster.java:177)
[2023-01-26T20:25:09.147+0000] {spark_submit.py:495} INFO - at com.mongodb.internal.connection.SingleServerCluster.getDescription(SingleServerCluster.java:41)
[2023-01-26T20:25:09.147+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate.getConnectedClusterDescription(MongoClientDelegate.java:147)
[2023-01-26T20:25:09.148+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate.createClientSession(MongoClientDelegate.java:98)
[2023-01-26T20:25:09.148+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.getClientSession(MongoClientDelegate.java:278)
[2023-01-26T20:25:09.148+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:202)
[2023-01-26T20:25:09.149+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:441)
[2023-01-26T20:25:09.149+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:421)
[2023-01-26T20:25:09.149+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$4(MongoSpark.scala:138)
[2023-01-26T20:25:09.150+0000] {spark_submit.py:495} INFO - at scala.collection.Iterator.foreach(Iterator.scala:943)
[2023-01-26T20:25:09.150+0000] {spark_submit.py:495} INFO - at scala.collection.Iterator.foreach$(Iterator.scala:943)
[2023-01-26T20:25:09.151+0000] {spark_submit.py:495} INFO - at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
[2023-01-26T20:25:09.151+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$3(MongoSpark.scala:122)
[2023-01-26T20:25:09.151+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$3$adapted(MongoSpark.scala:121)
[2023-01-26T20:25:09.152+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.$anonfun$withCollectionDo$1(MongoConnector.scala:186)
[2023-01-26T20:25:09.152+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.$anonfun$withDatabaseDo$1(MongoConnector.scala:171)
[2023-01-26T20:25:09.152+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withMongoClientDo(MongoConnector.scala:154)
[2023-01-26T20:25:09.153+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withDatabaseDo(MongoConnector.scala:171)
[2023-01-26T20:25:09.153+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withCollectionDo(MongoConnector.scala:184)
[2023-01-26T20:25:09.153+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$2(MongoSpark.scala:121)
[2023-01-26T20:25:09.154+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$2$adapted(MongoSpark.scala:120)
[2023-01-26T20:25:09.154+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1011)
[2023-01-26T20:25:09.154+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1011)
[2023-01-26T20:25:09.155+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2268)
[2023-01-26T20:25:09.155+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
[2023-01-26T20:25:09.155+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.Task.run(Task.scala:136)
[2023-01-26T20:25:09.156+0000] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)
[2023-01-26T20:25:09.156+0000] {spark_submit.py:495} INFO - at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)
[2023-01-26T20:25:09.156+0000] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)
[2023-01-26T20:25:09.157+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[2023-01-26T20:25:09.157+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[2023-01-26T20:25:09.157+0000] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2023-01-26T20:25:09.158+0000] {spark_submit.py:495} INFO - 
[2023-01-26T20:25:09.158+0000] {spark_submit.py:495} INFO - Driver stacktrace:
[2023-01-26T20:25:09.158+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)
[2023-01-26T20:25:09.159+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)
[2023-01-26T20:25:09.159+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)
[2023-01-26T20:25:09.159+0000] {spark_submit.py:495} INFO - at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
[2023-01-26T20:25:09.160+0000] {spark_submit.py:495} INFO - at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
[2023-01-26T20:25:09.160+0000] {spark_submit.py:495} INFO - at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
[2023-01-26T20:25:09.160+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)
[2023-01-26T20:25:09.161+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)
[2023-01-26T20:25:09.161+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)
[2023-01-26T20:25:09.161+0000] {spark_submit.py:495} INFO - at scala.Option.foreach(Option.scala:407)
[2023-01-26T20:25:09.162+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)
[2023-01-26T20:25:09.162+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)
[2023-01-26T20:25:09.162+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)
[2023-01-26T20:25:09.163+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)
[2023-01-26T20:25:09.163+0000] {spark_submit.py:495} INFO - at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
[2023-01-26T20:25:09.163+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)
[2023-01-26T20:25:09.163+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.runJob(SparkContext.scala:2228)
[2023-01-26T20:25:09.164+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.runJob(SparkContext.scala:2249)
[2023-01-26T20:25:09.164+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.runJob(SparkContext.scala:2268)
[2023-01-26T20:25:09.164+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.runJob(SparkContext.scala:2293)
[2023-01-26T20:25:09.165+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$1(RDD.scala:1011)
[2023-01-26T20:25:09.165+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
[2023-01-26T20:25:09.165+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
[2023-01-26T20:25:09.166+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.withScope(RDD.scala:406)
[2023-01-26T20:25:09.166+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.foreachPartition(RDD.scala:1009)
[2023-01-26T20:25:09.166+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.save(MongoSpark.scala:120)
[2023-01-26T20:25:09.167+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.save(MongoSpark.scala:169)
[2023-01-26T20:25:09.167+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.sql.DefaultSource.createRelation(DefaultSource.scala:70)
[2023-01-26T20:25:09.167+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:47)
[2023-01-26T20:25:09.168+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)
[2023-01-26T20:25:09.168+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)
[2023-01-26T20:25:09.168+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)
[2023-01-26T20:25:09.169+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)
[2023-01-26T20:25:09.169+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)
[2023-01-26T20:25:09.169+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)
[2023-01-26T20:25:09.170+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)
[2023-01-26T20:25:09.170+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
[2023-01-26T20:25:09.170+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
[2023-01-26T20:25:09.171+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)
[2023-01-26T20:25:09.171+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)
[2023-01-26T20:25:09.171+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:584)
[2023-01-26T20:25:09.172+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:176)
[2023-01-26T20:25:09.172+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:584)
[2023-01-26T20:25:09.172+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)
[2023-01-26T20:25:09.173+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
[2023-01-26T20:25:09.173+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
[2023-01-26T20:25:09.173+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
[2023-01-26T20:25:09.173+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
[2023-01-26T20:25:09.174+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:560)
[2023-01-26T20:25:09.174+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)
[2023-01-26T20:25:09.174+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)
[2023-01-26T20:25:09.174+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)
[2023-01-26T20:25:09.175+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:116)
[2023-01-26T20:25:09.175+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:860)
[2023-01-26T20:25:09.175+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:390)
[2023-01-26T20:25:09.175+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:363)
[2023-01-26T20:25:09.176+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:247)
[2023-01-26T20:25:09.176+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
[2023-01-26T20:25:09.176+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
[2023-01-26T20:25:09.176+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
[2023-01-26T20:25:09.176+0000] {spark_submit.py:495} INFO - at java.base/java.lang.reflect.Method.invoke(Method.java:566)
[2023-01-26T20:25:09.177+0000] {spark_submit.py:495} INFO - at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
[2023-01-26T20:25:09.177+0000] {spark_submit.py:495} INFO - at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
[2023-01-26T20:25:09.177+0000] {spark_submit.py:495} INFO - at py4j.Gateway.invoke(Gateway.java:282)
[2023-01-26T20:25:09.177+0000] {spark_submit.py:495} INFO - at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
[2023-01-26T20:25:09.178+0000] {spark_submit.py:495} INFO - at py4j.commands.CallCommand.execute(CallCommand.java:79)
[2023-01-26T20:25:09.178+0000] {spark_submit.py:495} INFO - at py4j.ClientServerConnection.sendCommand(ClientServerConnection.java:244)
[2023-01-26T20:25:09.178+0000] {spark_submit.py:495} INFO - at py4j.CallbackClient.sendCommand(CallbackClient.java:384)
[2023-01-26T20:25:09.178+0000] {spark_submit.py:495} INFO - at py4j.CallbackClient.sendCommand(CallbackClient.java:356)
[2023-01-26T20:25:09.178+0000] {spark_submit.py:495} INFO - at py4j.reflection.PythonProxyHandler.invoke(PythonProxyHandler.java:106)
[2023-01-26T20:25:09.179+0000] {spark_submit.py:495} INFO - at com.sun.proxy.$Proxy32.call(Unknown Source)
[2023-01-26T20:25:09.179+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1(ForeachBatchSink.scala:51)
[2023-01-26T20:25:09.179+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1$adapted(ForeachBatchSink.scala:51)
[2023-01-26T20:25:09.179+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.sources.ForeachBatchSink.addBatch(ForeachBatchSink.scala:32)
[2023-01-26T20:25:09.179+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:666)
[2023-01-26T20:25:09.179+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)
[2023-01-26T20:25:09.180+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)
[2023-01-26T20:25:09.180+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)
[2023-01-26T20:25:09.180+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
[2023-01-26T20:25:09.180+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
[2023-01-26T20:25:09.180+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:664)
[2023-01-26T20:25:09.180+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:375)
[2023-01-26T20:25:09.181+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:373)
[2023-01-26T20:25:09.181+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:68)
[2023-01-26T20:25:09.181+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:664)
[2023-01-26T20:25:09.181+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:256)
[2023-01-26T20:25:09.181+0000] {spark_submit.py:495} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2023-01-26T20:25:09.181+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:375)
[2023-01-26T20:25:09.182+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:373)
[2023-01-26T20:25:09.182+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:68)
[2023-01-26T20:25:09.182+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:219)
[2023-01-26T20:25:09.182+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)
[2023-01-26T20:25:09.182+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:213)
[2023-01-26T20:25:09.182+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:307)
[2023-01-26T20:25:09.183+0000] {spark_submit.py:495} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2023-01-26T20:25:09.183+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
[2023-01-26T20:25:09.183+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:285)
[2023-01-26T20:25:09.183+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:208)
[2023-01-26T20:25:09.183+0000] {spark_submit.py:495} INFO - Caused by: com.mongodb.MongoTimeoutException: Timed out after 30000 ms while waiting to connect. Client view of cluster state is {type=UNKNOWN, servers=[{address=172.22.0.2:27017, type=UNKNOWN, state=CONNECTING, exception={com.mongodb.MongoSocketOpenException: Exception opening socket}, caused by {java.net.SocketTimeoutException: connect timed out}}]
[2023-01-26T20:25:09.183+0000] {spark_submit.py:495} INFO - at com.mongodb.internal.connection.BaseCluster.getDescription(BaseCluster.java:177)
[2023-01-26T20:25:09.183+0000] {spark_submit.py:495} INFO - at com.mongodb.internal.connection.SingleServerCluster.getDescription(SingleServerCluster.java:41)
[2023-01-26T20:25:09.184+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate.getConnectedClusterDescription(MongoClientDelegate.java:147)
[2023-01-26T20:25:09.184+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate.createClientSession(MongoClientDelegate.java:98)
[2023-01-26T20:25:09.184+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.getClientSession(MongoClientDelegate.java:278)
[2023-01-26T20:25:09.184+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:202)
[2023-01-26T20:25:09.184+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:441)
[2023-01-26T20:25:09.184+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:421)
[2023-01-26T20:25:09.184+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$4(MongoSpark.scala:138)
[2023-01-26T20:25:09.185+0000] {spark_submit.py:495} INFO - at scala.collection.Iterator.foreach(Iterator.scala:943)
[2023-01-26T20:25:09.185+0000] {spark_submit.py:495} INFO - at scala.collection.Iterator.foreach$(Iterator.scala:943)
[2023-01-26T20:25:09.185+0000] {spark_submit.py:495} INFO - at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
[2023-01-26T20:25:09.185+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$3(MongoSpark.scala:122)
[2023-01-26T20:25:09.185+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$3$adapted(MongoSpark.scala:121)
[2023-01-26T20:25:09.185+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.$anonfun$withCollectionDo$1(MongoConnector.scala:186)
[2023-01-26T20:25:09.185+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.$anonfun$withDatabaseDo$1(MongoConnector.scala:171)
[2023-01-26T20:25:09.186+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withMongoClientDo(MongoConnector.scala:154)
[2023-01-26T20:25:09.186+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withDatabaseDo(MongoConnector.scala:171)
[2023-01-26T20:25:09.186+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withCollectionDo(MongoConnector.scala:184)
[2023-01-26T20:25:09.186+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$2(MongoSpark.scala:121)
[2023-01-26T20:25:09.186+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$2$adapted(MongoSpark.scala:120)
[2023-01-26T20:25:09.186+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1011)
[2023-01-26T20:25:09.186+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1011)
[2023-01-26T20:25:09.186+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2268)
[2023-01-26T20:25:09.187+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
[2023-01-26T20:25:09.187+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.Task.run(Task.scala:136)
[2023-01-26T20:25:09.187+0000] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)
[2023-01-26T20:25:09.187+0000] {spark_submit.py:495} INFO - at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)
[2023-01-26T20:25:09.187+0000] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)
[2023-01-26T20:25:09.187+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[2023-01-26T20:25:09.187+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[2023-01-26T20:25:09.188+0000] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2023-01-26T20:25:09.188+0000] {spark_submit.py:495} INFO - [0m
[2023-01-26T21:08:10.394+0000] {spark_submit.py:495} INFO - [[34m2023-01-26 21:08:10,393[0m] {[34mclientserver.py:[0m577} INFO[0m - Received command c on object id p0[0m
[2023-01-26T21:08:15.694+0000] {spark_submit.py:495} INFO - -------------------------------------------
[2023-01-26T21:08:15.695+0000] {spark_submit.py:495} INFO - Batch: 7
[2023-01-26T21:08:15.695+0000] {spark_submit.py:495} INFO - -------------------------------------------
[2023-01-26T21:08:15.716+0000] {spark_submit.py:495} INFO - +-------------+--------------------+--------+-------------+--------------------+-------------------+------------+-----------------+--------------------+-------------+
[2023-01-26T21:08:15.716+0000] {spark_submit.py:495} INFO - |           id|       nameOfProduct|   price|ProductStatus|          SellerName|   RegistrationDate|QuantitySold|QuantityAvailable|      Free shipping?|Store ratings|
[2023-01-26T21:08:15.716+0000] {spark_submit.py:495} INFO - +-------------+--------------------+--------+-------------+--------------------+-------------------+------------+-----------------+--------------------+-------------+
[2023-01-26T21:08:15.717+0000] {spark_submit.py:495} INFO - |MLA1221192372|Disco Duro Extern...| 26299.0|          new|    OVERHARD DIGITAL|2015-01-08 12:19:41|         250|              100|Your shipment arr...|          4.5|
[2023-01-26T21:08:15.717+0000] {spark_submit.py:495} INFO - |MLA1157512489|Placa De Video Nv...|124999.0|          new|      PCREGISTRADAOK|2020-11-02 13:17:33|          50|                1|Your shipment arr...|          4.5|
[2023-01-26T21:08:15.717+0000] {spark_submit.py:495} INFO - |MLA1115295971|Procesador Amd Ry...| 93999.0|          new|      PCREGISTRADAOK|2020-11-02 13:17:33|          25|                1|Your shipment arr...|          4.5|
[2023-01-26T21:08:15.717+0000] {spark_submit.py:495} INFO - |MLA1163238194|Placa De Video Am...| 33999.0|          new|       MICYBERCOMPRA|2011-09-13 13:15:53|          50|                1|Your shipment arr...|          4.5|
[2023-01-26T21:08:15.717+0000] {spark_submit.py:495} INFO - |MLA1318244331|Disco Solido Ssd ...| 28799.0|          new|    OVERHARD DIGITAL|2015-01-08 12:19:41|           5|              150|Your shipment arr...|          4.5|
[2023-01-26T21:08:15.717+0000] {spark_submit.py:495} INFO - |MLA1148481665|Microprocesador P...| 58899.0|          new|            MALL WEB|2016-05-26 20:28:35|         150|                1|Your shipment arr...|          4.5|
[2023-01-26T21:08:15.717+0000] {spark_submit.py:495} INFO - |MLA1246852007|Memoria Ram Fury ...| 15199.2|          new|        NOXIESTORESA|2022-05-20 16:20:29|         250|               50|Your shipment arr...|          4.5|
[2023-01-26T21:08:15.717+0000] {spark_submit.py:495} INFO - | MLA923637669|Placa De Video Ge...| 28699.0|          new|         INDIGOTRADE|2018-01-05 19:21:16|         500|               50|Your shipment arr...|          4.5|
[2023-01-26T21:08:15.717+0000] {spark_submit.py:495} INFO - |MLA1133481354|Disco Sólido Inte...| 13799.0|          new|           DALECLÍCK|2015-03-16 12:07:23|         500|                1|You have to pay t...|          4.5|
[2023-01-26T21:08:15.717+0000] {spark_submit.py:495} INFO - |MLA1139583771|Disco Sólido Inte...|  8339.0|          new|DIAMONDSYSTEMPADUAML|2015-07-01 12:32:22|         250|              250|You have to pay t...|          4.5|
[2023-01-26T21:08:15.717+0000] {spark_submit.py:495} INFO - | MLA882277177|Capturadora Video...|  2699.0|          new|         BSK-IMPORTS|2019-12-30 23:07:04|         500|               50|You have to pay t...|          3.0|
[2023-01-26T21:08:15.717+0000] {spark_submit.py:495} INFO - |MLA1228990119|Memoria Ram Fury ...| 23255.0|          new|MARSTECHSRLMARSTE...|2020-08-05 14:27:59|          50|                1|Your shipment arr...|          4.5|
[2023-01-26T21:08:15.717+0000] {spark_submit.py:495} INFO - |MLA1183925519|Placa De Video Nv...|299999.0|          new|ESTUDIOSMPSRLESTU...|2020-11-29 18:36:32|          50|                1|Your shipment arr...|          4.5|
[2023-01-26T21:08:15.718+0000] {spark_submit.py:495} INFO - | MLA844489972|Disco Duro Intern...| 11599.0|          new|           DALECLÍCK|2015-03-16 12:07:23|        5000|              500|You have to pay t...|          5.0|
[2023-01-26T21:08:15.718+0000] {spark_submit.py:495} INFO - |MLA1168362675|Memoria Ram Fury ...| 26698.0|          new|MARSTECHSRLMARSTE...|2020-08-05 14:27:59|         150|                1|Your shipment arr...|          4.5|
[2023-01-26T21:08:15.718+0000] {spark_submit.py:495} INFO - |MLA1314590445|Disco Sólido Inte...|  7999.0|          new|       ELECTRO-SHOWS|2010-03-26 01:08:16|         250|              250|You have to pay t...|          4.5|
[2023-01-26T21:08:15.718+0000] {spark_submit.py:495} INFO - |MLA1163233412|Placa De Video Nv...| 34659.0|          new|      PCREGISTRADAOK|2020-11-02 13:17:33|           1|                1|Your shipment arr...|          4.5|
[2023-01-26T21:08:15.718+0000] {spark_submit.py:495} INFO - | MLA902808321|Procesador Gamer ...| 23699.0|          new|            MALL WEB|2016-05-26 20:28:35|         500|                1|Your shipment arr...|          4.5|
[2023-01-26T21:08:15.718+0000] {spark_submit.py:495} INFO - |MLA1302136118|Disco Duro Intern...| 18271.0|          new|DIAMONDSYSTEMSANJ...|2016-04-08 17:21:06|         100|               50|Your shipment arr...|          4.5|
[2023-01-26T21:08:15.718+0000] {spark_submit.py:495} INFO - |MLA1137238635|Fuente De Aliment...|  3235.0|          new|     DIAMONDSYSTEMML|2014-04-07 22:00:36|          25|                1|You have to pay t...|          3.5|
[2023-01-26T21:08:15.718+0000] {spark_submit.py:495} INFO - +-------------+--------------------+--------+-------------+--------------------+-------------------+------------+-----------------+--------------------+-------------+
[2023-01-26T21:08:15.718+0000] {spark_submit.py:495} INFO - only showing top 20 rows
[2023-01-26T21:08:15.718+0000] {spark_submit.py:495} INFO - 
[2023-01-26T21:08:41.015+0000] {spark_submit.py:495} INFO - 23/01/26 21:08:41 ERROR Executor: Exception in task 0.0 in stage 19.0 (TID 19)
[2023-01-26T21:08:41.015+0000] {spark_submit.py:495} INFO - com.mongodb.MongoTimeoutException: Timed out after 30000 ms while waiting to connect. Client view of cluster state is {type=UNKNOWN, servers=[{address=172.22.0.2:27017, type=UNKNOWN, state=CONNECTING, exception={com.mongodb.MongoSocketOpenException: Exception opening socket}, caused by {java.net.SocketTimeoutException: connect timed out}}]
[2023-01-26T21:08:41.015+0000] {spark_submit.py:495} INFO - at com.mongodb.internal.connection.BaseCluster.getDescription(BaseCluster.java:177)
[2023-01-26T21:08:41.015+0000] {spark_submit.py:495} INFO - at com.mongodb.internal.connection.SingleServerCluster.getDescription(SingleServerCluster.java:41)
[2023-01-26T21:08:41.015+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate.getConnectedClusterDescription(MongoClientDelegate.java:147)
[2023-01-26T21:08:41.015+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate.createClientSession(MongoClientDelegate.java:98)
[2023-01-26T21:08:41.015+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.getClientSession(MongoClientDelegate.java:278)
[2023-01-26T21:08:41.015+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:202)
[2023-01-26T21:08:41.015+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:441)
[2023-01-26T21:08:41.016+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:421)
[2023-01-26T21:08:41.016+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$4(MongoSpark.scala:138)
[2023-01-26T21:08:41.016+0000] {spark_submit.py:495} INFO - at scala.collection.Iterator.foreach(Iterator.scala:943)
[2023-01-26T21:08:41.016+0000] {spark_submit.py:495} INFO - at scala.collection.Iterator.foreach$(Iterator.scala:943)
[2023-01-26T21:08:41.016+0000] {spark_submit.py:495} INFO - at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
[2023-01-26T21:08:41.016+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$3(MongoSpark.scala:122)
[2023-01-26T21:08:41.016+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$3$adapted(MongoSpark.scala:121)
[2023-01-26T21:08:41.016+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.$anonfun$withCollectionDo$1(MongoConnector.scala:186)
[2023-01-26T21:08:41.016+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.$anonfun$withDatabaseDo$1(MongoConnector.scala:171)
[2023-01-26T21:08:41.016+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withMongoClientDo(MongoConnector.scala:154)
[2023-01-26T21:08:41.016+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withDatabaseDo(MongoConnector.scala:171)
[2023-01-26T21:08:41.016+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withCollectionDo(MongoConnector.scala:184)
[2023-01-26T21:08:41.016+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$2(MongoSpark.scala:121)
[2023-01-26T21:08:41.017+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$2$adapted(MongoSpark.scala:120)
[2023-01-26T21:08:41.017+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1011)
[2023-01-26T21:08:41.017+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1011)
[2023-01-26T21:08:41.017+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2268)
[2023-01-26T21:08:41.017+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
[2023-01-26T21:08:41.017+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.Task.run(Task.scala:136)
[2023-01-26T21:08:41.017+0000] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)
[2023-01-26T21:08:41.017+0000] {spark_submit.py:495} INFO - at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)
[2023-01-26T21:08:41.017+0000] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)
[2023-01-26T21:08:41.017+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[2023-01-26T21:08:41.017+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[2023-01-26T21:08:41.017+0000] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2023-01-26T21:08:41.023+0000] {spark_submit.py:495} INFO - 23/01/26 21:08:41 ERROR TaskSetManager: Task 0 in stage 19.0 failed 1 times; aborting job
[2023-01-26T21:08:41.136+0000] {spark_submit.py:495} INFO - [[34m2023-01-26 21:08:41,136[0m] {[34mconsumerSpark.py:[0m37} ERROR[0m - Error al escribir en Mongo: An error occurred while calling o230.save.
[2023-01-26T21:08:41.137+0000] {spark_submit.py:495} INFO - : org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 19.0 failed 1 times, most recent failure: Lost task 0.0 in stage 19.0 (TID 19) (f1cbb5df32e6 executor driver): com.mongodb.MongoTimeoutException: Timed out after 30000 ms while waiting to connect. Client view of cluster state is {type=UNKNOWN, servers=[{address=172.22.0.2:27017, type=UNKNOWN, state=CONNECTING, exception={com.mongodb.MongoSocketOpenException: Exception opening socket}, caused by {java.net.SocketTimeoutException: connect timed out}}]
[2023-01-26T21:08:41.137+0000] {spark_submit.py:495} INFO - at com.mongodb.internal.connection.BaseCluster.getDescription(BaseCluster.java:177)
[2023-01-26T21:08:41.137+0000] {spark_submit.py:495} INFO - at com.mongodb.internal.connection.SingleServerCluster.getDescription(SingleServerCluster.java:41)
[2023-01-26T21:08:41.137+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate.getConnectedClusterDescription(MongoClientDelegate.java:147)
[2023-01-26T21:08:41.137+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate.createClientSession(MongoClientDelegate.java:98)
[2023-01-26T21:08:41.137+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.getClientSession(MongoClientDelegate.java:278)
[2023-01-26T21:08:41.137+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:202)
[2023-01-26T21:08:41.138+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:441)
[2023-01-26T21:08:41.138+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:421)
[2023-01-26T21:08:41.138+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$4(MongoSpark.scala:138)
[2023-01-26T21:08:41.138+0000] {spark_submit.py:495} INFO - at scala.collection.Iterator.foreach(Iterator.scala:943)
[2023-01-26T21:08:41.138+0000] {spark_submit.py:495} INFO - at scala.collection.Iterator.foreach$(Iterator.scala:943)
[2023-01-26T21:08:41.138+0000] {spark_submit.py:495} INFO - at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
[2023-01-26T21:08:41.138+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$3(MongoSpark.scala:122)
[2023-01-26T21:08:41.138+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$3$adapted(MongoSpark.scala:121)
[2023-01-26T21:08:41.138+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.$anonfun$withCollectionDo$1(MongoConnector.scala:186)
[2023-01-26T21:08:41.141+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.$anonfun$withDatabaseDo$1(MongoConnector.scala:171)
[2023-01-26T21:08:41.142+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withMongoClientDo(MongoConnector.scala:154)
[2023-01-26T21:08:41.142+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withDatabaseDo(MongoConnector.scala:171)
[2023-01-26T21:08:41.142+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withCollectionDo(MongoConnector.scala:184)
[2023-01-26T21:08:41.142+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$2(MongoSpark.scala:121)
[2023-01-26T21:08:41.142+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$2$adapted(MongoSpark.scala:120)
[2023-01-26T21:08:41.142+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1011)
[2023-01-26T21:08:41.142+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1011)
[2023-01-26T21:08:41.142+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2268)
[2023-01-26T21:08:41.142+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
[2023-01-26T21:08:41.142+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.Task.run(Task.scala:136)
[2023-01-26T21:08:41.142+0000] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)
[2023-01-26T21:08:41.142+0000] {spark_submit.py:495} INFO - at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)
[2023-01-26T21:08:41.142+0000] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)
[2023-01-26T21:08:41.143+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[2023-01-26T21:08:41.143+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[2023-01-26T21:08:41.143+0000] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2023-01-26T21:08:41.143+0000] {spark_submit.py:495} INFO - 
[2023-01-26T21:08:41.143+0000] {spark_submit.py:495} INFO - Driver stacktrace:
[2023-01-26T21:08:41.143+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)
[2023-01-26T21:08:41.143+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)
[2023-01-26T21:08:41.143+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)
[2023-01-26T21:08:41.143+0000] {spark_submit.py:495} INFO - at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
[2023-01-26T21:08:41.143+0000] {spark_submit.py:495} INFO - at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
[2023-01-26T21:08:41.143+0000] {spark_submit.py:495} INFO - at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
[2023-01-26T21:08:41.143+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)
[2023-01-26T21:08:41.143+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)
[2023-01-26T21:08:41.144+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)
[2023-01-26T21:08:41.144+0000] {spark_submit.py:495} INFO - at scala.Option.foreach(Option.scala:407)
[2023-01-26T21:08:41.144+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)
[2023-01-26T21:08:41.144+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)
[2023-01-26T21:08:41.144+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)
[2023-01-26T21:08:41.144+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)
[2023-01-26T21:08:41.144+0000] {spark_submit.py:495} INFO - at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
[2023-01-26T21:08:41.144+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)
[2023-01-26T21:08:41.144+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.runJob(SparkContext.scala:2228)
[2023-01-26T21:08:41.144+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.runJob(SparkContext.scala:2249)
[2023-01-26T21:08:41.144+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.runJob(SparkContext.scala:2268)
[2023-01-26T21:08:41.145+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.runJob(SparkContext.scala:2293)
[2023-01-26T21:08:41.145+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$1(RDD.scala:1011)
[2023-01-26T21:08:41.145+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
[2023-01-26T21:08:41.145+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
[2023-01-26T21:08:41.145+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.withScope(RDD.scala:406)
[2023-01-26T21:08:41.145+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.foreachPartition(RDD.scala:1009)
[2023-01-26T21:08:41.145+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.save(MongoSpark.scala:120)
[2023-01-26T21:08:41.145+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.save(MongoSpark.scala:169)
[2023-01-26T21:08:41.145+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.sql.DefaultSource.createRelation(DefaultSource.scala:70)
[2023-01-26T21:08:41.145+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:47)
[2023-01-26T21:08:41.145+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)
[2023-01-26T21:08:41.145+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)
[2023-01-26T21:08:41.145+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)
[2023-01-26T21:08:41.146+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)
[2023-01-26T21:08:41.146+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)
[2023-01-26T21:08:41.146+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)
[2023-01-26T21:08:41.146+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)
[2023-01-26T21:08:41.146+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
[2023-01-26T21:08:41.146+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
[2023-01-26T21:08:41.146+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)
[2023-01-26T21:08:41.146+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)
[2023-01-26T21:08:41.146+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:584)
[2023-01-26T21:08:41.146+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:176)
[2023-01-26T21:08:41.146+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:584)
[2023-01-26T21:08:41.146+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)
[2023-01-26T21:08:41.146+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
[2023-01-26T21:08:41.146+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
[2023-01-26T21:08:41.147+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
[2023-01-26T21:08:41.147+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
[2023-01-26T21:08:41.147+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:560)
[2023-01-26T21:08:41.147+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)
[2023-01-26T21:08:41.147+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)
[2023-01-26T21:08:41.147+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)
[2023-01-26T21:08:41.147+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:116)
[2023-01-26T21:08:41.147+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:860)
[2023-01-26T21:08:41.147+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:390)
[2023-01-26T21:08:41.147+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:363)
[2023-01-26T21:08:41.147+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:247)
[2023-01-26T21:08:41.147+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
[2023-01-26T21:08:41.147+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
[2023-01-26T21:08:41.148+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
[2023-01-26T21:08:41.148+0000] {spark_submit.py:495} INFO - at java.base/java.lang.reflect.Method.invoke(Method.java:566)
[2023-01-26T21:08:41.148+0000] {spark_submit.py:495} INFO - at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
[2023-01-26T21:08:41.148+0000] {spark_submit.py:495} INFO - at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
[2023-01-26T21:08:41.148+0000] {spark_submit.py:495} INFO - at py4j.Gateway.invoke(Gateway.java:282)
[2023-01-26T21:08:41.148+0000] {spark_submit.py:495} INFO - at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
[2023-01-26T21:08:41.148+0000] {spark_submit.py:495} INFO - at py4j.commands.CallCommand.execute(CallCommand.java:79)
[2023-01-26T21:08:41.148+0000] {spark_submit.py:495} INFO - at py4j.ClientServerConnection.sendCommand(ClientServerConnection.java:244)
[2023-01-26T21:08:41.148+0000] {spark_submit.py:495} INFO - at py4j.CallbackClient.sendCommand(CallbackClient.java:384)
[2023-01-26T21:08:41.148+0000] {spark_submit.py:495} INFO - at py4j.CallbackClient.sendCommand(CallbackClient.java:356)
[2023-01-26T21:08:41.148+0000] {spark_submit.py:495} INFO - at py4j.reflection.PythonProxyHandler.invoke(PythonProxyHandler.java:106)
[2023-01-26T21:08:41.148+0000] {spark_submit.py:495} INFO - at com.sun.proxy.$Proxy32.call(Unknown Source)
[2023-01-26T21:08:41.148+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1(ForeachBatchSink.scala:51)
[2023-01-26T21:08:41.149+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1$adapted(ForeachBatchSink.scala:51)
[2023-01-26T21:08:41.149+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.sources.ForeachBatchSink.addBatch(ForeachBatchSink.scala:32)
[2023-01-26T21:08:41.149+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:666)
[2023-01-26T21:08:41.149+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)
[2023-01-26T21:08:41.149+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)
[2023-01-26T21:08:41.149+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)
[2023-01-26T21:08:41.149+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
[2023-01-26T21:08:41.149+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
[2023-01-26T21:08:41.149+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:664)
[2023-01-26T21:08:41.150+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:375)
[2023-01-26T21:08:41.150+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:373)
[2023-01-26T21:08:41.150+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:68)
[2023-01-26T21:08:41.150+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:664)
[2023-01-26T21:08:41.150+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:256)
[2023-01-26T21:08:41.150+0000] {spark_submit.py:495} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2023-01-26T21:08:41.150+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:375)
[2023-01-26T21:08:41.150+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:373)
[2023-01-26T21:08:41.150+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:68)
[2023-01-26T21:08:41.150+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:219)
[2023-01-26T21:08:41.150+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)
[2023-01-26T21:08:41.150+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:213)
[2023-01-26T21:08:41.150+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:307)
[2023-01-26T21:08:41.151+0000] {spark_submit.py:495} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2023-01-26T21:08:41.151+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
[2023-01-26T21:08:41.151+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:285)
[2023-01-26T21:08:41.151+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:208)
[2023-01-26T21:08:41.151+0000] {spark_submit.py:495} INFO - Caused by: com.mongodb.MongoTimeoutException: Timed out after 30000 ms while waiting to connect. Client view of cluster state is {type=UNKNOWN, servers=[{address=172.22.0.2:27017, type=UNKNOWN, state=CONNECTING, exception={com.mongodb.MongoSocketOpenException: Exception opening socket}, caused by {java.net.SocketTimeoutException: connect timed out}}]
[2023-01-26T21:08:41.151+0000] {spark_submit.py:495} INFO - at com.mongodb.internal.connection.BaseCluster.getDescription(BaseCluster.java:177)
[2023-01-26T21:08:41.151+0000] {spark_submit.py:495} INFO - at com.mongodb.internal.connection.SingleServerCluster.getDescription(SingleServerCluster.java:41)
[2023-01-26T21:08:41.151+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate.getConnectedClusterDescription(MongoClientDelegate.java:147)
[2023-01-26T21:08:41.151+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate.createClientSession(MongoClientDelegate.java:98)
[2023-01-26T21:08:41.151+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.getClientSession(MongoClientDelegate.java:278)
[2023-01-26T21:08:41.151+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:202)
[2023-01-26T21:08:41.151+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:441)
[2023-01-26T21:08:41.151+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:421)
[2023-01-26T21:08:41.151+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$4(MongoSpark.scala:138)
[2023-01-26T21:08:41.152+0000] {spark_submit.py:495} INFO - at scala.collection.Iterator.foreach(Iterator.scala:943)
[2023-01-26T21:08:41.152+0000] {spark_submit.py:495} INFO - at scala.collection.Iterator.foreach$(Iterator.scala:943)
[2023-01-26T21:08:41.152+0000] {spark_submit.py:495} INFO - at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
[2023-01-26T21:08:41.152+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$3(MongoSpark.scala:122)
[2023-01-26T21:08:41.152+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$3$adapted(MongoSpark.scala:121)
[2023-01-26T21:08:41.152+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.$anonfun$withCollectionDo$1(MongoConnector.scala:186)
[2023-01-26T21:08:41.152+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.$anonfun$withDatabaseDo$1(MongoConnector.scala:171)
[2023-01-26T21:08:41.152+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withMongoClientDo(MongoConnector.scala:154)
[2023-01-26T21:08:41.152+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withDatabaseDo(MongoConnector.scala:171)
[2023-01-26T21:08:41.152+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withCollectionDo(MongoConnector.scala:184)
[2023-01-26T21:08:41.152+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$2(MongoSpark.scala:121)
[2023-01-26T21:08:41.152+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$2$adapted(MongoSpark.scala:120)
[2023-01-26T21:08:41.152+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1011)
[2023-01-26T21:08:41.153+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1011)
[2023-01-26T21:08:41.153+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2268)
[2023-01-26T21:08:41.153+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
[2023-01-26T21:08:41.153+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.Task.run(Task.scala:136)
[2023-01-26T21:08:41.153+0000] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)
[2023-01-26T21:08:41.153+0000] {spark_submit.py:495} INFO - at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)
[2023-01-26T21:08:41.153+0000] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)
[2023-01-26T21:08:41.153+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[2023-01-26T21:08:41.153+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[2023-01-26T21:08:41.153+0000] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2023-01-26T21:08:41.153+0000] {spark_submit.py:495} INFO - [0m
[2023-01-26T21:08:41.234+0000] {spark_submit.py:495} INFO - [[34m2023-01-26 21:08:41,234[0m] {[34mclientserver.py:[0m577} INFO[0m - Received command c on object id p0[0m
[2023-01-26T21:09:11.316+0000] {spark_submit.py:495} INFO - 23/01/26 21:09:11 ERROR Executor: Exception in task 0.0 in stage 21.0 (TID 21)
[2023-01-26T21:09:11.316+0000] {spark_submit.py:495} INFO - com.mongodb.MongoTimeoutException: Timed out after 30000 ms while waiting to connect. Client view of cluster state is {type=UNKNOWN, servers=[{address=172.22.0.2:27017, type=UNKNOWN, state=CONNECTING, exception={com.mongodb.MongoSocketOpenException: Exception opening socket}, caused by {java.net.SocketTimeoutException: connect timed out}}]
[2023-01-26T21:09:11.316+0000] {spark_submit.py:495} INFO - at com.mongodb.internal.connection.BaseCluster.getDescription(BaseCluster.java:177)
[2023-01-26T21:09:11.316+0000] {spark_submit.py:495} INFO - at com.mongodb.internal.connection.SingleServerCluster.getDescription(SingleServerCluster.java:41)
[2023-01-26T21:09:11.316+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate.getConnectedClusterDescription(MongoClientDelegate.java:147)
[2023-01-26T21:09:11.316+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate.createClientSession(MongoClientDelegate.java:98)
[2023-01-26T21:09:11.316+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.getClientSession(MongoClientDelegate.java:278)
[2023-01-26T21:09:11.317+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:202)
[2023-01-26T21:09:11.317+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:441)
[2023-01-26T21:09:11.317+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:421)
[2023-01-26T21:09:11.317+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$4(MongoSpark.scala:138)
[2023-01-26T21:09:11.317+0000] {spark_submit.py:495} INFO - at scala.collection.Iterator.foreach(Iterator.scala:943)
[2023-01-26T21:09:11.317+0000] {spark_submit.py:495} INFO - at scala.collection.Iterator.foreach$(Iterator.scala:943)
[2023-01-26T21:09:11.317+0000] {spark_submit.py:495} INFO - at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
[2023-01-26T21:09:11.317+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$3(MongoSpark.scala:122)
[2023-01-26T21:09:11.317+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$3$adapted(MongoSpark.scala:121)
[2023-01-26T21:09:11.317+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.$anonfun$withCollectionDo$1(MongoConnector.scala:186)
[2023-01-26T21:09:11.317+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.$anonfun$withDatabaseDo$1(MongoConnector.scala:171)
[2023-01-26T21:09:11.318+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withMongoClientDo(MongoConnector.scala:154)
[2023-01-26T21:09:11.318+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withDatabaseDo(MongoConnector.scala:171)
[2023-01-26T21:09:11.318+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withCollectionDo(MongoConnector.scala:184)
[2023-01-26T21:09:11.318+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$2(MongoSpark.scala:121)
[2023-01-26T21:09:11.318+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$2$adapted(MongoSpark.scala:120)
[2023-01-26T21:09:11.318+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1011)
[2023-01-26T21:09:11.318+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1011)
[2023-01-26T21:09:11.318+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2268)
[2023-01-26T21:09:11.318+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
[2023-01-26T21:09:11.318+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.Task.run(Task.scala:136)
[2023-01-26T21:09:11.318+0000] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)
[2023-01-26T21:09:11.319+0000] {spark_submit.py:495} INFO - at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)
[2023-01-26T21:09:11.319+0000] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)
[2023-01-26T21:09:11.319+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[2023-01-26T21:09:11.319+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[2023-01-26T21:09:11.319+0000] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2023-01-26T21:09:11.319+0000] {spark_submit.py:495} INFO - 23/01/26 21:09:11 ERROR TaskSetManager: Task 0 in stage 21.0 failed 1 times; aborting job
[2023-01-26T21:09:11.473+0000] {spark_submit.py:495} INFO - [[34m2023-01-26 21:09:11,472[0m] {[34mconsumerSpark.py:[0m37} ERROR[0m - Error al escribir en Mongo: An error occurred while calling o239.save.
[2023-01-26T21:09:11.473+0000] {spark_submit.py:495} INFO - : org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 21.0 failed 1 times, most recent failure: Lost task 0.0 in stage 21.0 (TID 21) (f1cbb5df32e6 executor driver): com.mongodb.MongoTimeoutException: Timed out after 30000 ms while waiting to connect. Client view of cluster state is {type=UNKNOWN, servers=[{address=172.22.0.2:27017, type=UNKNOWN, state=CONNECTING, exception={com.mongodb.MongoSocketOpenException: Exception opening socket}, caused by {java.net.SocketTimeoutException: connect timed out}}]
[2023-01-26T21:09:11.474+0000] {spark_submit.py:495} INFO - at com.mongodb.internal.connection.BaseCluster.getDescription(BaseCluster.java:177)
[2023-01-26T21:09:11.474+0000] {spark_submit.py:495} INFO - at com.mongodb.internal.connection.SingleServerCluster.getDescription(SingleServerCluster.java:41)
[2023-01-26T21:09:11.474+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate.getConnectedClusterDescription(MongoClientDelegate.java:147)
[2023-01-26T21:09:11.474+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate.createClientSession(MongoClientDelegate.java:98)
[2023-01-26T21:09:11.475+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.getClientSession(MongoClientDelegate.java:278)
[2023-01-26T21:09:11.475+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:202)
[2023-01-26T21:09:11.475+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:441)
[2023-01-26T21:09:11.476+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:421)
[2023-01-26T21:09:11.476+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$4(MongoSpark.scala:138)
[2023-01-26T21:09:11.476+0000] {spark_submit.py:495} INFO - at scala.collection.Iterator.foreach(Iterator.scala:943)
[2023-01-26T21:09:11.477+0000] {spark_submit.py:495} INFO - at scala.collection.Iterator.foreach$(Iterator.scala:943)
[2023-01-26T21:09:11.477+0000] {spark_submit.py:495} INFO - at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
[2023-01-26T21:09:11.477+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$3(MongoSpark.scala:122)
[2023-01-26T21:09:11.477+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$3$adapted(MongoSpark.scala:121)
[2023-01-26T21:09:11.478+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.$anonfun$withCollectionDo$1(MongoConnector.scala:186)
[2023-01-26T21:09:11.478+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.$anonfun$withDatabaseDo$1(MongoConnector.scala:171)
[2023-01-26T21:09:11.478+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withMongoClientDo(MongoConnector.scala:154)
[2023-01-26T21:09:11.479+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withDatabaseDo(MongoConnector.scala:171)
[2023-01-26T21:09:11.479+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withCollectionDo(MongoConnector.scala:184)
[2023-01-26T21:09:11.479+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$2(MongoSpark.scala:121)
[2023-01-26T21:09:11.479+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$2$adapted(MongoSpark.scala:120)
[2023-01-26T21:09:11.480+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1011)
[2023-01-26T21:09:11.480+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1011)
[2023-01-26T21:09:11.480+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2268)
[2023-01-26T21:09:11.481+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
[2023-01-26T21:09:11.481+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.Task.run(Task.scala:136)
[2023-01-26T21:09:11.481+0000] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)
[2023-01-26T21:09:11.481+0000] {spark_submit.py:495} INFO - at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)
[2023-01-26T21:09:11.482+0000] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)
[2023-01-26T21:09:11.482+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[2023-01-26T21:09:11.482+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[2023-01-26T21:09:11.482+0000] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2023-01-26T21:09:11.483+0000] {spark_submit.py:495} INFO - 
[2023-01-26T21:09:11.483+0000] {spark_submit.py:495} INFO - Driver stacktrace:
[2023-01-26T21:09:11.483+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)
[2023-01-26T21:09:11.484+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)
[2023-01-26T21:09:11.484+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)
[2023-01-26T21:09:11.484+0000] {spark_submit.py:495} INFO - at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
[2023-01-26T21:09:11.485+0000] {spark_submit.py:495} INFO - at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
[2023-01-26T21:09:11.485+0000] {spark_submit.py:495} INFO - at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
[2023-01-26T21:09:11.485+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)
[2023-01-26T21:09:11.486+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)
[2023-01-26T21:09:11.486+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)
[2023-01-26T21:09:11.486+0000] {spark_submit.py:495} INFO - at scala.Option.foreach(Option.scala:407)
[2023-01-26T21:09:11.486+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)
[2023-01-26T21:09:11.487+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)
[2023-01-26T21:09:11.487+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)
[2023-01-26T21:09:11.488+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)
[2023-01-26T21:09:11.488+0000] {spark_submit.py:495} INFO - at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
[2023-01-26T21:09:11.488+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)
[2023-01-26T21:09:11.489+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.runJob(SparkContext.scala:2228)
[2023-01-26T21:09:11.489+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.runJob(SparkContext.scala:2249)
[2023-01-26T21:09:11.489+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.runJob(SparkContext.scala:2268)
[2023-01-26T21:09:11.489+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.runJob(SparkContext.scala:2293)
[2023-01-26T21:09:11.490+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$1(RDD.scala:1011)
[2023-01-26T21:09:11.490+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
[2023-01-26T21:09:11.490+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
[2023-01-26T21:09:11.491+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.withScope(RDD.scala:406)
[2023-01-26T21:09:11.491+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.foreachPartition(RDD.scala:1009)
[2023-01-26T21:09:11.491+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.save(MongoSpark.scala:120)
[2023-01-26T21:09:11.491+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.save(MongoSpark.scala:169)
[2023-01-26T21:09:11.492+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.sql.DefaultSource.createRelation(DefaultSource.scala:70)
[2023-01-26T21:09:11.492+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:47)
[2023-01-26T21:09:11.493+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)
[2023-01-26T21:09:11.493+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)
[2023-01-26T21:09:11.493+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)
[2023-01-26T21:09:11.493+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)
[2023-01-26T21:09:11.494+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)
[2023-01-26T21:09:11.494+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)
[2023-01-26T21:09:11.494+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)
[2023-01-26T21:09:11.495+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
[2023-01-26T21:09:11.495+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
[2023-01-26T21:09:11.495+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)
[2023-01-26T21:09:11.495+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)
[2023-01-26T21:09:11.496+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:584)
[2023-01-26T21:09:11.496+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:176)
[2023-01-26T21:09:11.496+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:584)
[2023-01-26T21:09:11.497+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)
[2023-01-26T21:09:11.497+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
[2023-01-26T21:09:11.497+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
[2023-01-26T21:09:11.497+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
[2023-01-26T21:09:11.498+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
[2023-01-26T21:09:11.498+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:560)
[2023-01-26T21:09:11.498+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)
[2023-01-26T21:09:11.499+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)
[2023-01-26T21:09:11.499+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)
[2023-01-26T21:09:11.499+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:116)
[2023-01-26T21:09:11.499+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:860)
[2023-01-26T21:09:11.500+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:390)
[2023-01-26T21:09:11.500+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:363)
[2023-01-26T21:09:11.500+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:247)
[2023-01-26T21:09:11.501+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
[2023-01-26T21:09:11.501+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
[2023-01-26T21:09:11.501+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
[2023-01-26T21:09:11.501+0000] {spark_submit.py:495} INFO - at java.base/java.lang.reflect.Method.invoke(Method.java:566)
[2023-01-26T21:09:11.502+0000] {spark_submit.py:495} INFO - at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
[2023-01-26T21:09:11.502+0000] {spark_submit.py:495} INFO - at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
[2023-01-26T21:09:11.502+0000] {spark_submit.py:495} INFO - at py4j.Gateway.invoke(Gateway.java:282)
[2023-01-26T21:09:11.502+0000] {spark_submit.py:495} INFO - at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
[2023-01-26T21:09:11.503+0000] {spark_submit.py:495} INFO - at py4j.commands.CallCommand.execute(CallCommand.java:79)
[2023-01-26T21:09:11.503+0000] {spark_submit.py:495} INFO - at py4j.ClientServerConnection.sendCommand(ClientServerConnection.java:244)
[2023-01-26T21:09:11.503+0000] {spark_submit.py:495} INFO - at py4j.CallbackClient.sendCommand(CallbackClient.java:384)
[2023-01-26T21:09:11.503+0000] {spark_submit.py:495} INFO - at py4j.CallbackClient.sendCommand(CallbackClient.java:356)
[2023-01-26T21:09:11.503+0000] {spark_submit.py:495} INFO - at py4j.reflection.PythonProxyHandler.invoke(PythonProxyHandler.java:106)
[2023-01-26T21:09:11.504+0000] {spark_submit.py:495} INFO - at com.sun.proxy.$Proxy32.call(Unknown Source)
[2023-01-26T21:09:11.504+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1(ForeachBatchSink.scala:51)
[2023-01-26T21:09:11.504+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1$adapted(ForeachBatchSink.scala:51)
[2023-01-26T21:09:11.504+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.sources.ForeachBatchSink.addBatch(ForeachBatchSink.scala:32)
[2023-01-26T21:09:11.505+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:666)
[2023-01-26T21:09:11.505+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)
[2023-01-26T21:09:11.505+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)
[2023-01-26T21:09:11.505+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)
[2023-01-26T21:09:11.505+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
[2023-01-26T21:09:11.506+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
[2023-01-26T21:09:11.506+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:664)
[2023-01-26T21:09:11.506+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:375)
[2023-01-26T21:09:11.506+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:373)
[2023-01-26T21:09:11.506+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:68)
[2023-01-26T21:09:11.506+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:664)
[2023-01-26T21:09:11.507+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:256)
[2023-01-26T21:09:11.507+0000] {spark_submit.py:495} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2023-01-26T21:09:11.507+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:375)
[2023-01-26T21:09:11.507+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:373)
[2023-01-26T21:09:11.507+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:68)
[2023-01-26T21:09:11.508+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:219)
[2023-01-26T21:09:11.508+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)
[2023-01-26T21:09:11.508+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:213)
[2023-01-26T21:09:11.508+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:307)
[2023-01-26T21:09:11.508+0000] {spark_submit.py:495} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2023-01-26T21:09:11.508+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
[2023-01-26T21:09:11.509+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:285)
[2023-01-26T21:09:11.509+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:208)
[2023-01-26T21:09:11.509+0000] {spark_submit.py:495} INFO - Caused by: com.mongodb.MongoTimeoutException: Timed out after 30000 ms while waiting to connect. Client view of cluster state is {type=UNKNOWN, servers=[{address=172.22.0.2:27017, type=UNKNOWN, state=CONNECTING, exception={com.mongodb.MongoSocketOpenException: Exception opening socket}, caused by {java.net.SocketTimeoutException: connect timed out}}]
[2023-01-26T21:09:11.509+0000] {spark_submit.py:495} INFO - at com.mongodb.internal.connection.BaseCluster.getDescription(BaseCluster.java:177)
[2023-01-26T21:09:11.509+0000] {spark_submit.py:495} INFO - at com.mongodb.internal.connection.SingleServerCluster.getDescription(SingleServerCluster.java:41)
[2023-01-26T21:09:11.509+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate.getConnectedClusterDescription(MongoClientDelegate.java:147)
[2023-01-26T21:09:11.509+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate.createClientSession(MongoClientDelegate.java:98)
[2023-01-26T21:09:11.510+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.getClientSession(MongoClientDelegate.java:278)
[2023-01-26T21:09:11.510+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:202)
[2023-01-26T21:09:11.510+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:441)
[2023-01-26T21:09:11.510+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:421)
[2023-01-26T21:09:11.510+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$4(MongoSpark.scala:138)
[2023-01-26T21:09:11.510+0000] {spark_submit.py:495} INFO - at scala.collection.Iterator.foreach(Iterator.scala:943)
[2023-01-26T21:09:11.510+0000] {spark_submit.py:495} INFO - at scala.collection.Iterator.foreach$(Iterator.scala:943)
[2023-01-26T21:09:11.511+0000] {spark_submit.py:495} INFO - at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
[2023-01-26T21:09:11.511+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$3(MongoSpark.scala:122)
[2023-01-26T21:09:11.511+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$3$adapted(MongoSpark.scala:121)
[2023-01-26T21:09:11.511+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.$anonfun$withCollectionDo$1(MongoConnector.scala:186)
[2023-01-26T21:09:11.511+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.$anonfun$withDatabaseDo$1(MongoConnector.scala:171)
[2023-01-26T21:09:11.511+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withMongoClientDo(MongoConnector.scala:154)
[2023-01-26T21:09:11.511+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withDatabaseDo(MongoConnector.scala:171)
[2023-01-26T21:09:11.512+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withCollectionDo(MongoConnector.scala:184)
[2023-01-26T21:09:11.512+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$2(MongoSpark.scala:121)
[2023-01-26T21:09:11.512+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$2$adapted(MongoSpark.scala:120)
[2023-01-26T21:09:11.512+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1011)
[2023-01-26T21:09:11.512+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1011)
[2023-01-26T21:09:11.512+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2268)
[2023-01-26T21:09:11.512+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
[2023-01-26T21:09:11.513+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.Task.run(Task.scala:136)
[2023-01-26T21:09:11.513+0000] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)
[2023-01-26T21:09:11.513+0000] {spark_submit.py:495} INFO - at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)
[2023-01-26T21:09:11.513+0000] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)
[2023-01-26T21:09:11.513+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[2023-01-26T21:09:11.513+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[2023-01-26T21:09:11.514+0000] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2023-01-26T21:09:11.514+0000] {spark_submit.py:495} INFO - [0m
[2023-01-26T21:11:55.351+0000] {spark_submit.py:495} INFO - [[34m2023-01-26 21:11:55,351[0m] {[34mclientserver.py:[0m577} INFO[0m - Received command c on object id p0[0m
[2023-01-26T21:12:00.748+0000] {spark_submit.py:495} INFO - -------------------------------------------
[2023-01-26T21:12:00.749+0000] {spark_submit.py:495} INFO - Batch: 8
[2023-01-26T21:12:00.749+0000] {spark_submit.py:495} INFO - -------------------------------------------
[2023-01-26T21:12:00.785+0000] {spark_submit.py:495} INFO - +-------------+--------------------+--------+-------------+--------------------+-------------------+------------+-----------------+--------------------+-------------+
[2023-01-26T21:12:00.786+0000] {spark_submit.py:495} INFO - |           id|       nameOfProduct|   price|ProductStatus|          SellerName|   RegistrationDate|QuantitySold|QuantityAvailable|      Free shipping?|Store ratings|
[2023-01-26T21:12:00.786+0000] {spark_submit.py:495} INFO - +-------------+--------------------+--------+-------------+--------------------+-------------------+------------+-----------------+--------------------+-------------+
[2023-01-26T21:12:00.786+0000] {spark_submit.py:495} INFO - |MLA1221192372|Disco Duro Extern...| 26299.0|          new|    OVERHARD DIGITAL|2015-01-08 12:19:41|         250|              100|Your shipment arr...|          4.5|
[2023-01-26T21:12:00.786+0000] {spark_submit.py:495} INFO - |MLA1157512489|Placa De Video Nv...|124999.0|          new|      PCREGISTRADAOK|2020-11-02 13:17:33|          50|                1|Your shipment arr...|          4.5|
[2023-01-26T21:12:00.786+0000] {spark_submit.py:495} INFO - |MLA1115295971|Procesador Amd Ry...| 93999.0|          new|      PCREGISTRADAOK|2020-11-02 13:17:33|          25|                1|Your shipment arr...|          4.5|
[2023-01-26T21:12:00.786+0000] {spark_submit.py:495} INFO - |MLA1163238194|Placa De Video Am...| 33999.0|          new|       MICYBERCOMPRA|2011-09-13 13:15:53|          50|                1|Your shipment arr...|          4.5|
[2023-01-26T21:12:00.786+0000] {spark_submit.py:495} INFO - |MLA1318244331|Disco Solido Ssd ...| 28799.0|          new|    OVERHARD DIGITAL|2015-01-08 12:19:41|           5|              150|Your shipment arr...|          4.5|
[2023-01-26T21:12:00.786+0000] {spark_submit.py:495} INFO - |MLA1148481665|Microprocesador P...| 58899.0|          new|            MALL WEB|2016-05-26 20:28:35|         150|                1|Your shipment arr...|          4.5|
[2023-01-26T21:12:00.786+0000] {spark_submit.py:495} INFO - |MLA1246852007|Memoria Ram Fury ...| 15199.2|          new|        NOXIESTORESA|2022-05-20 16:20:29|         250|               50|Your shipment arr...|          4.5|
[2023-01-26T21:12:00.786+0000] {spark_submit.py:495} INFO - | MLA923637669|Placa De Video Ge...| 28699.0|          new|         INDIGOTRADE|2018-01-05 19:21:16|         500|               50|Your shipment arr...|          4.5|
[2023-01-26T21:12:00.786+0000] {spark_submit.py:495} INFO - |MLA1133481354|Disco Sólido Inte...| 13799.0|          new|           DALECLÍCK|2015-03-16 12:07:23|         500|                1|You have to pay t...|          4.5|
[2023-01-26T21:12:00.787+0000] {spark_submit.py:495} INFO - |MLA1139583771|Disco Sólido Inte...|  8339.0|          new|DIAMONDSYSTEMPADUAML|2015-07-01 12:32:22|         250|              250|You have to pay t...|          4.5|
[2023-01-26T21:12:00.787+0000] {spark_submit.py:495} INFO - | MLA882277177|Capturadora Video...|  2699.0|          new|         BSK-IMPORTS|2019-12-30 23:07:04|         500|               50|You have to pay t...|          3.0|
[2023-01-26T21:12:00.787+0000] {spark_submit.py:495} INFO - |MLA1228990119|Memoria Ram Fury ...| 23255.0|          new|MARSTECHSRLMARSTE...|2020-08-05 14:27:59|          50|                1|Your shipment arr...|          4.5|
[2023-01-26T21:12:00.787+0000] {spark_submit.py:495} INFO - |MLA1183925519|Placa De Video Nv...|299999.0|          new|ESTUDIOSMPSRLESTU...|2020-11-29 18:36:32|          50|                1|Your shipment arr...|          4.5|
[2023-01-26T21:12:00.787+0000] {spark_submit.py:495} INFO - | MLA844489972|Disco Duro Intern...| 11599.0|          new|           DALECLÍCK|2015-03-16 12:07:23|        5000|              500|You have to pay t...|          5.0|
[2023-01-26T21:12:00.787+0000] {spark_submit.py:495} INFO - |MLA1168362675|Memoria Ram Fury ...| 26698.0|          new|MARSTECHSRLMARSTE...|2020-08-05 14:27:59|         150|                1|Your shipment arr...|          4.5|
[2023-01-26T21:12:00.787+0000] {spark_submit.py:495} INFO - |MLA1314590445|Disco Sólido Inte...|  7999.0|          new|       ELECTRO-SHOWS|2010-03-26 01:08:16|         250|              250|You have to pay t...|          4.5|
[2023-01-26T21:12:00.787+0000] {spark_submit.py:495} INFO - |MLA1163233412|Placa De Video Nv...| 34659.0|          new|      PCREGISTRADAOK|2020-11-02 13:17:33|           1|                1|Your shipment arr...|          4.5|
[2023-01-26T21:12:00.787+0000] {spark_submit.py:495} INFO - | MLA902808321|Procesador Gamer ...| 23699.0|          new|            MALL WEB|2016-05-26 20:28:35|         500|                1|Your shipment arr...|          4.5|
[2023-01-26T21:12:00.787+0000] {spark_submit.py:495} INFO - |MLA1302136118|Disco Duro Intern...| 18271.0|          new|DIAMONDSYSTEMSANJ...|2016-04-08 17:21:06|         100|               50|Your shipment arr...|          4.5|
[2023-01-26T21:12:00.787+0000] {spark_submit.py:495} INFO - |MLA1137238635|Fuente De Aliment...|  3235.0|          new|     DIAMONDSYSTEMML|2014-04-07 22:00:36|          25|                1|You have to pay t...|          3.5|
[2023-01-26T21:12:00.787+0000] {spark_submit.py:495} INFO - +-------------+--------------------+--------+-------------+--------------------+-------------------+------------+-----------------+--------------------+-------------+
[2023-01-26T21:12:00.787+0000] {spark_submit.py:495} INFO - only showing top 20 rows
[2023-01-26T21:12:00.787+0000] {spark_submit.py:495} INFO - 
[2023-01-26T21:12:26.008+0000] {spark_submit.py:495} INFO - 23/01/26 21:12:26 ERROR Executor: Exception in task 0.0 in stage 22.0 (TID 22)
[2023-01-26T21:12:26.009+0000] {spark_submit.py:495} INFO - com.mongodb.MongoTimeoutException: Timed out after 30000 ms while waiting to connect. Client view of cluster state is {type=UNKNOWN, servers=[{address=172.22.0.2:27017, type=UNKNOWN, state=CONNECTING, exception={com.mongodb.MongoSocketOpenException: Exception opening socket}, caused by {java.net.SocketTimeoutException: connect timed out}}]
[2023-01-26T21:12:26.009+0000] {spark_submit.py:495} INFO - at com.mongodb.internal.connection.BaseCluster.getDescription(BaseCluster.java:177)
[2023-01-26T21:12:26.009+0000] {spark_submit.py:495} INFO - at com.mongodb.internal.connection.SingleServerCluster.getDescription(SingleServerCluster.java:41)
[2023-01-26T21:12:26.010+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate.getConnectedClusterDescription(MongoClientDelegate.java:147)
[2023-01-26T21:12:26.010+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate.createClientSession(MongoClientDelegate.java:98)
[2023-01-26T21:12:26.010+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.getClientSession(MongoClientDelegate.java:278)
[2023-01-26T21:12:26.010+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:202)
[2023-01-26T21:12:26.011+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:441)
[2023-01-26T21:12:26.011+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:421)
[2023-01-26T21:12:26.011+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$4(MongoSpark.scala:138)
[2023-01-26T21:12:26.011+0000] {spark_submit.py:495} INFO - at scala.collection.Iterator.foreach(Iterator.scala:943)
[2023-01-26T21:12:26.012+0000] {spark_submit.py:495} INFO - at scala.collection.Iterator.foreach$(Iterator.scala:943)
[2023-01-26T21:12:26.012+0000] {spark_submit.py:495} INFO - at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
[2023-01-26T21:12:26.012+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$3(MongoSpark.scala:122)
[2023-01-26T21:12:26.012+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$3$adapted(MongoSpark.scala:121)
[2023-01-26T21:12:26.013+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.$anonfun$withCollectionDo$1(MongoConnector.scala:186)
[2023-01-26T21:12:26.014+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.$anonfun$withDatabaseDo$1(MongoConnector.scala:171)
[2023-01-26T21:12:26.014+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withMongoClientDo(MongoConnector.scala:154)
[2023-01-26T21:12:26.015+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withDatabaseDo(MongoConnector.scala:171)
[2023-01-26T21:12:26.015+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withCollectionDo(MongoConnector.scala:184)
[2023-01-26T21:12:26.016+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$2(MongoSpark.scala:121)
[2023-01-26T21:12:26.016+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$2$adapted(MongoSpark.scala:120)
[2023-01-26T21:12:26.016+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1011)
[2023-01-26T21:12:26.016+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1011)
[2023-01-26T21:12:26.017+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2268)
[2023-01-26T21:12:26.017+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
[2023-01-26T21:12:26.017+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.Task.run(Task.scala:136)
[2023-01-26T21:12:26.017+0000] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)
[2023-01-26T21:12:26.018+0000] {spark_submit.py:495} INFO - at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)
[2023-01-26T21:12:26.018+0000] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)
[2023-01-26T21:12:26.018+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[2023-01-26T21:12:26.018+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[2023-01-26T21:12:26.019+0000] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2023-01-26T21:12:26.019+0000] {spark_submit.py:495} INFO - 23/01/26 21:12:26 ERROR TaskSetManager: Task 0 in stage 22.0 failed 1 times; aborting job
[2023-01-26T21:12:26.192+0000] {spark_submit.py:495} INFO - [[34m2023-01-26 21:12:26,192[0m] {[34mconsumerSpark.py:[0m37} ERROR[0m - Error al escribir en Mongo: An error occurred while calling o248.save.
[2023-01-26T21:12:26.193+0000] {spark_submit.py:495} INFO - : org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 22.0 failed 1 times, most recent failure: Lost task 0.0 in stage 22.0 (TID 22) (f1cbb5df32e6 executor driver): com.mongodb.MongoTimeoutException: Timed out after 30000 ms while waiting to connect. Client view of cluster state is {type=UNKNOWN, servers=[{address=172.22.0.2:27017, type=UNKNOWN, state=CONNECTING, exception={com.mongodb.MongoSocketOpenException: Exception opening socket}, caused by {java.net.SocketTimeoutException: connect timed out}}]
[2023-01-26T21:12:26.193+0000] {spark_submit.py:495} INFO - at com.mongodb.internal.connection.BaseCluster.getDescription(BaseCluster.java:177)
[2023-01-26T21:12:26.193+0000] {spark_submit.py:495} INFO - at com.mongodb.internal.connection.SingleServerCluster.getDescription(SingleServerCluster.java:41)
[2023-01-26T21:12:26.193+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate.getConnectedClusterDescription(MongoClientDelegate.java:147)
[2023-01-26T21:12:26.193+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate.createClientSession(MongoClientDelegate.java:98)
[2023-01-26T21:12:26.193+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.getClientSession(MongoClientDelegate.java:278)
[2023-01-26T21:12:26.193+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:202)
[2023-01-26T21:12:26.193+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:441)
[2023-01-26T21:12:26.193+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:421)
[2023-01-26T21:12:26.193+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$4(MongoSpark.scala:138)
[2023-01-26T21:12:26.193+0000] {spark_submit.py:495} INFO - at scala.collection.Iterator.foreach(Iterator.scala:943)
[2023-01-26T21:12:26.194+0000] {spark_submit.py:495} INFO - at scala.collection.Iterator.foreach$(Iterator.scala:943)
[2023-01-26T21:12:26.194+0000] {spark_submit.py:495} INFO - at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
[2023-01-26T21:12:26.194+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$3(MongoSpark.scala:122)
[2023-01-26T21:12:26.194+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$3$adapted(MongoSpark.scala:121)
[2023-01-26T21:12:26.194+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.$anonfun$withCollectionDo$1(MongoConnector.scala:186)
[2023-01-26T21:12:26.194+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.$anonfun$withDatabaseDo$1(MongoConnector.scala:171)
[2023-01-26T21:12:26.194+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withMongoClientDo(MongoConnector.scala:154)
[2023-01-26T21:12:26.194+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withDatabaseDo(MongoConnector.scala:171)
[2023-01-26T21:12:26.194+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withCollectionDo(MongoConnector.scala:184)
[2023-01-26T21:12:26.194+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$2(MongoSpark.scala:121)
[2023-01-26T21:12:26.194+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$2$adapted(MongoSpark.scala:120)
[2023-01-26T21:12:26.194+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1011)
[2023-01-26T21:12:26.195+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1011)
[2023-01-26T21:12:26.195+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2268)
[2023-01-26T21:12:26.195+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
[2023-01-26T21:12:26.195+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.Task.run(Task.scala:136)
[2023-01-26T21:12:26.195+0000] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)
[2023-01-26T21:12:26.195+0000] {spark_submit.py:495} INFO - at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)
[2023-01-26T21:12:26.195+0000] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)
[2023-01-26T21:12:26.195+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[2023-01-26T21:12:26.195+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[2023-01-26T21:12:26.195+0000] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2023-01-26T21:12:26.195+0000] {spark_submit.py:495} INFO - 
[2023-01-26T21:12:26.195+0000] {spark_submit.py:495} INFO - Driver stacktrace:
[2023-01-26T21:12:26.196+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)
[2023-01-26T21:12:26.196+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)
[2023-01-26T21:12:26.196+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)
[2023-01-26T21:12:26.196+0000] {spark_submit.py:495} INFO - at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
[2023-01-26T21:12:26.196+0000] {spark_submit.py:495} INFO - at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
[2023-01-26T21:12:26.196+0000] {spark_submit.py:495} INFO - at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
[2023-01-26T21:12:26.196+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)
[2023-01-26T21:12:26.196+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)
[2023-01-26T21:12:26.196+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)
[2023-01-26T21:12:26.196+0000] {spark_submit.py:495} INFO - at scala.Option.foreach(Option.scala:407)
[2023-01-26T21:12:26.196+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)
[2023-01-26T21:12:26.196+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)
[2023-01-26T21:12:26.196+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)
[2023-01-26T21:12:26.196+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)
[2023-01-26T21:12:26.196+0000] {spark_submit.py:495} INFO - at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
[2023-01-26T21:12:26.197+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)
[2023-01-26T21:12:26.197+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.runJob(SparkContext.scala:2228)
[2023-01-26T21:12:26.197+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.runJob(SparkContext.scala:2249)
[2023-01-26T21:12:26.197+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.runJob(SparkContext.scala:2268)
[2023-01-26T21:12:26.197+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.runJob(SparkContext.scala:2293)
[2023-01-26T21:12:26.197+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$1(RDD.scala:1011)
[2023-01-26T21:12:26.197+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
[2023-01-26T21:12:26.197+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
[2023-01-26T21:12:26.197+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.withScope(RDD.scala:406)
[2023-01-26T21:12:26.197+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.foreachPartition(RDD.scala:1009)
[2023-01-26T21:12:26.197+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.save(MongoSpark.scala:120)
[2023-01-26T21:12:26.197+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.save(MongoSpark.scala:169)
[2023-01-26T21:12:26.197+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.sql.DefaultSource.createRelation(DefaultSource.scala:70)
[2023-01-26T21:12:26.197+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:47)
[2023-01-26T21:12:26.197+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)
[2023-01-26T21:12:26.198+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)
[2023-01-26T21:12:26.199+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)
[2023-01-26T21:12:26.199+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)
[2023-01-26T21:12:26.199+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)
[2023-01-26T21:12:26.199+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)
[2023-01-26T21:12:26.199+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)
[2023-01-26T21:12:26.199+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
[2023-01-26T21:12:26.199+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
[2023-01-26T21:12:26.199+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)
[2023-01-26T21:12:26.199+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)
[2023-01-26T21:12:26.200+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:584)
[2023-01-26T21:12:26.200+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:176)
[2023-01-26T21:12:26.200+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:584)
[2023-01-26T21:12:26.200+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)
[2023-01-26T21:12:26.200+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
[2023-01-26T21:12:26.200+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
[2023-01-26T21:12:26.200+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
[2023-01-26T21:12:26.200+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
[2023-01-26T21:12:26.200+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:560)
[2023-01-26T21:12:26.200+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)
[2023-01-26T21:12:26.200+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)
[2023-01-26T21:12:26.200+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)
[2023-01-26T21:12:26.200+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:116)
[2023-01-26T21:12:26.201+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:860)
[2023-01-26T21:12:26.201+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:390)
[2023-01-26T21:12:26.201+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:363)
[2023-01-26T21:12:26.201+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:247)
[2023-01-26T21:12:26.201+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
[2023-01-26T21:12:26.201+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
[2023-01-26T21:12:26.201+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
[2023-01-26T21:12:26.201+0000] {spark_submit.py:495} INFO - at java.base/java.lang.reflect.Method.invoke(Method.java:566)
[2023-01-26T21:12:26.201+0000] {spark_submit.py:495} INFO - at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
[2023-01-26T21:12:26.201+0000] {spark_submit.py:495} INFO - at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
[2023-01-26T21:12:26.201+0000] {spark_submit.py:495} INFO - at py4j.Gateway.invoke(Gateway.java:282)
[2023-01-26T21:12:26.201+0000] {spark_submit.py:495} INFO - at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
[2023-01-26T21:12:26.201+0000] {spark_submit.py:495} INFO - at py4j.commands.CallCommand.execute(CallCommand.java:79)
[2023-01-26T21:12:26.202+0000] {spark_submit.py:495} INFO - at py4j.ClientServerConnection.sendCommand(ClientServerConnection.java:244)
[2023-01-26T21:12:26.202+0000] {spark_submit.py:495} INFO - at py4j.CallbackClient.sendCommand(CallbackClient.java:384)
[2023-01-26T21:12:26.202+0000] {spark_submit.py:495} INFO - at py4j.CallbackClient.sendCommand(CallbackClient.java:356)
[2023-01-26T21:12:26.202+0000] {spark_submit.py:495} INFO - at py4j.reflection.PythonProxyHandler.invoke(PythonProxyHandler.java:106)
[2023-01-26T21:12:26.202+0000] {spark_submit.py:495} INFO - at com.sun.proxy.$Proxy32.call(Unknown Source)
[2023-01-26T21:12:26.202+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1(ForeachBatchSink.scala:51)
[2023-01-26T21:12:26.202+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1$adapted(ForeachBatchSink.scala:51)
[2023-01-26T21:12:26.202+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.sources.ForeachBatchSink.addBatch(ForeachBatchSink.scala:32)
[2023-01-26T21:12:26.202+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:666)
[2023-01-26T21:12:26.202+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)
[2023-01-26T21:12:26.202+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)
[2023-01-26T21:12:26.202+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)
[2023-01-26T21:12:26.202+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
[2023-01-26T21:12:26.203+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
[2023-01-26T21:12:26.203+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:664)
[2023-01-26T21:12:26.203+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:375)
[2023-01-26T21:12:26.203+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:373)
[2023-01-26T21:12:26.203+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:68)
[2023-01-26T21:12:26.203+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:664)
[2023-01-26T21:12:26.203+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:256)
[2023-01-26T21:12:26.203+0000] {spark_submit.py:495} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2023-01-26T21:12:26.203+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:375)
[2023-01-26T21:12:26.203+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:373)
[2023-01-26T21:12:26.203+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:68)
[2023-01-26T21:12:26.203+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:219)
[2023-01-26T21:12:26.203+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)
[2023-01-26T21:12:26.204+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:213)
[2023-01-26T21:12:26.204+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:307)
[2023-01-26T21:12:26.204+0000] {spark_submit.py:495} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2023-01-26T21:12:26.204+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
[2023-01-26T21:12:26.204+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:285)
[2023-01-26T21:12:26.204+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:208)
[2023-01-26T21:12:26.204+0000] {spark_submit.py:495} INFO - Caused by: com.mongodb.MongoTimeoutException: Timed out after 30000 ms while waiting to connect. Client view of cluster state is {type=UNKNOWN, servers=[{address=172.22.0.2:27017, type=UNKNOWN, state=CONNECTING, exception={com.mongodb.MongoSocketOpenException: Exception opening socket}, caused by {java.net.SocketTimeoutException: connect timed out}}]
[2023-01-26T21:12:26.204+0000] {spark_submit.py:495} INFO - at com.mongodb.internal.connection.BaseCluster.getDescription(BaseCluster.java:177)
[2023-01-26T21:12:26.204+0000] {spark_submit.py:495} INFO - at com.mongodb.internal.connection.SingleServerCluster.getDescription(SingleServerCluster.java:41)
[2023-01-26T21:12:26.204+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate.getConnectedClusterDescription(MongoClientDelegate.java:147)
[2023-01-26T21:12:26.204+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate.createClientSession(MongoClientDelegate.java:98)
[2023-01-26T21:12:26.204+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.getClientSession(MongoClientDelegate.java:278)
[2023-01-26T21:12:26.204+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:202)
[2023-01-26T21:12:26.204+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:441)
[2023-01-26T21:12:26.205+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:421)
[2023-01-26T21:12:26.205+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$4(MongoSpark.scala:138)
[2023-01-26T21:12:26.205+0000] {spark_submit.py:495} INFO - at scala.collection.Iterator.foreach(Iterator.scala:943)
[2023-01-26T21:12:26.205+0000] {spark_submit.py:495} INFO - at scala.collection.Iterator.foreach$(Iterator.scala:943)
[2023-01-26T21:12:26.205+0000] {spark_submit.py:495} INFO - at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
[2023-01-26T21:12:26.205+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$3(MongoSpark.scala:122)
[2023-01-26T21:12:26.205+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$3$adapted(MongoSpark.scala:121)
[2023-01-26T21:12:26.205+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.$anonfun$withCollectionDo$1(MongoConnector.scala:186)
[2023-01-26T21:12:26.205+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.$anonfun$withDatabaseDo$1(MongoConnector.scala:171)
[2023-01-26T21:12:26.205+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withMongoClientDo(MongoConnector.scala:154)
[2023-01-26T21:12:26.205+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withDatabaseDo(MongoConnector.scala:171)
[2023-01-26T21:12:26.205+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withCollectionDo(MongoConnector.scala:184)
[2023-01-26T21:12:26.205+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$2(MongoSpark.scala:121)
[2023-01-26T21:12:26.205+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$2$adapted(MongoSpark.scala:120)
[2023-01-26T21:12:26.205+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1011)
[2023-01-26T21:12:26.206+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1011)
[2023-01-26T21:12:26.206+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2268)
[2023-01-26T21:12:26.206+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
[2023-01-26T21:12:26.206+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.Task.run(Task.scala:136)
[2023-01-26T21:12:26.206+0000] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)
[2023-01-26T21:12:26.206+0000] {spark_submit.py:495} INFO - at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)
[2023-01-26T21:12:26.206+0000] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)
[2023-01-26T21:12:26.206+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[2023-01-26T21:12:26.206+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[2023-01-26T21:12:26.207+0000] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2023-01-26T21:12:26.207+0000] {spark_submit.py:495} INFO - [0m
[2023-01-26T21:12:26.283+0000] {spark_submit.py:495} INFO - [[34m2023-01-26 21:12:26,283[0m] {[34mclientserver.py:[0m577} INFO[0m - Received command c on object id p0[0m
[2023-01-26T21:12:56.353+0000] {spark_submit.py:495} INFO - 23/01/26 21:12:56 ERROR Executor: Exception in task 0.0 in stage 24.0 (TID 24)
[2023-01-26T21:12:56.354+0000] {spark_submit.py:495} INFO - com.mongodb.MongoTimeoutException: Timed out after 30000 ms while waiting to connect. Client view of cluster state is {type=UNKNOWN, servers=[{address=172.22.0.2:27017, type=UNKNOWN, state=CONNECTING, exception={com.mongodb.MongoSocketOpenException: Exception opening socket}, caused by {java.net.SocketTimeoutException: connect timed out}}]
[2023-01-26T21:12:56.354+0000] {spark_submit.py:495} INFO - at com.mongodb.internal.connection.BaseCluster.getDescription(BaseCluster.java:177)
[2023-01-26T21:12:56.355+0000] {spark_submit.py:495} INFO - at com.mongodb.internal.connection.SingleServerCluster.getDescription(SingleServerCluster.java:41)
[2023-01-26T21:12:56.355+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate.getConnectedClusterDescription(MongoClientDelegate.java:147)
[2023-01-26T21:12:56.355+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate.createClientSession(MongoClientDelegate.java:98)
[2023-01-26T21:12:56.356+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.getClientSession(MongoClientDelegate.java:278)
[2023-01-26T21:12:56.356+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:202)
[2023-01-26T21:12:56.357+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:441)
[2023-01-26T21:12:56.357+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:421)
[2023-01-26T21:12:56.357+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$4(MongoSpark.scala:138)
[2023-01-26T21:12:56.358+0000] {spark_submit.py:495} INFO - at scala.collection.Iterator.foreach(Iterator.scala:943)
[2023-01-26T21:12:56.358+0000] {spark_submit.py:495} INFO - at scala.collection.Iterator.foreach$(Iterator.scala:943)
[2023-01-26T21:12:56.358+0000] {spark_submit.py:495} INFO - at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
[2023-01-26T21:12:56.359+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$3(MongoSpark.scala:122)
[2023-01-26T21:12:56.359+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$3$adapted(MongoSpark.scala:121)
[2023-01-26T21:12:56.359+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.$anonfun$withCollectionDo$1(MongoConnector.scala:186)
[2023-01-26T21:12:56.360+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.$anonfun$withDatabaseDo$1(MongoConnector.scala:171)
[2023-01-26T21:12:56.360+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withMongoClientDo(MongoConnector.scala:154)
[2023-01-26T21:12:56.360+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withDatabaseDo(MongoConnector.scala:171)
[2023-01-26T21:12:56.363+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withCollectionDo(MongoConnector.scala:184)
[2023-01-26T21:12:56.364+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$2(MongoSpark.scala:121)
[2023-01-26T21:12:56.365+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$2$adapted(MongoSpark.scala:120)
[2023-01-26T21:12:56.365+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1011)
[2023-01-26T21:12:56.366+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1011)
[2023-01-26T21:12:56.366+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2268)
[2023-01-26T21:12:56.367+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
[2023-01-26T21:12:56.367+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.Task.run(Task.scala:136)
[2023-01-26T21:12:56.367+0000] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)
[2023-01-26T21:12:56.368+0000] {spark_submit.py:495} INFO - at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)
[2023-01-26T21:12:56.369+0000] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)
[2023-01-26T21:12:56.369+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[2023-01-26T21:12:56.370+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[2023-01-26T21:12:56.370+0000] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2023-01-26T21:12:56.370+0000] {spark_submit.py:495} INFO - 23/01/26 21:12:56 ERROR TaskSetManager: Task 0 in stage 24.0 failed 1 times; aborting job
[2023-01-26T21:12:56.537+0000] {spark_submit.py:495} INFO - [[34m2023-01-26 21:12:56,536[0m] {[34mconsumerSpark.py:[0m37} ERROR[0m - Error al escribir en Mongo: An error occurred while calling o257.save.
[2023-01-26T21:12:56.537+0000] {spark_submit.py:495} INFO - : org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 24.0 failed 1 times, most recent failure: Lost task 0.0 in stage 24.0 (TID 24) (f1cbb5df32e6 executor driver): com.mongodb.MongoTimeoutException: Timed out after 30000 ms while waiting to connect. Client view of cluster state is {type=UNKNOWN, servers=[{address=172.22.0.2:27017, type=UNKNOWN, state=CONNECTING, exception={com.mongodb.MongoSocketOpenException: Exception opening socket}, caused by {java.net.SocketTimeoutException: connect timed out}}]
[2023-01-26T21:12:56.537+0000] {spark_submit.py:495} INFO - at com.mongodb.internal.connection.BaseCluster.getDescription(BaseCluster.java:177)
[2023-01-26T21:12:56.537+0000] {spark_submit.py:495} INFO - at com.mongodb.internal.connection.SingleServerCluster.getDescription(SingleServerCluster.java:41)
[2023-01-26T21:12:56.537+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate.getConnectedClusterDescription(MongoClientDelegate.java:147)
[2023-01-26T21:12:56.537+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate.createClientSession(MongoClientDelegate.java:98)
[2023-01-26T21:12:56.537+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.getClientSession(MongoClientDelegate.java:278)
[2023-01-26T21:12:56.538+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:202)
[2023-01-26T21:12:56.538+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:441)
[2023-01-26T21:12:56.538+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:421)
[2023-01-26T21:12:56.538+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$4(MongoSpark.scala:138)
[2023-01-26T21:12:56.538+0000] {spark_submit.py:495} INFO - at scala.collection.Iterator.foreach(Iterator.scala:943)
[2023-01-26T21:12:56.538+0000] {spark_submit.py:495} INFO - at scala.collection.Iterator.foreach$(Iterator.scala:943)
[2023-01-26T21:12:56.538+0000] {spark_submit.py:495} INFO - at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
[2023-01-26T21:12:56.538+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$3(MongoSpark.scala:122)
[2023-01-26T21:12:56.538+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$3$adapted(MongoSpark.scala:121)
[2023-01-26T21:12:56.538+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.$anonfun$withCollectionDo$1(MongoConnector.scala:186)
[2023-01-26T21:12:56.539+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.$anonfun$withDatabaseDo$1(MongoConnector.scala:171)
[2023-01-26T21:12:56.539+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withMongoClientDo(MongoConnector.scala:154)
[2023-01-26T21:12:56.539+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withDatabaseDo(MongoConnector.scala:171)
[2023-01-26T21:12:56.539+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withCollectionDo(MongoConnector.scala:184)
[2023-01-26T21:12:56.539+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$2(MongoSpark.scala:121)
[2023-01-26T21:12:56.539+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$2$adapted(MongoSpark.scala:120)
[2023-01-26T21:12:56.539+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1011)
[2023-01-26T21:12:56.539+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1011)
[2023-01-26T21:12:56.539+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2268)
[2023-01-26T21:12:56.540+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
[2023-01-26T21:12:56.540+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.Task.run(Task.scala:136)
[2023-01-26T21:12:56.540+0000] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)
[2023-01-26T21:12:56.540+0000] {spark_submit.py:495} INFO - at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)
[2023-01-26T21:12:56.540+0000] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)
[2023-01-26T21:12:56.540+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[2023-01-26T21:12:56.540+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[2023-01-26T21:12:56.540+0000] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2023-01-26T21:12:56.540+0000] {spark_submit.py:495} INFO - 
[2023-01-26T21:12:56.541+0000] {spark_submit.py:495} INFO - Driver stacktrace:
[2023-01-26T21:12:56.541+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)
[2023-01-26T21:12:56.541+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)
[2023-01-26T21:12:56.541+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)
[2023-01-26T21:12:56.541+0000] {spark_submit.py:495} INFO - at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
[2023-01-26T21:12:56.541+0000] {spark_submit.py:495} INFO - at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
[2023-01-26T21:12:56.541+0000] {spark_submit.py:495} INFO - at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
[2023-01-26T21:12:56.541+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)
[2023-01-26T21:12:56.541+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)
[2023-01-26T21:12:56.541+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)
[2023-01-26T21:12:56.542+0000] {spark_submit.py:495} INFO - at scala.Option.foreach(Option.scala:407)
[2023-01-26T21:12:56.542+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)
[2023-01-26T21:12:56.542+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)
[2023-01-26T21:12:56.542+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)
[2023-01-26T21:12:56.542+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)
[2023-01-26T21:12:56.542+0000] {spark_submit.py:495} INFO - at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
[2023-01-26T21:12:56.542+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)
[2023-01-26T21:12:56.542+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.runJob(SparkContext.scala:2228)
[2023-01-26T21:12:56.542+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.runJob(SparkContext.scala:2249)
[2023-01-26T21:12:56.543+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.runJob(SparkContext.scala:2268)
[2023-01-26T21:12:56.543+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.runJob(SparkContext.scala:2293)
[2023-01-26T21:12:56.543+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$1(RDD.scala:1011)
[2023-01-26T21:12:56.543+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
[2023-01-26T21:12:56.543+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
[2023-01-26T21:12:56.543+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.withScope(RDD.scala:406)
[2023-01-26T21:12:56.543+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.foreachPartition(RDD.scala:1009)
[2023-01-26T21:12:56.543+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.save(MongoSpark.scala:120)
[2023-01-26T21:12:56.543+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.save(MongoSpark.scala:169)
[2023-01-26T21:12:56.543+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.sql.DefaultSource.createRelation(DefaultSource.scala:70)
[2023-01-26T21:12:56.544+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:47)
[2023-01-26T21:12:56.544+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)
[2023-01-26T21:12:56.544+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)
[2023-01-26T21:12:56.544+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)
[2023-01-26T21:12:56.544+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)
[2023-01-26T21:12:56.544+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)
[2023-01-26T21:12:56.544+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)
[2023-01-26T21:12:56.544+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)
[2023-01-26T21:12:56.545+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
[2023-01-26T21:12:56.545+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
[2023-01-26T21:12:56.545+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)
[2023-01-26T21:12:56.545+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)
[2023-01-26T21:12:56.545+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:584)
[2023-01-26T21:12:56.545+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:176)
[2023-01-26T21:12:56.545+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:584)
[2023-01-26T21:12:56.546+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)
[2023-01-26T21:12:56.546+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
[2023-01-26T21:12:56.546+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
[2023-01-26T21:12:56.546+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
[2023-01-26T21:12:56.546+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
[2023-01-26T21:12:56.546+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:560)
[2023-01-26T21:12:56.546+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)
[2023-01-26T21:12:56.546+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)
[2023-01-26T21:12:56.546+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)
[2023-01-26T21:12:56.546+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:116)
[2023-01-26T21:12:56.547+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:860)
[2023-01-26T21:12:56.547+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:390)
[2023-01-26T21:12:56.547+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:363)
[2023-01-26T21:12:56.547+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:247)
[2023-01-26T21:12:56.547+0000] {spark_submit.py:495} INFO - at jdk.internal.reflect.GeneratedMethodAccessor69.invoke(Unknown Source)
[2023-01-26T21:12:56.547+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
[2023-01-26T21:12:56.547+0000] {spark_submit.py:495} INFO - at java.base/java.lang.reflect.Method.invoke(Method.java:566)
[2023-01-26T21:12:56.547+0000] {spark_submit.py:495} INFO - at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
[2023-01-26T21:12:56.547+0000] {spark_submit.py:495} INFO - at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
[2023-01-26T21:12:56.548+0000] {spark_submit.py:495} INFO - at py4j.Gateway.invoke(Gateway.java:282)
[2023-01-26T21:12:56.548+0000] {spark_submit.py:495} INFO - at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
[2023-01-26T21:12:56.548+0000] {spark_submit.py:495} INFO - at py4j.commands.CallCommand.execute(CallCommand.java:79)
[2023-01-26T21:12:56.548+0000] {spark_submit.py:495} INFO - at py4j.ClientServerConnection.sendCommand(ClientServerConnection.java:244)
[2023-01-26T21:12:56.548+0000] {spark_submit.py:495} INFO - at py4j.CallbackClient.sendCommand(CallbackClient.java:384)
[2023-01-26T21:12:56.548+0000] {spark_submit.py:495} INFO - at py4j.CallbackClient.sendCommand(CallbackClient.java:356)
[2023-01-26T21:12:56.548+0000] {spark_submit.py:495} INFO - at py4j.reflection.PythonProxyHandler.invoke(PythonProxyHandler.java:106)
[2023-01-26T21:12:56.548+0000] {spark_submit.py:495} INFO - at com.sun.proxy.$Proxy32.call(Unknown Source)
[2023-01-26T21:12:56.548+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1(ForeachBatchSink.scala:51)
[2023-01-26T21:12:56.549+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1$adapted(ForeachBatchSink.scala:51)
[2023-01-26T21:12:56.549+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.sources.ForeachBatchSink.addBatch(ForeachBatchSink.scala:32)
[2023-01-26T21:12:56.549+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:666)
[2023-01-26T21:12:56.549+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)
[2023-01-26T21:12:56.549+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)
[2023-01-26T21:12:56.549+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)
[2023-01-26T21:12:56.549+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
[2023-01-26T21:12:56.549+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
[2023-01-26T21:12:56.549+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:664)
[2023-01-26T21:12:56.550+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:375)
[2023-01-26T21:12:56.550+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:373)
[2023-01-26T21:12:56.550+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:68)
[2023-01-26T21:12:56.550+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:664)
[2023-01-26T21:12:56.550+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:256)
[2023-01-26T21:12:56.550+0000] {spark_submit.py:495} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2023-01-26T21:12:56.550+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:375)
[2023-01-26T21:12:56.550+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:373)
[2023-01-26T21:12:56.550+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:68)
[2023-01-26T21:12:56.550+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:219)
[2023-01-26T21:12:56.551+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)
[2023-01-26T21:12:56.551+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:213)
[2023-01-26T21:12:56.551+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:307)
[2023-01-26T21:12:56.551+0000] {spark_submit.py:495} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2023-01-26T21:12:56.551+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
[2023-01-26T21:12:56.551+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:285)
[2023-01-26T21:12:56.551+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:208)
[2023-01-26T21:12:56.551+0000] {spark_submit.py:495} INFO - Caused by: com.mongodb.MongoTimeoutException: Timed out after 30000 ms while waiting to connect. Client view of cluster state is {type=UNKNOWN, servers=[{address=172.22.0.2:27017, type=UNKNOWN, state=CONNECTING, exception={com.mongodb.MongoSocketOpenException: Exception opening socket}, caused by {java.net.SocketTimeoutException: connect timed out}}]
[2023-01-26T21:12:56.551+0000] {spark_submit.py:495} INFO - at com.mongodb.internal.connection.BaseCluster.getDescription(BaseCluster.java:177)
[2023-01-26T21:12:56.552+0000] {spark_submit.py:495} INFO - at com.mongodb.internal.connection.SingleServerCluster.getDescription(SingleServerCluster.java:41)
[2023-01-26T21:12:56.552+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate.getConnectedClusterDescription(MongoClientDelegate.java:147)
[2023-01-26T21:12:56.552+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate.createClientSession(MongoClientDelegate.java:98)
[2023-01-26T21:12:56.552+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.getClientSession(MongoClientDelegate.java:278)
[2023-01-26T21:12:56.552+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:202)
[2023-01-26T21:12:56.552+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:441)
[2023-01-26T21:12:56.552+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:421)
[2023-01-26T21:12:56.552+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$4(MongoSpark.scala:138)
[2023-01-26T21:12:56.552+0000] {spark_submit.py:495} INFO - at scala.collection.Iterator.foreach(Iterator.scala:943)
[2023-01-26T21:12:56.553+0000] {spark_submit.py:495} INFO - at scala.collection.Iterator.foreach$(Iterator.scala:943)
[2023-01-26T21:12:56.553+0000] {spark_submit.py:495} INFO - at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
[2023-01-26T21:12:56.553+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$3(MongoSpark.scala:122)
[2023-01-26T21:12:56.553+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$3$adapted(MongoSpark.scala:121)
[2023-01-26T21:12:56.553+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.$anonfun$withCollectionDo$1(MongoConnector.scala:186)
[2023-01-26T21:12:56.553+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.$anonfun$withDatabaseDo$1(MongoConnector.scala:171)
[2023-01-26T21:12:56.553+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withMongoClientDo(MongoConnector.scala:154)
[2023-01-26T21:12:56.553+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withDatabaseDo(MongoConnector.scala:171)
[2023-01-26T21:12:56.553+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withCollectionDo(MongoConnector.scala:184)
[2023-01-26T21:12:56.553+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$2(MongoSpark.scala:121)
[2023-01-26T21:12:56.554+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$2$adapted(MongoSpark.scala:120)
[2023-01-26T21:12:56.554+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1011)
[2023-01-26T21:12:56.554+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1011)
[2023-01-26T21:12:56.554+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2268)
[2023-01-26T21:12:56.554+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
[2023-01-26T21:12:56.554+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.Task.run(Task.scala:136)
[2023-01-26T21:12:56.554+0000] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)
[2023-01-26T21:12:56.554+0000] {spark_submit.py:495} INFO - at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)
[2023-01-26T21:12:56.554+0000] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)
[2023-01-26T21:12:56.555+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[2023-01-26T21:12:56.555+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[2023-01-26T21:12:56.555+0000] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2023-01-26T21:12:56.555+0000] {spark_submit.py:495} INFO - [0m
[2023-01-26T21:15:39.150+0000] {spark_submit.py:495} INFO - [[34m2023-01-26 21:15:39,150[0m] {[34mclientserver.py:[0m577} INFO[0m - Received command c on object id p0[0m
[2023-01-26T21:15:40.748+0000] {spark_submit.py:495} INFO - -------------------------------------------
[2023-01-26T21:15:40.748+0000] {spark_submit.py:495} INFO - Batch: 9
[2023-01-26T21:15:40.748+0000] {spark_submit.py:495} INFO - -------------------------------------------
[2023-01-26T21:15:40.773+0000] {spark_submit.py:495} INFO - +-------------+--------------------+--------+-------------+--------------------+-------------------+------------+-----------------+--------------------+-------------+
[2023-01-26T21:15:40.773+0000] {spark_submit.py:495} INFO - |           id|       nameOfProduct|   price|ProductStatus|          SellerName|   RegistrationDate|QuantitySold|QuantityAvailable|      Free shipping?|Store ratings|
[2023-01-26T21:15:40.773+0000] {spark_submit.py:495} INFO - +-------------+--------------------+--------+-------------+--------------------+-------------------+------------+-----------------+--------------------+-------------+
[2023-01-26T21:15:40.773+0000] {spark_submit.py:495} INFO - |MLA1221192372|Disco Duro Extern...| 26299.0|          new|    OVERHARD DIGITAL|2015-01-08 12:19:41|         250|              100|Your shipment arr...|          4.5|
[2023-01-26T21:15:40.773+0000] {spark_submit.py:495} INFO - |MLA1157512489|Placa De Video Nv...|124999.0|          new|      PCREGISTRADAOK|2020-11-02 13:17:33|          50|                1|Your shipment arr...|          4.5|
[2023-01-26T21:15:40.774+0000] {spark_submit.py:495} INFO - |MLA1115295971|Procesador Amd Ry...| 93999.0|          new|      PCREGISTRADAOK|2020-11-02 13:17:33|          25|                1|Your shipment arr...|          4.5|
[2023-01-26T21:15:40.774+0000] {spark_submit.py:495} INFO - |MLA1163238194|Placa De Video Am...| 33999.0|          new|       MICYBERCOMPRA|2011-09-13 13:15:53|          50|                1|Your shipment arr...|          4.5|
[2023-01-26T21:15:40.774+0000] {spark_submit.py:495} INFO - |MLA1318244331|Disco Solido Ssd ...| 28799.0|          new|    OVERHARD DIGITAL|2015-01-08 12:19:41|           5|              150|Your shipment arr...|          4.5|
[2023-01-26T21:15:40.774+0000] {spark_submit.py:495} INFO - |MLA1148481665|Microprocesador P...| 58899.0|          new|            MALL WEB|2016-05-26 20:28:35|         150|                1|Your shipment arr...|          4.5|
[2023-01-26T21:15:40.774+0000] {spark_submit.py:495} INFO - |MLA1246852007|Memoria Ram Fury ...| 18999.0|          new|        NOXIESTORESA|2022-05-20 16:20:29|         250|               50|Your shipment arr...|          4.5|
[2023-01-26T21:15:40.774+0000] {spark_submit.py:495} INFO - | MLA923637669|Placa De Video Ge...| 28699.0|          new|         INDIGOTRADE|2018-01-05 19:21:16|         500|               50|Your shipment arr...|          4.5|
[2023-01-26T21:15:40.774+0000] {spark_submit.py:495} INFO - |MLA1133481354|Disco Sólido Inte...| 13799.0|          new|           DALECLÍCK|2015-03-16 12:07:23|         500|                1|You have to pay t...|          4.5|
[2023-01-26T21:15:40.774+0000] {spark_submit.py:495} INFO - |MLA1139583771|Disco Sólido Inte...|  8339.0|          new|DIAMONDSYSTEMPADUAML|2015-07-01 12:32:22|         250|              250|You have to pay t...|          4.5|
[2023-01-26T21:15:40.774+0000] {spark_submit.py:495} INFO - | MLA882277177|Capturadora Video...|  2699.0|          new|         BSK-IMPORTS|2019-12-30 23:07:04|         500|               50|You have to pay t...|          3.0|
[2023-01-26T21:15:40.774+0000] {spark_submit.py:495} INFO - |MLA1228990119|Memoria Ram Fury ...| 23255.0|          new|MARSTECHSRLMARSTE...|2020-08-05 14:27:59|          50|                1|Your shipment arr...|          4.5|
[2023-01-26T21:15:40.774+0000] {spark_submit.py:495} INFO - |MLA1183925519|Placa De Video Nv...|299999.0|          new|ESTUDIOSMPSRLESTU...|2020-11-29 18:36:32|          50|                1|Your shipment arr...|          4.5|
[2023-01-26T21:15:40.774+0000] {spark_submit.py:495} INFO - | MLA844489972|Disco Duro Intern...| 11599.0|          new|           DALECLÍCK|2015-03-16 12:07:23|        5000|              500|You have to pay t...|          5.0|
[2023-01-26T21:15:40.774+0000] {spark_submit.py:495} INFO - |MLA1168362675|Memoria Ram Fury ...| 26698.0|          new|MARSTECHSRLMARSTE...|2020-08-05 14:27:59|         150|                1|Your shipment arr...|          4.5|
[2023-01-26T21:15:40.775+0000] {spark_submit.py:495} INFO - |MLA1314590445|Disco Sólido Inte...|  7999.0|          new|       ELECTRO-SHOWS|2010-03-26 01:08:16|         250|              250|You have to pay t...|          4.5|
[2023-01-26T21:15:40.775+0000] {spark_submit.py:495} INFO - |MLA1163233412|Placa De Video Nv...| 34659.0|          new|      PCREGISTRADAOK|2020-11-02 13:17:33|           1|                1|Your shipment arr...|          4.5|
[2023-01-26T21:15:40.775+0000] {spark_submit.py:495} INFO - | MLA902808321|Procesador Gamer ...| 23699.0|          new|            MALL WEB|2016-05-26 20:28:35|         500|                1|Your shipment arr...|          4.5|
[2023-01-26T21:15:40.775+0000] {spark_submit.py:495} INFO - |MLA1302136118|Disco Duro Intern...| 18271.0|          new|DIAMONDSYSTEMSANJ...|2016-04-08 17:21:06|         100|               50|Your shipment arr...|          4.5|
[2023-01-26T21:15:40.775+0000] {spark_submit.py:495} INFO - |MLA1137238635|Fuente De Aliment...|  3235.0|          new|     DIAMONDSYSTEMML|2014-04-07 22:00:36|          25|                1|You have to pay t...|          3.5|
[2023-01-26T21:15:40.775+0000] {spark_submit.py:495} INFO - +-------------+--------------------+--------+-------------+--------------------+-------------------+------------+-----------------+--------------------+-------------+
[2023-01-26T21:15:40.775+0000] {spark_submit.py:495} INFO - only showing top 20 rows
[2023-01-26T21:15:40.775+0000] {spark_submit.py:495} INFO - 
[2023-01-26T21:16:09.784+0000] {spark_submit.py:495} INFO - 23/01/26 21:16:09 ERROR Executor: Exception in task 0.0 in stage 25.0 (TID 25)
[2023-01-26T21:16:09.785+0000] {spark_submit.py:495} INFO - com.mongodb.MongoTimeoutException: Timed out after 30000 ms while waiting to connect. Client view of cluster state is {type=UNKNOWN, servers=[{address=172.22.0.2:27017, type=UNKNOWN, state=CONNECTING, exception={com.mongodb.MongoSocketOpenException: Exception opening socket}, caused by {java.net.SocketTimeoutException: connect timed out}}]
[2023-01-26T21:16:09.785+0000] {spark_submit.py:495} INFO - at com.mongodb.internal.connection.BaseCluster.getDescription(BaseCluster.java:177)
[2023-01-26T21:16:09.786+0000] {spark_submit.py:495} INFO - at com.mongodb.internal.connection.SingleServerCluster.getDescription(SingleServerCluster.java:41)
[2023-01-26T21:16:09.786+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate.getConnectedClusterDescription(MongoClientDelegate.java:147)
[2023-01-26T21:16:09.787+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate.createClientSession(MongoClientDelegate.java:98)
[2023-01-26T21:16:09.787+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.getClientSession(MongoClientDelegate.java:278)
[2023-01-26T21:16:09.787+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:202)
[2023-01-26T21:16:09.788+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:441)
[2023-01-26T21:16:09.788+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:421)
[2023-01-26T21:16:09.788+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$4(MongoSpark.scala:138)
[2023-01-26T21:16:09.789+0000] {spark_submit.py:495} INFO - at scala.collection.Iterator.foreach(Iterator.scala:943)
[2023-01-26T21:16:09.789+0000] {spark_submit.py:495} INFO - at scala.collection.Iterator.foreach$(Iterator.scala:943)
[2023-01-26T21:16:09.790+0000] {spark_submit.py:495} INFO - at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
[2023-01-26T21:16:09.790+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$3(MongoSpark.scala:122)
[2023-01-26T21:16:09.791+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$3$adapted(MongoSpark.scala:121)
[2023-01-26T21:16:09.791+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.$anonfun$withCollectionDo$1(MongoConnector.scala:186)
[2023-01-26T21:16:09.792+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.$anonfun$withDatabaseDo$1(MongoConnector.scala:171)
[2023-01-26T21:16:09.792+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withMongoClientDo(MongoConnector.scala:154)
[2023-01-26T21:16:09.792+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withDatabaseDo(MongoConnector.scala:171)
[2023-01-26T21:16:09.793+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withCollectionDo(MongoConnector.scala:184)
[2023-01-26T21:16:09.794+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$2(MongoSpark.scala:121)
[2023-01-26T21:16:09.794+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$2$adapted(MongoSpark.scala:120)
[2023-01-26T21:16:09.794+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1011)
[2023-01-26T21:16:09.795+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1011)
[2023-01-26T21:16:09.795+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2268)
[2023-01-26T21:16:09.795+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
[2023-01-26T21:16:09.796+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.Task.run(Task.scala:136)
[2023-01-26T21:16:09.796+0000] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)
[2023-01-26T21:16:09.796+0000] {spark_submit.py:495} INFO - at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)
[2023-01-26T21:16:09.797+0000] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)
[2023-01-26T21:16:09.797+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[2023-01-26T21:16:09.797+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[2023-01-26T21:16:09.798+0000] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2023-01-26T21:16:09.798+0000] {spark_submit.py:495} INFO - 23/01/26 21:16:09 ERROR TaskSetManager: Task 0 in stage 25.0 failed 1 times; aborting job
[2023-01-26T21:16:09.981+0000] {spark_submit.py:495} INFO - [[34m2023-01-26 21:16:09,981[0m] {[34mconsumerSpark.py:[0m37} ERROR[0m - Error al escribir en Mongo: An error occurred while calling o266.save.
[2023-01-26T21:16:09.982+0000] {spark_submit.py:495} INFO - : org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 25.0 failed 1 times, most recent failure: Lost task 0.0 in stage 25.0 (TID 25) (f1cbb5df32e6 executor driver): com.mongodb.MongoTimeoutException: Timed out after 30000 ms while waiting to connect. Client view of cluster state is {type=UNKNOWN, servers=[{address=172.22.0.2:27017, type=UNKNOWN, state=CONNECTING, exception={com.mongodb.MongoSocketOpenException: Exception opening socket}, caused by {java.net.SocketTimeoutException: connect timed out}}]
[2023-01-26T21:16:09.982+0000] {spark_submit.py:495} INFO - at com.mongodb.internal.connection.BaseCluster.getDescription(BaseCluster.java:177)
[2023-01-26T21:16:09.983+0000] {spark_submit.py:495} INFO - at com.mongodb.internal.connection.SingleServerCluster.getDescription(SingleServerCluster.java:41)
[2023-01-26T21:16:09.983+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate.getConnectedClusterDescription(MongoClientDelegate.java:147)
[2023-01-26T21:16:09.984+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate.createClientSession(MongoClientDelegate.java:98)
[2023-01-26T21:16:09.984+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.getClientSession(MongoClientDelegate.java:278)
[2023-01-26T21:16:09.984+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:202)
[2023-01-26T21:16:09.985+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:441)
[2023-01-26T21:16:09.985+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:421)
[2023-01-26T21:16:09.986+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$4(MongoSpark.scala:138)
[2023-01-26T21:16:09.986+0000] {spark_submit.py:495} INFO - at scala.collection.Iterator.foreach(Iterator.scala:943)
[2023-01-26T21:16:09.986+0000] {spark_submit.py:495} INFO - at scala.collection.Iterator.foreach$(Iterator.scala:943)
[2023-01-26T21:16:09.987+0000] {spark_submit.py:495} INFO - at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
[2023-01-26T21:16:09.987+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$3(MongoSpark.scala:122)
[2023-01-26T21:16:09.987+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$3$adapted(MongoSpark.scala:121)
[2023-01-26T21:16:09.988+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.$anonfun$withCollectionDo$1(MongoConnector.scala:186)
[2023-01-26T21:16:09.988+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.$anonfun$withDatabaseDo$1(MongoConnector.scala:171)
[2023-01-26T21:16:09.989+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withMongoClientDo(MongoConnector.scala:154)
[2023-01-26T21:16:09.989+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withDatabaseDo(MongoConnector.scala:171)
[2023-01-26T21:16:09.989+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withCollectionDo(MongoConnector.scala:184)
[2023-01-26T21:16:09.990+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$2(MongoSpark.scala:121)
[2023-01-26T21:16:09.990+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$2$adapted(MongoSpark.scala:120)
[2023-01-26T21:16:09.990+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1011)
[2023-01-26T21:16:09.991+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1011)
[2023-01-26T21:16:09.991+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2268)
[2023-01-26T21:16:09.992+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
[2023-01-26T21:16:09.992+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.Task.run(Task.scala:136)
[2023-01-26T21:16:09.992+0000] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)
[2023-01-26T21:16:09.993+0000] {spark_submit.py:495} INFO - at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)
[2023-01-26T21:16:09.993+0000] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)
[2023-01-26T21:16:09.993+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[2023-01-26T21:16:09.994+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[2023-01-26T21:16:09.994+0000] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2023-01-26T21:16:09.994+0000] {spark_submit.py:495} INFO - 
[2023-01-26T21:16:09.995+0000] {spark_submit.py:495} INFO - Driver stacktrace:
[2023-01-26T21:16:09.995+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)
[2023-01-26T21:16:09.995+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)
[2023-01-26T21:16:09.996+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)
[2023-01-26T21:16:09.996+0000] {spark_submit.py:495} INFO - at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
[2023-01-26T21:16:09.996+0000] {spark_submit.py:495} INFO - at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
[2023-01-26T21:16:09.997+0000] {spark_submit.py:495} INFO - at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
[2023-01-26T21:16:09.997+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)
[2023-01-26T21:16:09.997+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)
[2023-01-26T21:16:09.998+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)
[2023-01-26T21:16:09.998+0000] {spark_submit.py:495} INFO - at scala.Option.foreach(Option.scala:407)
[2023-01-26T21:16:09.998+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)
[2023-01-26T21:16:09.999+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)
[2023-01-26T21:16:09.999+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)
[2023-01-26T21:16:09.999+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)
[2023-01-26T21:16:10.000+0000] {spark_submit.py:495} INFO - at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
[2023-01-26T21:16:10.000+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)
[2023-01-26T21:16:10.001+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.runJob(SparkContext.scala:2228)
[2023-01-26T21:16:10.003+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.runJob(SparkContext.scala:2249)
[2023-01-26T21:16:10.004+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.runJob(SparkContext.scala:2268)
[2023-01-26T21:16:10.004+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.runJob(SparkContext.scala:2293)
[2023-01-26T21:16:10.004+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$1(RDD.scala:1011)
[2023-01-26T21:16:10.005+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
[2023-01-26T21:16:10.005+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
[2023-01-26T21:16:10.006+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.withScope(RDD.scala:406)
[2023-01-26T21:16:10.006+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.foreachPartition(RDD.scala:1009)
[2023-01-26T21:16:10.006+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.save(MongoSpark.scala:120)
[2023-01-26T21:16:10.007+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.save(MongoSpark.scala:169)
[2023-01-26T21:16:10.007+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.sql.DefaultSource.createRelation(DefaultSource.scala:70)
[2023-01-26T21:16:10.007+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:47)
[2023-01-26T21:16:10.007+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)
[2023-01-26T21:16:10.007+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)
[2023-01-26T21:16:10.008+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)
[2023-01-26T21:16:10.008+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)
[2023-01-26T21:16:10.008+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)
[2023-01-26T21:16:10.008+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)
[2023-01-26T21:16:10.008+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)
[2023-01-26T21:16:10.009+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
[2023-01-26T21:16:10.009+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
[2023-01-26T21:16:10.009+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)
[2023-01-26T21:16:10.009+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)
[2023-01-26T21:16:10.010+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:584)
[2023-01-26T21:16:10.010+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:176)
[2023-01-26T21:16:10.010+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:584)
[2023-01-26T21:16:10.010+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)
[2023-01-26T21:16:10.010+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
[2023-01-26T21:16:10.011+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
[2023-01-26T21:16:10.011+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
[2023-01-26T21:16:10.011+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
[2023-01-26T21:16:10.011+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:560)
[2023-01-26T21:16:10.011+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)
[2023-01-26T21:16:10.011+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)
[2023-01-26T21:16:10.012+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)
[2023-01-26T21:16:10.012+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:116)
[2023-01-26T21:16:10.012+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:860)
[2023-01-26T21:16:10.012+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:390)
[2023-01-26T21:16:10.013+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:363)
[2023-01-26T21:16:10.013+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:247)
[2023-01-26T21:16:10.013+0000] {spark_submit.py:495} INFO - at jdk.internal.reflect.GeneratedMethodAccessor69.invoke(Unknown Source)
[2023-01-26T21:16:10.013+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
[2023-01-26T21:16:10.013+0000] {spark_submit.py:495} INFO - at java.base/java.lang.reflect.Method.invoke(Method.java:566)
[2023-01-26T21:16:10.013+0000] {spark_submit.py:495} INFO - at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
[2023-01-26T21:16:10.014+0000] {spark_submit.py:495} INFO - at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
[2023-01-26T21:16:10.014+0000] {spark_submit.py:495} INFO - at py4j.Gateway.invoke(Gateway.java:282)
[2023-01-26T21:16:10.014+0000] {spark_submit.py:495} INFO - at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
[2023-01-26T21:16:10.014+0000] {spark_submit.py:495} INFO - at py4j.commands.CallCommand.execute(CallCommand.java:79)
[2023-01-26T21:16:10.014+0000] {spark_submit.py:495} INFO - at py4j.ClientServerConnection.sendCommand(ClientServerConnection.java:244)
[2023-01-26T21:16:10.014+0000] {spark_submit.py:495} INFO - at py4j.CallbackClient.sendCommand(CallbackClient.java:384)
[2023-01-26T21:16:10.015+0000] {spark_submit.py:495} INFO - at py4j.CallbackClient.sendCommand(CallbackClient.java:356)
[2023-01-26T21:16:10.015+0000] {spark_submit.py:495} INFO - at py4j.reflection.PythonProxyHandler.invoke(PythonProxyHandler.java:106)
[2023-01-26T21:16:10.015+0000] {spark_submit.py:495} INFO - at com.sun.proxy.$Proxy32.call(Unknown Source)
[2023-01-26T21:16:10.015+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1(ForeachBatchSink.scala:51)
[2023-01-26T21:16:10.015+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1$adapted(ForeachBatchSink.scala:51)
[2023-01-26T21:16:10.015+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.sources.ForeachBatchSink.addBatch(ForeachBatchSink.scala:32)
[2023-01-26T21:16:10.015+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:666)
[2023-01-26T21:16:10.016+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)
[2023-01-26T21:16:10.016+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)
[2023-01-26T21:16:10.016+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)
[2023-01-26T21:16:10.016+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
[2023-01-26T21:16:10.016+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
[2023-01-26T21:16:10.016+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:664)
[2023-01-26T21:16:10.016+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:375)
[2023-01-26T21:16:10.016+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:373)
[2023-01-26T21:16:10.017+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:68)
[2023-01-26T21:16:10.017+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:664)
[2023-01-26T21:16:10.017+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:256)
[2023-01-26T21:16:10.017+0000] {spark_submit.py:495} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2023-01-26T21:16:10.017+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:375)
[2023-01-26T21:16:10.017+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:373)
[2023-01-26T21:16:10.017+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:68)
[2023-01-26T21:16:10.018+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:219)
[2023-01-26T21:16:10.018+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)
[2023-01-26T21:16:10.018+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:213)
[2023-01-26T21:16:10.018+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:307)
[2023-01-26T21:16:10.018+0000] {spark_submit.py:495} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2023-01-26T21:16:10.018+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
[2023-01-26T21:16:10.018+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:285)
[2023-01-26T21:16:10.018+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:208)
[2023-01-26T21:16:10.018+0000] {spark_submit.py:495} INFO - Caused by: com.mongodb.MongoTimeoutException: Timed out after 30000 ms while waiting to connect. Client view of cluster state is {type=UNKNOWN, servers=[{address=172.22.0.2:27017, type=UNKNOWN, state=CONNECTING, exception={com.mongodb.MongoSocketOpenException: Exception opening socket}, caused by {java.net.SocketTimeoutException: connect timed out}}]
[2023-01-26T21:16:10.019+0000] {spark_submit.py:495} INFO - at com.mongodb.internal.connection.BaseCluster.getDescription(BaseCluster.java:177)
[2023-01-26T21:16:10.019+0000] {spark_submit.py:495} INFO - at com.mongodb.internal.connection.SingleServerCluster.getDescription(SingleServerCluster.java:41)
[2023-01-26T21:16:10.019+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate.getConnectedClusterDescription(MongoClientDelegate.java:147)
[2023-01-26T21:16:10.019+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate.createClientSession(MongoClientDelegate.java:98)
[2023-01-26T21:16:10.019+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.getClientSession(MongoClientDelegate.java:278)
[2023-01-26T21:16:10.019+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:202)
[2023-01-26T21:16:10.019+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:441)
[2023-01-26T21:16:10.019+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:421)
[2023-01-26T21:16:10.019+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$4(MongoSpark.scala:138)
[2023-01-26T21:16:10.020+0000] {spark_submit.py:495} INFO - at scala.collection.Iterator.foreach(Iterator.scala:943)
[2023-01-26T21:16:10.020+0000] {spark_submit.py:495} INFO - at scala.collection.Iterator.foreach$(Iterator.scala:943)
[2023-01-26T21:16:10.020+0000] {spark_submit.py:495} INFO - at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
[2023-01-26T21:16:10.020+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$3(MongoSpark.scala:122)
[2023-01-26T21:16:10.020+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$3$adapted(MongoSpark.scala:121)
[2023-01-26T21:16:10.020+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.$anonfun$withCollectionDo$1(MongoConnector.scala:186)
[2023-01-26T21:16:10.020+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.$anonfun$withDatabaseDo$1(MongoConnector.scala:171)
[2023-01-26T21:16:10.020+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withMongoClientDo(MongoConnector.scala:154)
[2023-01-26T21:16:10.021+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withDatabaseDo(MongoConnector.scala:171)
[2023-01-26T21:16:10.021+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withCollectionDo(MongoConnector.scala:184)
[2023-01-26T21:16:10.021+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$2(MongoSpark.scala:121)
[2023-01-26T21:16:10.021+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$2$adapted(MongoSpark.scala:120)
[2023-01-26T21:16:10.021+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1011)
[2023-01-26T21:16:10.021+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1011)
[2023-01-26T21:16:10.021+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2268)
[2023-01-26T21:16:10.021+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
[2023-01-26T21:16:10.021+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.Task.run(Task.scala:136)
[2023-01-26T21:16:10.021+0000] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)
[2023-01-26T21:16:10.022+0000] {spark_submit.py:495} INFO - at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)
[2023-01-26T21:16:10.022+0000] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)
[2023-01-26T21:16:10.022+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[2023-01-26T21:16:10.022+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[2023-01-26T21:16:10.022+0000] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2023-01-26T21:16:10.022+0000] {spark_submit.py:495} INFO - [0m
[2023-01-26T21:16:10.080+0000] {spark_submit.py:495} INFO - [[34m2023-01-26 21:16:10,080[0m] {[34mclientserver.py:[0m577} INFO[0m - Received command c on object id p0[0m
[2023-01-26T21:16:40.147+0000] {spark_submit.py:495} INFO - 23/01/26 21:16:40 ERROR Executor: Exception in task 0.0 in stage 27.0 (TID 27)
[2023-01-26T21:16:40.148+0000] {spark_submit.py:495} INFO - com.mongodb.MongoTimeoutException: Timed out after 30000 ms while waiting to connect. Client view of cluster state is {type=UNKNOWN, servers=[{address=172.22.0.2:27017, type=UNKNOWN, state=CONNECTING, exception={com.mongodb.MongoSocketOpenException: Exception opening socket}, caused by {java.net.SocketTimeoutException: connect timed out}}]
[2023-01-26T21:16:40.148+0000] {spark_submit.py:495} INFO - at com.mongodb.internal.connection.BaseCluster.getDescription(BaseCluster.java:177)
[2023-01-26T21:16:40.149+0000] {spark_submit.py:495} INFO - at com.mongodb.internal.connection.SingleServerCluster.getDescription(SingleServerCluster.java:41)
[2023-01-26T21:16:40.149+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate.getConnectedClusterDescription(MongoClientDelegate.java:147)
[2023-01-26T21:16:40.149+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate.createClientSession(MongoClientDelegate.java:98)
[2023-01-26T21:16:40.150+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.getClientSession(MongoClientDelegate.java:278)
[2023-01-26T21:16:40.150+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:202)
[2023-01-26T21:16:40.151+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:441)
[2023-01-26T21:16:40.151+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:421)
[2023-01-26T21:16:40.152+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$4(MongoSpark.scala:138)
[2023-01-26T21:16:40.152+0000] {spark_submit.py:495} INFO - at scala.collection.Iterator.foreach(Iterator.scala:943)
[2023-01-26T21:16:40.152+0000] {spark_submit.py:495} INFO - at scala.collection.Iterator.foreach$(Iterator.scala:943)
[2023-01-26T21:16:40.153+0000] {spark_submit.py:495} INFO - at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
[2023-01-26T21:16:40.153+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$3(MongoSpark.scala:122)
[2023-01-26T21:16:40.154+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$3$adapted(MongoSpark.scala:121)
[2023-01-26T21:16:40.154+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.$anonfun$withCollectionDo$1(MongoConnector.scala:186)
[2023-01-26T21:16:40.154+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.$anonfun$withDatabaseDo$1(MongoConnector.scala:171)
[2023-01-26T21:16:40.155+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withMongoClientDo(MongoConnector.scala:154)
[2023-01-26T21:16:40.155+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withDatabaseDo(MongoConnector.scala:171)
[2023-01-26T21:16:40.156+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withCollectionDo(MongoConnector.scala:184)
[2023-01-26T21:16:40.156+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$2(MongoSpark.scala:121)
[2023-01-26T21:16:40.156+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$2$adapted(MongoSpark.scala:120)
[2023-01-26T21:16:40.157+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1011)
[2023-01-26T21:16:40.157+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1011)
[2023-01-26T21:16:40.157+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2268)
[2023-01-26T21:16:40.158+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
[2023-01-26T21:16:40.158+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.Task.run(Task.scala:136)
[2023-01-26T21:16:40.159+0000] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)
[2023-01-26T21:16:40.159+0000] {spark_submit.py:495} INFO - at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)
[2023-01-26T21:16:40.159+0000] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)
[2023-01-26T21:16:40.160+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[2023-01-26T21:16:40.160+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[2023-01-26T21:16:40.160+0000] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2023-01-26T21:16:40.161+0000] {spark_submit.py:495} INFO - 23/01/26 21:16:40 ERROR TaskSetManager: Task 0 in stage 27.0 failed 1 times; aborting job
[2023-01-26T21:16:40.341+0000] {spark_submit.py:495} INFO - [[34m2023-01-26 21:16:40,341[0m] {[34mconsumerSpark.py:[0m37} ERROR[0m - Error al escribir en Mongo: An error occurred while calling o275.save.
[2023-01-26T21:16:40.342+0000] {spark_submit.py:495} INFO - : org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 27.0 failed 1 times, most recent failure: Lost task 0.0 in stage 27.0 (TID 27) (f1cbb5df32e6 executor driver): com.mongodb.MongoTimeoutException: Timed out after 30000 ms while waiting to connect. Client view of cluster state is {type=UNKNOWN, servers=[{address=172.22.0.2:27017, type=UNKNOWN, state=CONNECTING, exception={com.mongodb.MongoSocketOpenException: Exception opening socket}, caused by {java.net.SocketTimeoutException: connect timed out}}]
[2023-01-26T21:16:40.342+0000] {spark_submit.py:495} INFO - at com.mongodb.internal.connection.BaseCluster.getDescription(BaseCluster.java:177)
[2023-01-26T21:16:40.343+0000] {spark_submit.py:495} INFO - at com.mongodb.internal.connection.SingleServerCluster.getDescription(SingleServerCluster.java:41)
[2023-01-26T21:16:40.343+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate.getConnectedClusterDescription(MongoClientDelegate.java:147)
[2023-01-26T21:16:40.344+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate.createClientSession(MongoClientDelegate.java:98)
[2023-01-26T21:16:40.344+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.getClientSession(MongoClientDelegate.java:278)
[2023-01-26T21:16:40.344+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:202)
[2023-01-26T21:16:40.345+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:441)
[2023-01-26T21:16:40.345+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:421)
[2023-01-26T21:16:40.346+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$4(MongoSpark.scala:138)
[2023-01-26T21:16:40.346+0000] {spark_submit.py:495} INFO - at scala.collection.Iterator.foreach(Iterator.scala:943)
[2023-01-26T21:16:40.346+0000] {spark_submit.py:495} INFO - at scala.collection.Iterator.foreach$(Iterator.scala:943)
[2023-01-26T21:16:40.347+0000] {spark_submit.py:495} INFO - at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
[2023-01-26T21:16:40.347+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$3(MongoSpark.scala:122)
[2023-01-26T21:16:40.347+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$3$adapted(MongoSpark.scala:121)
[2023-01-26T21:16:40.348+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.$anonfun$withCollectionDo$1(MongoConnector.scala:186)
[2023-01-26T21:16:40.348+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.$anonfun$withDatabaseDo$1(MongoConnector.scala:171)
[2023-01-26T21:16:40.348+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withMongoClientDo(MongoConnector.scala:154)
[2023-01-26T21:16:40.349+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withDatabaseDo(MongoConnector.scala:171)
[2023-01-26T21:16:40.349+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withCollectionDo(MongoConnector.scala:184)
[2023-01-26T21:16:40.349+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$2(MongoSpark.scala:121)
[2023-01-26T21:16:40.350+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$2$adapted(MongoSpark.scala:120)
[2023-01-26T21:16:40.350+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1011)
[2023-01-26T21:16:40.351+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1011)
[2023-01-26T21:16:40.351+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2268)
[2023-01-26T21:16:40.351+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
[2023-01-26T21:16:40.352+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.Task.run(Task.scala:136)
[2023-01-26T21:16:40.352+0000] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)
[2023-01-26T21:16:40.352+0000] {spark_submit.py:495} INFO - at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)
[2023-01-26T21:16:40.353+0000] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)
[2023-01-26T21:16:40.353+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[2023-01-26T21:16:40.353+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[2023-01-26T21:16:40.354+0000] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2023-01-26T21:16:40.354+0000] {spark_submit.py:495} INFO - 
[2023-01-26T21:16:40.354+0000] {spark_submit.py:495} INFO - Driver stacktrace:
[2023-01-26T21:16:40.355+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)
[2023-01-26T21:16:40.355+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)
[2023-01-26T21:16:40.355+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)
[2023-01-26T21:16:40.356+0000] {spark_submit.py:495} INFO - at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
[2023-01-26T21:16:40.356+0000] {spark_submit.py:495} INFO - at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
[2023-01-26T21:16:40.356+0000] {spark_submit.py:495} INFO - at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
[2023-01-26T21:16:40.357+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)
[2023-01-26T21:16:40.357+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)
[2023-01-26T21:16:40.357+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)
[2023-01-26T21:16:40.358+0000] {spark_submit.py:495} INFO - at scala.Option.foreach(Option.scala:407)
[2023-01-26T21:16:40.358+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)
[2023-01-26T21:16:40.358+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)
[2023-01-26T21:16:40.359+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)
[2023-01-26T21:16:40.359+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)
[2023-01-26T21:16:40.359+0000] {spark_submit.py:495} INFO - at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
[2023-01-26T21:16:40.359+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)
[2023-01-26T21:16:40.360+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.runJob(SparkContext.scala:2228)
[2023-01-26T21:16:40.360+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.runJob(SparkContext.scala:2249)
[2023-01-26T21:16:40.360+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.runJob(SparkContext.scala:2268)
[2023-01-26T21:16:40.360+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.runJob(SparkContext.scala:2293)
[2023-01-26T21:16:40.361+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$1(RDD.scala:1011)
[2023-01-26T21:16:40.361+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
[2023-01-26T21:16:40.361+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
[2023-01-26T21:16:40.361+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.withScope(RDD.scala:406)
[2023-01-26T21:16:40.362+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.foreachPartition(RDD.scala:1009)
[2023-01-26T21:16:40.362+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.save(MongoSpark.scala:120)
[2023-01-26T21:16:40.362+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.save(MongoSpark.scala:169)
[2023-01-26T21:16:40.362+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.sql.DefaultSource.createRelation(DefaultSource.scala:70)
[2023-01-26T21:16:40.362+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:47)
[2023-01-26T21:16:40.363+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)
[2023-01-26T21:16:40.363+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)
[2023-01-26T21:16:40.363+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)
[2023-01-26T21:16:40.363+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)
[2023-01-26T21:16:40.364+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)
[2023-01-26T21:16:40.364+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)
[2023-01-26T21:16:40.364+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)
[2023-01-26T21:16:40.364+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
[2023-01-26T21:16:40.364+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
[2023-01-26T21:16:40.365+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)
[2023-01-26T21:16:40.365+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)
[2023-01-26T21:16:40.365+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:584)
[2023-01-26T21:16:40.365+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:176)
[2023-01-26T21:16:40.365+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:584)
[2023-01-26T21:16:40.365+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)
[2023-01-26T21:16:40.366+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
[2023-01-26T21:16:40.366+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
[2023-01-26T21:16:40.366+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
[2023-01-26T21:16:40.366+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
[2023-01-26T21:16:40.366+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:560)
[2023-01-26T21:16:40.366+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)
[2023-01-26T21:16:40.367+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)
[2023-01-26T21:16:40.367+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)
[2023-01-26T21:16:40.367+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:116)
[2023-01-26T21:16:40.367+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:860)
[2023-01-26T21:16:40.368+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:390)
[2023-01-26T21:16:40.368+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:363)
[2023-01-26T21:16:40.368+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:247)
[2023-01-26T21:16:40.368+0000] {spark_submit.py:495} INFO - at jdk.internal.reflect.GeneratedMethodAccessor69.invoke(Unknown Source)
[2023-01-26T21:16:40.368+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
[2023-01-26T21:16:40.368+0000] {spark_submit.py:495} INFO - at java.base/java.lang.reflect.Method.invoke(Method.java:566)
[2023-01-26T21:16:40.369+0000] {spark_submit.py:495} INFO - at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
[2023-01-26T21:16:40.369+0000] {spark_submit.py:495} INFO - at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
[2023-01-26T21:16:40.369+0000] {spark_submit.py:495} INFO - at py4j.Gateway.invoke(Gateway.java:282)
[2023-01-26T21:16:40.369+0000] {spark_submit.py:495} INFO - at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
[2023-01-26T21:16:40.369+0000] {spark_submit.py:495} INFO - at py4j.commands.CallCommand.execute(CallCommand.java:79)
[2023-01-26T21:16:40.369+0000] {spark_submit.py:495} INFO - at py4j.ClientServerConnection.sendCommand(ClientServerConnection.java:244)
[2023-01-26T21:16:40.369+0000] {spark_submit.py:495} INFO - at py4j.CallbackClient.sendCommand(CallbackClient.java:384)
[2023-01-26T21:16:40.370+0000] {spark_submit.py:495} INFO - at py4j.CallbackClient.sendCommand(CallbackClient.java:356)
[2023-01-26T21:16:40.370+0000] {spark_submit.py:495} INFO - at py4j.reflection.PythonProxyHandler.invoke(PythonProxyHandler.java:106)
[2023-01-26T21:16:40.370+0000] {spark_submit.py:495} INFO - at com.sun.proxy.$Proxy32.call(Unknown Source)
[2023-01-26T21:16:40.370+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1(ForeachBatchSink.scala:51)
[2023-01-26T21:16:40.370+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1$adapted(ForeachBatchSink.scala:51)
[2023-01-26T21:16:40.370+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.sources.ForeachBatchSink.addBatch(ForeachBatchSink.scala:32)
[2023-01-26T21:16:40.370+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:666)
[2023-01-26T21:16:40.371+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)
[2023-01-26T21:16:40.371+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)
[2023-01-26T21:16:40.371+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)
[2023-01-26T21:16:40.371+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
[2023-01-26T21:16:40.371+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
[2023-01-26T21:16:40.371+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:664)
[2023-01-26T21:16:40.371+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:375)
[2023-01-26T21:16:40.372+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:373)
[2023-01-26T21:16:40.372+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:68)
[2023-01-26T21:16:40.372+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:664)
[2023-01-26T21:16:40.372+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:256)
[2023-01-26T21:16:40.372+0000] {spark_submit.py:495} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2023-01-26T21:16:40.372+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:375)
[2023-01-26T21:16:40.372+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:373)
[2023-01-26T21:16:40.372+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:68)
[2023-01-26T21:16:40.373+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:219)
[2023-01-26T21:16:40.373+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)
[2023-01-26T21:16:40.373+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:213)
[2023-01-26T21:16:40.373+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:307)
[2023-01-26T21:16:40.373+0000] {spark_submit.py:495} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2023-01-26T21:16:40.373+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
[2023-01-26T21:16:40.373+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:285)
[2023-01-26T21:16:40.374+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:208)
[2023-01-26T21:16:40.374+0000] {spark_submit.py:495} INFO - Caused by: com.mongodb.MongoTimeoutException: Timed out after 30000 ms while waiting to connect. Client view of cluster state is {type=UNKNOWN, servers=[{address=172.22.0.2:27017, type=UNKNOWN, state=CONNECTING, exception={com.mongodb.MongoSocketOpenException: Exception opening socket}, caused by {java.net.SocketTimeoutException: connect timed out}}]
[2023-01-26T21:16:40.374+0000] {spark_submit.py:495} INFO - at com.mongodb.internal.connection.BaseCluster.getDescription(BaseCluster.java:177)
[2023-01-26T21:16:40.374+0000] {spark_submit.py:495} INFO - at com.mongodb.internal.connection.SingleServerCluster.getDescription(SingleServerCluster.java:41)
[2023-01-26T21:16:40.374+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate.getConnectedClusterDescription(MongoClientDelegate.java:147)
[2023-01-26T21:16:40.374+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate.createClientSession(MongoClientDelegate.java:98)
[2023-01-26T21:16:40.374+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.getClientSession(MongoClientDelegate.java:278)
[2023-01-26T21:16:40.374+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:202)
[2023-01-26T21:16:40.375+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:441)
[2023-01-26T21:16:40.375+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:421)
[2023-01-26T21:16:40.375+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$4(MongoSpark.scala:138)
[2023-01-26T21:16:40.375+0000] {spark_submit.py:495} INFO - at scala.collection.Iterator.foreach(Iterator.scala:943)
[2023-01-26T21:16:40.375+0000] {spark_submit.py:495} INFO - at scala.collection.Iterator.foreach$(Iterator.scala:943)
[2023-01-26T21:16:40.375+0000] {spark_submit.py:495} INFO - at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
[2023-01-26T21:16:40.375+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$3(MongoSpark.scala:122)
[2023-01-26T21:16:40.375+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$3$adapted(MongoSpark.scala:121)
[2023-01-26T21:16:40.375+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.$anonfun$withCollectionDo$1(MongoConnector.scala:186)
[2023-01-26T21:16:40.376+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.$anonfun$withDatabaseDo$1(MongoConnector.scala:171)
[2023-01-26T21:16:40.376+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withMongoClientDo(MongoConnector.scala:154)
[2023-01-26T21:16:40.376+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withDatabaseDo(MongoConnector.scala:171)
[2023-01-26T21:16:40.376+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withCollectionDo(MongoConnector.scala:184)
[2023-01-26T21:16:40.376+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$2(MongoSpark.scala:121)
[2023-01-26T21:16:40.376+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$2$adapted(MongoSpark.scala:120)
[2023-01-26T21:16:40.376+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1011)
[2023-01-26T21:16:40.376+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1011)
[2023-01-26T21:16:40.376+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2268)
[2023-01-26T21:16:40.377+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
[2023-01-26T21:16:40.377+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.Task.run(Task.scala:136)
[2023-01-26T21:16:40.377+0000] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)
[2023-01-26T21:16:40.377+0000] {spark_submit.py:495} INFO - at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)
[2023-01-26T21:16:40.377+0000] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)
[2023-01-26T21:16:40.377+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[2023-01-26T21:16:40.377+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[2023-01-26T21:16:40.377+0000] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2023-01-26T21:16:40.377+0000] {spark_submit.py:495} INFO - [0m
[2023-01-26T21:27:08.749+0000] {spark_submit.py:495} INFO - [[34m2023-01-26 21:27:08,744[0m] {[34mclientserver.py:[0m577} INFO[0m - Received command c on object id p0[0m
[2023-01-26T21:27:11.024+0000] {spark_submit.py:495} INFO - -------------------------------------------
[2023-01-26T21:27:11.024+0000] {spark_submit.py:495} INFO - Batch: 10
[2023-01-26T21:27:11.024+0000] {spark_submit.py:495} INFO - -------------------------------------------
[2023-01-26T21:27:11.039+0000] {spark_submit.py:495} INFO - +-------------+--------------------+--------+-------------+--------------------+-------------------+------------+-----------------+--------------------+-------------+
[2023-01-26T21:27:11.039+0000] {spark_submit.py:495} INFO - |           id|       nameOfProduct|   price|ProductStatus|          SellerName|   RegistrationDate|QuantitySold|QuantityAvailable|      Free shipping?|Store ratings|
[2023-01-26T21:27:11.039+0000] {spark_submit.py:495} INFO - +-------------+--------------------+--------+-------------+--------------------+-------------------+------------+-----------------+--------------------+-------------+
[2023-01-26T21:27:11.039+0000] {spark_submit.py:495} INFO - |MLA1221192372|Disco Duro Extern...| 26299.0|          new|    OVERHARD DIGITAL|2015-01-08 12:19:41|         250|              100|Your shipment arr...|          4.5|
[2023-01-26T21:27:11.040+0000] {spark_submit.py:495} INFO - |MLA1157512489|Placa De Video Nv...|124999.0|          new|      PCREGISTRADAOK|2020-11-02 13:17:33|          50|                1|Your shipment arr...|          4.5|
[2023-01-26T21:27:11.040+0000] {spark_submit.py:495} INFO - |MLA1115295971|Procesador Amd Ry...| 93999.0|          new|      PCREGISTRADAOK|2020-11-02 13:17:33|          25|                1|Your shipment arr...|          4.5|
[2023-01-26T21:27:11.040+0000] {spark_submit.py:495} INFO - |MLA1163238194|Placa De Video Am...| 33999.0|          new|       MICYBERCOMPRA|2011-09-13 13:15:53|          50|                1|Your shipment arr...|          4.5|
[2023-01-26T21:27:11.040+0000] {spark_submit.py:495} INFO - |MLA1160204806|Memoria Ram Fury ...| 15499.0|          new|    OVERHARD DIGITAL|2015-01-08 12:19:41|         500|               50|Your shipment arr...|          4.5|
[2023-01-26T21:27:11.040+0000] {spark_submit.py:495} INFO - |MLA1148481665|Microprocesador P...| 58899.0|          new|            MALL WEB|2016-05-26 20:28:35|         150|                1|Your shipment arr...|          4.5|
[2023-01-26T21:27:11.040+0000] {spark_submit.py:495} INFO - |MLA1318244331|Disco Solido Ssd ...| 28799.0|          new|    OVERHARD DIGITAL|2015-01-08 12:19:41|           5|              150|Your shipment arr...|          4.5|
[2023-01-26T21:27:11.040+0000] {spark_submit.py:495} INFO - | MLA923637669|Placa De Video Ge...| 28699.0|          new|         INDIGOTRADE|2018-01-05 19:21:16|         500|               50|Your shipment arr...|          4.5|
[2023-01-26T21:27:11.041+0000] {spark_submit.py:495} INFO - |MLA1133481354|Disco Sólido Inte...| 13799.0|          new|           DALECLÍCK|2015-03-16 12:07:23|         500|                1|You have to pay t...|          4.5|
[2023-01-26T21:27:11.041+0000] {spark_submit.py:495} INFO - |MLA1139583771|Disco Sólido Inte...|  8339.0|          new|DIAMONDSYSTEMPADUAML|2015-07-01 12:32:22|         250|              250|You have to pay t...|          4.5|
[2023-01-26T21:27:11.041+0000] {spark_submit.py:495} INFO - | MLA882277177|Capturadora Video...|  2699.0|          new|         BSK-IMPORTS|2019-12-30 23:07:04|         500|               50|You have to pay t...|          3.0|
[2023-01-26T21:27:11.041+0000] {spark_submit.py:495} INFO - |MLA1228990119|Memoria Ram Fury ...| 23255.0|          new|MARSTECHSRLMARSTE...|2020-08-05 14:27:59|          50|                1|Your shipment arr...|          4.5|
[2023-01-26T21:27:11.041+0000] {spark_submit.py:495} INFO - |MLA1119996182|Disco Sólido Inte...| 25499.0|          new|            MALL WEB|2016-05-26 20:28:35|         500|                1|Your shipment arr...|          4.5|
[2023-01-26T21:27:11.041+0000] {spark_submit.py:495} INFO - |MLA1183925519|Placa De Video Nv...|299999.0|          new|ESTUDIOSMPSRLESTU...|2020-11-29 18:36:32|          50|                1|Your shipment arr...|          4.5|
[2023-01-26T21:27:11.041+0000] {spark_submit.py:495} INFO - | MLA844489972|Disco Duro Intern...| 11599.0|          new|           DALECLÍCK|2015-03-16 12:07:23|        5000|              500|You have to pay t...|          5.0|
[2023-01-26T21:27:11.041+0000] {spark_submit.py:495} INFO - |MLA1168362675|Memoria Ram Fury ...| 26698.0|          new|MARSTECHSRLMARSTE...|2020-08-05 14:27:59|         150|                1|Your shipment arr...|          4.5|
[2023-01-26T21:27:11.041+0000] {spark_submit.py:495} INFO - |MLA1163233412|Placa De Video Nv...| 34659.0|          new|      PCREGISTRADAOK|2020-11-02 13:17:33|           1|                1|Your shipment arr...|          4.5|
[2023-01-26T21:27:11.041+0000] {spark_submit.py:495} INFO - | MLA902808321|Procesador Gamer ...| 23699.0|          new|            MALL WEB|2016-05-26 20:28:35|         500|                1|Your shipment arr...|          4.5|
[2023-01-26T21:27:11.041+0000] {spark_submit.py:495} INFO - |MLA1314590445|Disco Sólido Inte...|  7999.0|          new|       ELECTRO-SHOWS|2010-03-26 01:08:16|         250|              250|You have to pay t...|          4.5|
[2023-01-26T21:27:11.041+0000] {spark_submit.py:495} INFO - |MLA1302136118|Disco Duro Intern...| 18271.0|          new|DIAMONDSYSTEMSANJ...|2016-04-08 17:21:06|         100|               50|Your shipment arr...|          4.5|
[2023-01-26T21:27:11.042+0000] {spark_submit.py:495} INFO - +-------------+--------------------+--------+-------------+--------------------+-------------------+------------+-----------------+--------------------+-------------+
[2023-01-26T21:27:11.042+0000] {spark_submit.py:495} INFO - only showing top 20 rows
[2023-01-26T21:27:11.042+0000] {spark_submit.py:495} INFO - 
[2023-01-26T21:27:39.617+0000] {spark_submit.py:495} INFO - 23/01/26 21:27:39 ERROR Executor: Exception in task 0.0 in stage 28.0 (TID 28)
[2023-01-26T21:27:39.618+0000] {spark_submit.py:495} INFO - com.mongodb.MongoTimeoutException: Timed out after 30000 ms while waiting to connect. Client view of cluster state is {type=UNKNOWN, servers=[{address=172.22.0.2:27017, type=UNKNOWN, state=CONNECTING, exception={com.mongodb.MongoSocketOpenException: Exception opening socket}, caused by {java.net.SocketTimeoutException: connect timed out}}]
[2023-01-26T21:27:39.619+0000] {spark_submit.py:495} INFO - at com.mongodb.internal.connection.BaseCluster.getDescription(BaseCluster.java:177)
[2023-01-26T21:27:39.619+0000] {spark_submit.py:495} INFO - at com.mongodb.internal.connection.SingleServerCluster.getDescription(SingleServerCluster.java:41)
[2023-01-26T21:27:39.619+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate.getConnectedClusterDescription(MongoClientDelegate.java:147)
[2023-01-26T21:27:39.620+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate.createClientSession(MongoClientDelegate.java:98)
[2023-01-26T21:27:39.620+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.getClientSession(MongoClientDelegate.java:278)
[2023-01-26T21:27:39.621+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:202)
[2023-01-26T21:27:39.622+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:441)
[2023-01-26T21:27:39.622+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:421)
[2023-01-26T21:27:39.623+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$4(MongoSpark.scala:138)
[2023-01-26T21:27:39.623+0000] {spark_submit.py:495} INFO - at scala.collection.Iterator.foreach(Iterator.scala:943)
[2023-01-26T21:27:39.623+0000] {spark_submit.py:495} INFO - at scala.collection.Iterator.foreach$(Iterator.scala:943)
[2023-01-26T21:27:39.624+0000] {spark_submit.py:495} INFO - at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
[2023-01-26T21:27:39.624+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$3(MongoSpark.scala:122)
[2023-01-26T21:27:39.625+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$3$adapted(MongoSpark.scala:121)
[2023-01-26T21:27:39.625+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.$anonfun$withCollectionDo$1(MongoConnector.scala:186)
[2023-01-26T21:27:39.625+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.$anonfun$withDatabaseDo$1(MongoConnector.scala:171)
[2023-01-26T21:27:39.626+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withMongoClientDo(MongoConnector.scala:154)
[2023-01-26T21:27:39.626+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withDatabaseDo(MongoConnector.scala:171)
[2023-01-26T21:27:39.626+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withCollectionDo(MongoConnector.scala:184)
[2023-01-26T21:27:39.627+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$2(MongoSpark.scala:121)
[2023-01-26T21:27:39.628+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$2$adapted(MongoSpark.scala:120)
[2023-01-26T21:27:39.628+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1011)
[2023-01-26T21:27:39.628+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1011)
[2023-01-26T21:27:39.629+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2268)
[2023-01-26T21:27:39.629+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
[2023-01-26T21:27:39.629+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.Task.run(Task.scala:136)
[2023-01-26T21:27:39.630+0000] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)
[2023-01-26T21:27:39.630+0000] {spark_submit.py:495} INFO - at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)
[2023-01-26T21:27:39.631+0000] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)
[2023-01-26T21:27:39.631+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[2023-01-26T21:27:39.631+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[2023-01-26T21:27:39.632+0000] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2023-01-26T21:27:39.632+0000] {spark_submit.py:495} INFO - 23/01/26 21:27:39 ERROR TaskSetManager: Task 0 in stage 28.0 failed 1 times; aborting job
[2023-01-26T21:27:39.817+0000] {spark_submit.py:495} INFO - [[34m2023-01-26 21:27:39,817[0m] {[34mconsumerSpark.py:[0m37} ERROR[0m - Error al escribir en Mongo: An error occurred while calling o284.save.
[2023-01-26T21:27:39.818+0000] {spark_submit.py:495} INFO - : org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 28.0 failed 1 times, most recent failure: Lost task 0.0 in stage 28.0 (TID 28) (f1cbb5df32e6 executor driver): com.mongodb.MongoTimeoutException: Timed out after 30000 ms while waiting to connect. Client view of cluster state is {type=UNKNOWN, servers=[{address=172.22.0.2:27017, type=UNKNOWN, state=CONNECTING, exception={com.mongodb.MongoSocketOpenException: Exception opening socket}, caused by {java.net.SocketTimeoutException: connect timed out}}]
[2023-01-26T21:27:39.818+0000] {spark_submit.py:495} INFO - at com.mongodb.internal.connection.BaseCluster.getDescription(BaseCluster.java:177)
[2023-01-26T21:27:39.819+0000] {spark_submit.py:495} INFO - at com.mongodb.internal.connection.SingleServerCluster.getDescription(SingleServerCluster.java:41)
[2023-01-26T21:27:39.819+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate.getConnectedClusterDescription(MongoClientDelegate.java:147)
[2023-01-26T21:27:39.819+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate.createClientSession(MongoClientDelegate.java:98)
[2023-01-26T21:27:39.820+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.getClientSession(MongoClientDelegate.java:278)
[2023-01-26T21:27:39.820+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:202)
[2023-01-26T21:27:39.821+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:441)
[2023-01-26T21:27:39.821+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:421)
[2023-01-26T21:27:39.821+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$4(MongoSpark.scala:138)
[2023-01-26T21:27:39.822+0000] {spark_submit.py:495} INFO - at scala.collection.Iterator.foreach(Iterator.scala:943)
[2023-01-26T21:27:39.822+0000] {spark_submit.py:495} INFO - at scala.collection.Iterator.foreach$(Iterator.scala:943)
[2023-01-26T21:27:39.822+0000] {spark_submit.py:495} INFO - at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
[2023-01-26T21:27:39.823+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$3(MongoSpark.scala:122)
[2023-01-26T21:27:39.823+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$3$adapted(MongoSpark.scala:121)
[2023-01-26T21:27:39.823+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.$anonfun$withCollectionDo$1(MongoConnector.scala:186)
[2023-01-26T21:27:39.824+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.$anonfun$withDatabaseDo$1(MongoConnector.scala:171)
[2023-01-26T21:27:39.824+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withMongoClientDo(MongoConnector.scala:154)
[2023-01-26T21:27:39.825+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withDatabaseDo(MongoConnector.scala:171)
[2023-01-26T21:27:39.825+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withCollectionDo(MongoConnector.scala:184)
[2023-01-26T21:27:39.825+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$2(MongoSpark.scala:121)
[2023-01-26T21:27:39.826+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$2$adapted(MongoSpark.scala:120)
[2023-01-26T21:27:39.826+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1011)
[2023-01-26T21:27:39.826+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1011)
[2023-01-26T21:27:39.827+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2268)
[2023-01-26T21:27:39.827+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
[2023-01-26T21:27:39.827+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.Task.run(Task.scala:136)
[2023-01-26T21:27:39.828+0000] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)
[2023-01-26T21:27:39.828+0000] {spark_submit.py:495} INFO - at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)
[2023-01-26T21:27:39.828+0000] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)
[2023-01-26T21:27:39.829+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[2023-01-26T21:27:39.829+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[2023-01-26T21:27:39.829+0000] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2023-01-26T21:27:39.830+0000] {spark_submit.py:495} INFO - 
[2023-01-26T21:27:39.830+0000] {spark_submit.py:495} INFO - Driver stacktrace:
[2023-01-26T21:27:39.830+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)
[2023-01-26T21:27:39.831+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)
[2023-01-26T21:27:39.831+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)
[2023-01-26T21:27:39.831+0000] {spark_submit.py:495} INFO - at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
[2023-01-26T21:27:39.832+0000] {spark_submit.py:495} INFO - at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
[2023-01-26T21:27:39.832+0000] {spark_submit.py:495} INFO - at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
[2023-01-26T21:27:39.832+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)
[2023-01-26T21:27:39.833+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)
[2023-01-26T21:27:39.833+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)
[2023-01-26T21:27:39.833+0000] {spark_submit.py:495} INFO - at scala.Option.foreach(Option.scala:407)
[2023-01-26T21:27:39.834+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)
[2023-01-26T21:27:39.834+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)
[2023-01-26T21:27:39.834+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)
[2023-01-26T21:27:39.835+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)
[2023-01-26T21:27:39.835+0000] {spark_submit.py:495} INFO - at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
[2023-01-26T21:27:39.835+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)
[2023-01-26T21:27:39.836+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.runJob(SparkContext.scala:2228)
[2023-01-26T21:27:39.836+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.runJob(SparkContext.scala:2249)
[2023-01-26T21:27:39.836+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.runJob(SparkContext.scala:2268)
[2023-01-26T21:27:39.837+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.runJob(SparkContext.scala:2293)
[2023-01-26T21:27:39.837+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$1(RDD.scala:1011)
[2023-01-26T21:27:39.837+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
[2023-01-26T21:27:39.837+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
[2023-01-26T21:27:39.838+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.withScope(RDD.scala:406)
[2023-01-26T21:27:39.838+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.foreachPartition(RDD.scala:1009)
[2023-01-26T21:27:39.838+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.save(MongoSpark.scala:120)
[2023-01-26T21:27:39.839+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.save(MongoSpark.scala:169)
[2023-01-26T21:27:39.839+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.sql.DefaultSource.createRelation(DefaultSource.scala:70)
[2023-01-26T21:27:39.839+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:47)
[2023-01-26T21:27:39.840+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)
[2023-01-26T21:27:39.840+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)
[2023-01-26T21:27:39.840+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)
[2023-01-26T21:27:39.841+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)
[2023-01-26T21:27:39.841+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)
[2023-01-26T21:27:39.841+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)
[2023-01-26T21:27:39.842+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)
[2023-01-26T21:27:39.842+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
[2023-01-26T21:27:39.842+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
[2023-01-26T21:27:39.843+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)
[2023-01-26T21:27:39.843+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)
[2023-01-26T21:27:39.843+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:584)
[2023-01-26T21:27:39.844+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:176)
[2023-01-26T21:27:39.844+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:584)
[2023-01-26T21:27:39.844+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)
[2023-01-26T21:27:39.845+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
[2023-01-26T21:27:39.845+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
[2023-01-26T21:27:39.845+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
[2023-01-26T21:27:39.846+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
[2023-01-26T21:27:39.846+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:560)
[2023-01-26T21:27:39.846+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)
[2023-01-26T21:27:39.846+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)
[2023-01-26T21:27:39.847+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)
[2023-01-26T21:27:39.847+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:116)
[2023-01-26T21:27:39.847+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:860)
[2023-01-26T21:27:39.847+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:390)
[2023-01-26T21:27:39.848+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:363)
[2023-01-26T21:27:39.848+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:247)
[2023-01-26T21:27:39.848+0000] {spark_submit.py:495} INFO - at jdk.internal.reflect.GeneratedMethodAccessor69.invoke(Unknown Source)
[2023-01-26T21:27:39.848+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
[2023-01-26T21:27:39.848+0000] {spark_submit.py:495} INFO - at java.base/java.lang.reflect.Method.invoke(Method.java:566)
[2023-01-26T21:27:39.849+0000] {spark_submit.py:495} INFO - at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
[2023-01-26T21:27:39.849+0000] {spark_submit.py:495} INFO - at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
[2023-01-26T21:27:39.849+0000] {spark_submit.py:495} INFO - at py4j.Gateway.invoke(Gateway.java:282)
[2023-01-26T21:27:39.849+0000] {spark_submit.py:495} INFO - at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
[2023-01-26T21:27:39.850+0000] {spark_submit.py:495} INFO - at py4j.commands.CallCommand.execute(CallCommand.java:79)
[2023-01-26T21:27:39.850+0000] {spark_submit.py:495} INFO - at py4j.ClientServerConnection.sendCommand(ClientServerConnection.java:244)
[2023-01-26T21:27:39.850+0000] {spark_submit.py:495} INFO - at py4j.CallbackClient.sendCommand(CallbackClient.java:384)
[2023-01-26T21:27:39.850+0000] {spark_submit.py:495} INFO - at py4j.CallbackClient.sendCommand(CallbackClient.java:356)
[2023-01-26T21:27:39.851+0000] {spark_submit.py:495} INFO - at py4j.reflection.PythonProxyHandler.invoke(PythonProxyHandler.java:106)
[2023-01-26T21:27:39.851+0000] {spark_submit.py:495} INFO - at com.sun.proxy.$Proxy32.call(Unknown Source)
[2023-01-26T21:27:39.851+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1(ForeachBatchSink.scala:51)
[2023-01-26T21:27:39.851+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1$adapted(ForeachBatchSink.scala:51)
[2023-01-26T21:27:39.851+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.sources.ForeachBatchSink.addBatch(ForeachBatchSink.scala:32)
[2023-01-26T21:27:39.851+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:666)
[2023-01-26T21:27:39.852+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)
[2023-01-26T21:27:39.852+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)
[2023-01-26T21:27:39.852+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)
[2023-01-26T21:27:39.852+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
[2023-01-26T21:27:39.852+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
[2023-01-26T21:27:39.852+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:664)
[2023-01-26T21:27:39.853+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:375)
[2023-01-26T21:27:39.853+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:373)
[2023-01-26T21:27:39.853+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:68)
[2023-01-26T21:27:39.853+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:664)
[2023-01-26T21:27:39.853+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:256)
[2023-01-26T21:27:39.853+0000] {spark_submit.py:495} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2023-01-26T21:27:39.854+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:375)
[2023-01-26T21:27:39.854+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:373)
[2023-01-26T21:27:39.854+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:68)
[2023-01-26T21:27:39.854+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:219)
[2023-01-26T21:27:39.854+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)
[2023-01-26T21:27:39.854+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:213)
[2023-01-26T21:27:39.855+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:307)
[2023-01-26T21:27:39.855+0000] {spark_submit.py:495} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2023-01-26T21:27:39.855+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
[2023-01-26T21:27:39.855+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:285)
[2023-01-26T21:27:39.855+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:208)
[2023-01-26T21:27:39.855+0000] {spark_submit.py:495} INFO - Caused by: com.mongodb.MongoTimeoutException: Timed out after 30000 ms while waiting to connect. Client view of cluster state is {type=UNKNOWN, servers=[{address=172.22.0.2:27017, type=UNKNOWN, state=CONNECTING, exception={com.mongodb.MongoSocketOpenException: Exception opening socket}, caused by {java.net.SocketTimeoutException: connect timed out}}]
[2023-01-26T21:27:39.856+0000] {spark_submit.py:495} INFO - at com.mongodb.internal.connection.BaseCluster.getDescription(BaseCluster.java:177)
[2023-01-26T21:27:39.856+0000] {spark_submit.py:495} INFO - at com.mongodb.internal.connection.SingleServerCluster.getDescription(SingleServerCluster.java:41)
[2023-01-26T21:27:39.856+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate.getConnectedClusterDescription(MongoClientDelegate.java:147)
[2023-01-26T21:27:39.856+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate.createClientSession(MongoClientDelegate.java:98)
[2023-01-26T21:27:39.856+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.getClientSession(MongoClientDelegate.java:278)
[2023-01-26T21:27:39.856+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:202)
[2023-01-26T21:27:39.856+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:441)
[2023-01-26T21:27:39.857+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:421)
[2023-01-26T21:27:39.857+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$4(MongoSpark.scala:138)
[2023-01-26T21:27:39.857+0000] {spark_submit.py:495} INFO - at scala.collection.Iterator.foreach(Iterator.scala:943)
[2023-01-26T21:27:39.857+0000] {spark_submit.py:495} INFO - at scala.collection.Iterator.foreach$(Iterator.scala:943)
[2023-01-26T21:27:39.857+0000] {spark_submit.py:495} INFO - at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
[2023-01-26T21:27:39.857+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$3(MongoSpark.scala:122)
[2023-01-26T21:27:39.857+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$3$adapted(MongoSpark.scala:121)
[2023-01-26T21:27:39.858+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.$anonfun$withCollectionDo$1(MongoConnector.scala:186)
[2023-01-26T21:27:39.858+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.$anonfun$withDatabaseDo$1(MongoConnector.scala:171)
[2023-01-26T21:27:39.858+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withMongoClientDo(MongoConnector.scala:154)
[2023-01-26T21:27:39.858+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withDatabaseDo(MongoConnector.scala:171)
[2023-01-26T21:27:39.858+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withCollectionDo(MongoConnector.scala:184)
[2023-01-26T21:27:39.858+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$2(MongoSpark.scala:121)
[2023-01-26T21:27:39.858+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$2$adapted(MongoSpark.scala:120)
[2023-01-26T21:27:39.859+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1011)
[2023-01-26T21:27:39.859+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1011)
[2023-01-26T21:27:39.859+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2268)
[2023-01-26T21:27:39.859+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
[2023-01-26T21:27:39.859+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.Task.run(Task.scala:136)
[2023-01-26T21:27:39.859+0000] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)
[2023-01-26T21:27:39.859+0000] {spark_submit.py:495} INFO - at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)
[2023-01-26T21:27:39.859+0000] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)
[2023-01-26T21:27:39.860+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[2023-01-26T21:27:39.860+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[2023-01-26T21:27:39.860+0000] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2023-01-26T21:27:39.860+0000] {spark_submit.py:495} INFO - [0m
[2023-01-26T21:27:39.921+0000] {spark_submit.py:495} INFO - [[34m2023-01-26 21:27:39,921[0m] {[34mclientserver.py:[0m577} INFO[0m - Received command c on object id p0[0m
[2023-01-26T21:28:09.967+0000] {spark_submit.py:495} INFO - 23/01/26 21:28:09 ERROR Executor: Exception in task 0.0 in stage 30.0 (TID 30)
[2023-01-26T21:28:09.968+0000] {spark_submit.py:495} INFO - com.mongodb.MongoTimeoutException: Timed out after 30000 ms while waiting to connect. Client view of cluster state is {type=UNKNOWN, servers=[{address=172.22.0.2:27017, type=UNKNOWN, state=CONNECTING, exception={com.mongodb.MongoSocketOpenException: Exception opening socket}, caused by {java.net.SocketTimeoutException: connect timed out}}]
[2023-01-26T21:28:09.968+0000] {spark_submit.py:495} INFO - at com.mongodb.internal.connection.BaseCluster.getDescription(BaseCluster.java:177)
[2023-01-26T21:28:09.968+0000] {spark_submit.py:495} INFO - at com.mongodb.internal.connection.SingleServerCluster.getDescription(SingleServerCluster.java:41)
[2023-01-26T21:28:09.968+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate.getConnectedClusterDescription(MongoClientDelegate.java:147)
[2023-01-26T21:28:09.968+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate.createClientSession(MongoClientDelegate.java:98)
[2023-01-26T21:28:09.968+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.getClientSession(MongoClientDelegate.java:278)
[2023-01-26T21:28:09.968+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:202)
[2023-01-26T21:28:09.968+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:441)
[2023-01-26T21:28:09.968+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:421)
[2023-01-26T21:28:09.968+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$4(MongoSpark.scala:138)
[2023-01-26T21:28:09.969+0000] {spark_submit.py:495} INFO - at scala.collection.Iterator.foreach(Iterator.scala:943)
[2023-01-26T21:28:09.969+0000] {spark_submit.py:495} INFO - at scala.collection.Iterator.foreach$(Iterator.scala:943)
[2023-01-26T21:28:09.969+0000] {spark_submit.py:495} INFO - at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
[2023-01-26T21:28:09.969+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$3(MongoSpark.scala:122)
[2023-01-26T21:28:09.969+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$3$adapted(MongoSpark.scala:121)
[2023-01-26T21:28:09.969+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.$anonfun$withCollectionDo$1(MongoConnector.scala:186)
[2023-01-26T21:28:09.969+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.$anonfun$withDatabaseDo$1(MongoConnector.scala:171)
[2023-01-26T21:28:09.969+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withMongoClientDo(MongoConnector.scala:154)
[2023-01-26T21:28:09.969+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withDatabaseDo(MongoConnector.scala:171)
[2023-01-26T21:28:09.969+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withCollectionDo(MongoConnector.scala:184)
[2023-01-26T21:28:09.969+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$2(MongoSpark.scala:121)
[2023-01-26T21:28:09.969+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$2$adapted(MongoSpark.scala:120)
[2023-01-26T21:28:09.969+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1011)
[2023-01-26T21:28:09.970+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1011)
[2023-01-26T21:28:09.970+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2268)
[2023-01-26T21:28:09.970+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
[2023-01-26T21:28:09.970+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.Task.run(Task.scala:136)
[2023-01-26T21:28:09.970+0000] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)
[2023-01-26T21:28:09.970+0000] {spark_submit.py:495} INFO - at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)
[2023-01-26T21:28:09.970+0000] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)
[2023-01-26T21:28:09.970+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[2023-01-26T21:28:09.970+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[2023-01-26T21:28:09.970+0000] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2023-01-26T21:28:09.975+0000] {spark_submit.py:495} INFO - 23/01/26 21:28:09 ERROR TaskSetManager: Task 0 in stage 30.0 failed 1 times; aborting job
[2023-01-26T21:28:10.088+0000] {spark_submit.py:495} INFO - [[34m2023-01-26 21:28:10,088[0m] {[34mconsumerSpark.py:[0m37} ERROR[0m - Error al escribir en Mongo: An error occurred while calling o293.save.
[2023-01-26T21:28:10.089+0000] {spark_submit.py:495} INFO - : org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 30.0 failed 1 times, most recent failure: Lost task 0.0 in stage 30.0 (TID 30) (f1cbb5df32e6 executor driver): com.mongodb.MongoTimeoutException: Timed out after 30000 ms while waiting to connect. Client view of cluster state is {type=UNKNOWN, servers=[{address=172.22.0.2:27017, type=UNKNOWN, state=CONNECTING, exception={com.mongodb.MongoSocketOpenException: Exception opening socket}, caused by {java.net.SocketTimeoutException: connect timed out}}]
[2023-01-26T21:28:10.089+0000] {spark_submit.py:495} INFO - at com.mongodb.internal.connection.BaseCluster.getDescription(BaseCluster.java:177)
[2023-01-26T21:28:10.089+0000] {spark_submit.py:495} INFO - at com.mongodb.internal.connection.SingleServerCluster.getDescription(SingleServerCluster.java:41)
[2023-01-26T21:28:10.089+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate.getConnectedClusterDescription(MongoClientDelegate.java:147)
[2023-01-26T21:28:10.089+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate.createClientSession(MongoClientDelegate.java:98)
[2023-01-26T21:28:10.089+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.getClientSession(MongoClientDelegate.java:278)
[2023-01-26T21:28:10.089+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:202)
[2023-01-26T21:28:10.089+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:441)
[2023-01-26T21:28:10.089+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:421)
[2023-01-26T21:28:10.090+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$4(MongoSpark.scala:138)
[2023-01-26T21:28:10.090+0000] {spark_submit.py:495} INFO - at scala.collection.Iterator.foreach(Iterator.scala:943)
[2023-01-26T21:28:10.090+0000] {spark_submit.py:495} INFO - at scala.collection.Iterator.foreach$(Iterator.scala:943)
[2023-01-26T21:28:10.090+0000] {spark_submit.py:495} INFO - at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
[2023-01-26T21:28:10.090+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$3(MongoSpark.scala:122)
[2023-01-26T21:28:10.090+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$3$adapted(MongoSpark.scala:121)
[2023-01-26T21:28:10.090+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.$anonfun$withCollectionDo$1(MongoConnector.scala:186)
[2023-01-26T21:28:10.090+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.$anonfun$withDatabaseDo$1(MongoConnector.scala:171)
[2023-01-26T21:28:10.090+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withMongoClientDo(MongoConnector.scala:154)
[2023-01-26T21:28:10.090+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withDatabaseDo(MongoConnector.scala:171)
[2023-01-26T21:28:10.090+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withCollectionDo(MongoConnector.scala:184)
[2023-01-26T21:28:10.090+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$2(MongoSpark.scala:121)
[2023-01-26T21:28:10.091+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$2$adapted(MongoSpark.scala:120)
[2023-01-26T21:28:10.091+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1011)
[2023-01-26T21:28:10.091+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1011)
[2023-01-26T21:28:10.091+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2268)
[2023-01-26T21:28:10.091+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
[2023-01-26T21:28:10.091+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.Task.run(Task.scala:136)
[2023-01-26T21:28:10.091+0000] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)
[2023-01-26T21:28:10.091+0000] {spark_submit.py:495} INFO - at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)
[2023-01-26T21:28:10.091+0000] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)
[2023-01-26T21:28:10.091+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[2023-01-26T21:28:10.091+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[2023-01-26T21:28:10.091+0000] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2023-01-26T21:28:10.091+0000] {spark_submit.py:495} INFO - 
[2023-01-26T21:28:10.092+0000] {spark_submit.py:495} INFO - Driver stacktrace:
[2023-01-26T21:28:10.092+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)
[2023-01-26T21:28:10.092+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)
[2023-01-26T21:28:10.092+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)
[2023-01-26T21:28:10.092+0000] {spark_submit.py:495} INFO - at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
[2023-01-26T21:28:10.092+0000] {spark_submit.py:495} INFO - at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
[2023-01-26T21:28:10.092+0000] {spark_submit.py:495} INFO - at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
[2023-01-26T21:28:10.092+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)
[2023-01-26T21:28:10.092+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)
[2023-01-26T21:28:10.092+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)
[2023-01-26T21:28:10.092+0000] {spark_submit.py:495} INFO - at scala.Option.foreach(Option.scala:407)
[2023-01-26T21:28:10.092+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)
[2023-01-26T21:28:10.093+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)
[2023-01-26T21:28:10.093+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)
[2023-01-26T21:28:10.093+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)
[2023-01-26T21:28:10.093+0000] {spark_submit.py:495} INFO - at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
[2023-01-26T21:28:10.093+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)
[2023-01-26T21:28:10.093+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.runJob(SparkContext.scala:2228)
[2023-01-26T21:28:10.093+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.runJob(SparkContext.scala:2249)
[2023-01-26T21:28:10.093+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.runJob(SparkContext.scala:2268)
[2023-01-26T21:28:10.093+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.runJob(SparkContext.scala:2293)
[2023-01-26T21:28:10.093+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$1(RDD.scala:1011)
[2023-01-26T21:28:10.093+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
[2023-01-26T21:28:10.093+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
[2023-01-26T21:28:10.093+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.withScope(RDD.scala:406)
[2023-01-26T21:28:10.094+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.foreachPartition(RDD.scala:1009)
[2023-01-26T21:28:10.094+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.save(MongoSpark.scala:120)
[2023-01-26T21:28:10.094+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.save(MongoSpark.scala:169)
[2023-01-26T21:28:10.094+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.sql.DefaultSource.createRelation(DefaultSource.scala:70)
[2023-01-26T21:28:10.094+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:47)
[2023-01-26T21:28:10.094+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)
[2023-01-26T21:28:10.094+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)
[2023-01-26T21:28:10.094+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)
[2023-01-26T21:28:10.094+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)
[2023-01-26T21:28:10.094+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)
[2023-01-26T21:28:10.094+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)
[2023-01-26T21:28:10.095+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)
[2023-01-26T21:28:10.095+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
[2023-01-26T21:28:10.095+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
[2023-01-26T21:28:10.095+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)
[2023-01-26T21:28:10.095+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)
[2023-01-26T21:28:10.095+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:584)
[2023-01-26T21:28:10.095+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:176)
[2023-01-26T21:28:10.095+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:584)
[2023-01-26T21:28:10.095+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)
[2023-01-26T21:28:10.095+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
[2023-01-26T21:28:10.096+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
[2023-01-26T21:28:10.096+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
[2023-01-26T21:28:10.096+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
[2023-01-26T21:28:10.096+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:560)
[2023-01-26T21:28:10.096+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)
[2023-01-26T21:28:10.096+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)
[2023-01-26T21:28:10.096+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)
[2023-01-26T21:28:10.096+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:116)
[2023-01-26T21:28:10.096+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:860)
[2023-01-26T21:28:10.096+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:390)
[2023-01-26T21:28:10.096+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:363)
[2023-01-26T21:28:10.096+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:247)
[2023-01-26T21:28:10.096+0000] {spark_submit.py:495} INFO - at jdk.internal.reflect.GeneratedMethodAccessor69.invoke(Unknown Source)
[2023-01-26T21:28:10.097+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
[2023-01-26T21:28:10.097+0000] {spark_submit.py:495} INFO - at java.base/java.lang.reflect.Method.invoke(Method.java:566)
[2023-01-26T21:28:10.097+0000] {spark_submit.py:495} INFO - at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
[2023-01-26T21:28:10.097+0000] {spark_submit.py:495} INFO - at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
[2023-01-26T21:28:10.097+0000] {spark_submit.py:495} INFO - at py4j.Gateway.invoke(Gateway.java:282)
[2023-01-26T21:28:10.097+0000] {spark_submit.py:495} INFO - at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
[2023-01-26T21:28:10.097+0000] {spark_submit.py:495} INFO - at py4j.commands.CallCommand.execute(CallCommand.java:79)
[2023-01-26T21:28:10.097+0000] {spark_submit.py:495} INFO - at py4j.ClientServerConnection.sendCommand(ClientServerConnection.java:244)
[2023-01-26T21:28:10.097+0000] {spark_submit.py:495} INFO - at py4j.CallbackClient.sendCommand(CallbackClient.java:384)
[2023-01-26T21:28:10.097+0000] {spark_submit.py:495} INFO - at py4j.CallbackClient.sendCommand(CallbackClient.java:356)
[2023-01-26T21:28:10.097+0000] {spark_submit.py:495} INFO - at py4j.reflection.PythonProxyHandler.invoke(PythonProxyHandler.java:106)
[2023-01-26T21:28:10.097+0000] {spark_submit.py:495} INFO - at com.sun.proxy.$Proxy32.call(Unknown Source)
[2023-01-26T21:28:10.098+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1(ForeachBatchSink.scala:51)
[2023-01-26T21:28:10.098+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1$adapted(ForeachBatchSink.scala:51)
[2023-01-26T21:28:10.098+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.sources.ForeachBatchSink.addBatch(ForeachBatchSink.scala:32)
[2023-01-26T21:28:10.098+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:666)
[2023-01-26T21:28:10.098+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)
[2023-01-26T21:28:10.098+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)
[2023-01-26T21:28:10.098+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)
[2023-01-26T21:28:10.098+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
[2023-01-26T21:28:10.098+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
[2023-01-26T21:28:10.098+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:664)
[2023-01-26T21:28:10.098+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:375)
[2023-01-26T21:28:10.098+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:373)
[2023-01-26T21:28:10.098+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:68)
[2023-01-26T21:28:10.099+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:664)
[2023-01-26T21:28:10.099+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:256)
[2023-01-26T21:28:10.099+0000] {spark_submit.py:495} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2023-01-26T21:28:10.099+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:375)
[2023-01-26T21:28:10.099+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:373)
[2023-01-26T21:28:10.099+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:68)
[2023-01-26T21:28:10.099+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:219)
[2023-01-26T21:28:10.099+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)
[2023-01-26T21:28:10.099+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:213)
[2023-01-26T21:28:10.099+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:307)
[2023-01-26T21:28:10.099+0000] {spark_submit.py:495} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2023-01-26T21:28:10.099+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
[2023-01-26T21:28:10.100+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:285)
[2023-01-26T21:28:10.100+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:208)
[2023-01-26T21:28:10.100+0000] {spark_submit.py:495} INFO - Caused by: com.mongodb.MongoTimeoutException: Timed out after 30000 ms while waiting to connect. Client view of cluster state is {type=UNKNOWN, servers=[{address=172.22.0.2:27017, type=UNKNOWN, state=CONNECTING, exception={com.mongodb.MongoSocketOpenException: Exception opening socket}, caused by {java.net.SocketTimeoutException: connect timed out}}]
[2023-01-26T21:28:10.100+0000] {spark_submit.py:495} INFO - at com.mongodb.internal.connection.BaseCluster.getDescription(BaseCluster.java:177)
[2023-01-26T21:28:10.100+0000] {spark_submit.py:495} INFO - at com.mongodb.internal.connection.SingleServerCluster.getDescription(SingleServerCluster.java:41)
[2023-01-26T21:28:10.100+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate.getConnectedClusterDescription(MongoClientDelegate.java:147)
[2023-01-26T21:28:10.100+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate.createClientSession(MongoClientDelegate.java:98)
[2023-01-26T21:28:10.100+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.getClientSession(MongoClientDelegate.java:278)
[2023-01-26T21:28:10.100+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:202)
[2023-01-26T21:28:10.100+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:441)
[2023-01-26T21:28:10.100+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:421)
[2023-01-26T21:28:10.100+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$4(MongoSpark.scala:138)
[2023-01-26T21:28:10.100+0000] {spark_submit.py:495} INFO - at scala.collection.Iterator.foreach(Iterator.scala:943)
[2023-01-26T21:28:10.101+0000] {spark_submit.py:495} INFO - at scala.collection.Iterator.foreach$(Iterator.scala:943)
[2023-01-26T21:28:10.101+0000] {spark_submit.py:495} INFO - at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
[2023-01-26T21:28:10.101+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$3(MongoSpark.scala:122)
[2023-01-26T21:28:10.101+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$3$adapted(MongoSpark.scala:121)
[2023-01-26T21:28:10.101+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.$anonfun$withCollectionDo$1(MongoConnector.scala:186)
[2023-01-26T21:28:10.101+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.$anonfun$withDatabaseDo$1(MongoConnector.scala:171)
[2023-01-26T21:28:10.101+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withMongoClientDo(MongoConnector.scala:154)
[2023-01-26T21:28:10.101+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withDatabaseDo(MongoConnector.scala:171)
[2023-01-26T21:28:10.101+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withCollectionDo(MongoConnector.scala:184)
[2023-01-26T21:28:10.101+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$2(MongoSpark.scala:121)
[2023-01-26T21:28:10.101+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$2$adapted(MongoSpark.scala:120)
[2023-01-26T21:28:10.102+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1011)
[2023-01-26T21:28:10.102+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1011)
[2023-01-26T21:28:10.102+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2268)
[2023-01-26T21:28:10.102+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
[2023-01-26T21:28:10.102+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.Task.run(Task.scala:136)
[2023-01-26T21:28:10.102+0000] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)
[2023-01-26T21:28:10.102+0000] {spark_submit.py:495} INFO - at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)
[2023-01-26T21:28:10.102+0000] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)
[2023-01-26T21:28:10.102+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[2023-01-26T21:28:10.102+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[2023-01-26T21:28:10.102+0000] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2023-01-26T21:28:10.102+0000] {spark_submit.py:495} INFO - [0m
[2023-01-26T21:56:06.049+0000] {spark_submit.py:495} INFO - [[34m2023-01-26 21:56:06,049[0m] {[34mclientserver.py:[0m577} INFO[0m - Received command c on object id p0[0m
[2023-01-26T21:56:10.676+0000] {spark_submit.py:495} INFO - -------------------------------------------
[2023-01-26T21:56:10.676+0000] {spark_submit.py:495} INFO - Batch: 11
[2023-01-26T21:56:10.676+0000] {spark_submit.py:495} INFO - -------------------------------------------
[2023-01-26T21:56:10.700+0000] {spark_submit.py:495} INFO - +-------------+--------------------+--------+-------------+--------------------+-------------------+------------+-----------------+--------------------+-------------+
[2023-01-26T21:56:10.700+0000] {spark_submit.py:495} INFO - |           id|       nameOfProduct|   price|ProductStatus|          SellerName|   RegistrationDate|QuantitySold|QuantityAvailable|      Free shipping?|Store ratings|
[2023-01-26T21:56:10.700+0000] {spark_submit.py:495} INFO - +-------------+--------------------+--------+-------------+--------------------+-------------------+------------+-----------------+--------------------+-------------+
[2023-01-26T21:56:10.700+0000] {spark_submit.py:495} INFO - |MLA1220964588|Procesador Gamer ...| 45899.0|          new|    OVERHARD DIGITAL|2015-01-08 12:19:41|          50|                1|Your shipment arr...|          4.5|
[2023-01-26T21:56:10.701+0000] {spark_submit.py:495} INFO - |MLA1157512489|Placa De Video Nv...|124999.0|          new|      PCREGISTRADAOK|2020-11-02 13:17:33|          50|                1|Your shipment arr...|          4.5|
[2023-01-26T21:56:10.701+0000] {spark_submit.py:495} INFO - |MLA1221192372|Disco Duro Extern...| 26299.0|          new|    OVERHARD DIGITAL|2015-01-08 12:19:41|         250|              100|Your shipment arr...|          4.5|
[2023-01-26T21:56:10.701+0000] {spark_submit.py:495} INFO - |MLA1163238194|Placa De Video Am...| 33999.0|          new|       MICYBERCOMPRA|2011-09-13 13:15:53|          50|                1|Your shipment arr...|          4.5|
[2023-01-26T21:56:10.701+0000] {spark_submit.py:495} INFO - |MLA1160204806|Memoria Ram Fury ...| 15499.0|          new|    OVERHARD DIGITAL|2015-01-08 12:19:41|         500|               50|Your shipment arr...|          4.5|
[2023-01-26T21:56:10.701+0000] {spark_submit.py:495} INFO - |MLA1115295971|Procesador Amd Ry...| 93999.0|          new|      PCREGISTRADAOK|2020-11-02 13:17:33|          25|                1|Your shipment arr...|          4.5|
[2023-01-26T21:56:10.701+0000] {spark_submit.py:495} INFO - |MLA1318244331|Disco Solido Ssd ...| 28799.0|          new|    OVERHARD DIGITAL|2015-01-08 12:19:41|           5|              150|Your shipment arr...|          4.5|
[2023-01-26T21:56:10.701+0000] {spark_submit.py:495} INFO - | MLA923637669|Placa De Video Ge...| 28699.0|          new|         INDIGOTRADE|2018-01-05 19:21:16|         500|               50|Your shipment arr...|          4.5|
[2023-01-26T21:56:10.701+0000] {spark_submit.py:495} INFO - |MLA1133481354|Disco Sólido Inte...| 13799.0|          new|           DALECLÍCK|2015-03-16 12:07:23|         500|                1|You have to pay t...|          4.5|
[2023-01-26T21:56:10.701+0000] {spark_submit.py:495} INFO - |MLA1139583771|Disco Sólido Inte...|  8339.0|          new|DIAMONDSYSTEMPADUAML|2015-07-01 12:32:22|         250|              250|You have to pay t...|          4.5|
[2023-01-26T21:56:10.701+0000] {spark_submit.py:495} INFO - | MLA882277177|Capturadora Video...|  2699.0|          new|         BSK-IMPORTS|2019-12-30 23:07:04|         500|               50|You have to pay t...|          3.0|
[2023-01-26T21:56:10.701+0000] {spark_submit.py:495} INFO - |MLA1148481665|Microprocesador P...| 58899.0|          new|            MALL WEB|2016-05-26 20:28:35|         150|                1|Your shipment arr...|          4.5|
[2023-01-26T21:56:10.702+0000] {spark_submit.py:495} INFO - |MLA1228990119|Memoria Ram Fury ...| 23255.0|          new|MARSTECHSRLMARSTE...|2020-08-05 14:27:59|          50|                1|Your shipment arr...|          4.5|
[2023-01-26T21:56:10.702+0000] {spark_submit.py:495} INFO - |MLA1119996182|Disco Sólido Inte...| 25499.0|          new|            MALL WEB|2016-05-26 20:28:35|         500|                1|Your shipment arr...|          4.5|
[2023-01-26T21:56:10.702+0000] {spark_submit.py:495} INFO - |MLA1183925519|Placa De Video Nv...|299999.0|          new|ESTUDIOSMPSRLESTU...|2020-11-29 18:36:32|          50|                1|Your shipment arr...|          4.5|
[2023-01-26T21:56:10.702+0000] {spark_submit.py:495} INFO - | MLA844489972|Disco Duro Intern...| 11599.0|          new|           DALECLÍCK|2015-03-16 12:07:23|        5000|              500|You have to pay t...|          5.0|
[2023-01-26T21:56:10.702+0000] {spark_submit.py:495} INFO - |MLA1168362675|Memoria Ram Fury ...| 26698.0|          new|MARSTECHSRLMARSTE...|2020-08-05 14:27:59|         150|                1|Your shipment arr...|          4.5|
[2023-01-26T21:56:10.702+0000] {spark_submit.py:495} INFO - |MLA1163233412|Placa De Video Nv...| 34659.0|          new|      PCREGISTRADAOK|2020-11-02 13:17:33|           1|                1|Your shipment arr...|          4.5|
[2023-01-26T21:56:10.702+0000] {spark_submit.py:495} INFO - |MLA1314590445|Disco Sólido Inte...|  7999.0|          new|       ELECTRO-SHOWS|2010-03-26 01:08:16|         250|              250|You have to pay t...|          4.5|
[2023-01-26T21:56:10.702+0000] {spark_submit.py:495} INFO - |MLA1302136118|Disco Duro Intern...| 18271.0|          new|DIAMONDSYSTEMSANJ...|2016-04-08 17:21:06|         100|               50|Your shipment arr...|          4.5|
[2023-01-26T21:56:10.702+0000] {spark_submit.py:495} INFO - +-------------+--------------------+--------+-------------+--------------------+-------------------+------------+-----------------+--------------------+-------------+
[2023-01-26T21:56:10.702+0000] {spark_submit.py:495} INFO - only showing top 20 rows
[2023-01-26T21:56:10.702+0000] {spark_submit.py:495} INFO - 
[2023-01-26T21:56:36.773+0000] {spark_submit.py:495} INFO - 23/01/26 21:56:36 ERROR Executor: Exception in task 0.0 in stage 31.0 (TID 31)
[2023-01-26T21:56:36.773+0000] {spark_submit.py:495} INFO - com.mongodb.MongoTimeoutException: Timed out after 30000 ms while waiting to connect. Client view of cluster state is {type=UNKNOWN, servers=[{address=172.22.0.2:27017, type=UNKNOWN, state=CONNECTING, exception={com.mongodb.MongoSocketOpenException: Exception opening socket}, caused by {java.net.SocketTimeoutException: connect timed out}}]
[2023-01-26T21:56:36.773+0000] {spark_submit.py:495} INFO - at com.mongodb.internal.connection.BaseCluster.getDescription(BaseCluster.java:177)
[2023-01-26T21:56:36.774+0000] {spark_submit.py:495} INFO - at com.mongodb.internal.connection.SingleServerCluster.getDescription(SingleServerCluster.java:41)
[2023-01-26T21:56:36.774+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate.getConnectedClusterDescription(MongoClientDelegate.java:147)
[2023-01-26T21:56:36.774+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate.createClientSession(MongoClientDelegate.java:98)
[2023-01-26T21:56:36.774+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.getClientSession(MongoClientDelegate.java:278)
[2023-01-26T21:56:36.774+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:202)
[2023-01-26T21:56:36.774+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:441)
[2023-01-26T21:56:36.774+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:421)
[2023-01-26T21:56:36.774+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$4(MongoSpark.scala:138)
[2023-01-26T21:56:36.774+0000] {spark_submit.py:495} INFO - at scala.collection.Iterator.foreach(Iterator.scala:943)
[2023-01-26T21:56:36.774+0000] {spark_submit.py:495} INFO - at scala.collection.Iterator.foreach$(Iterator.scala:943)
[2023-01-26T21:56:36.774+0000] {spark_submit.py:495} INFO - at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
[2023-01-26T21:56:36.777+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$3(MongoSpark.scala:122)
[2023-01-26T21:56:36.780+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$3$adapted(MongoSpark.scala:121)
[2023-01-26T21:56:36.780+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.$anonfun$withCollectionDo$1(MongoConnector.scala:186)
[2023-01-26T21:56:36.780+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.$anonfun$withDatabaseDo$1(MongoConnector.scala:171)
[2023-01-26T21:56:36.780+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withMongoClientDo(MongoConnector.scala:154)
[2023-01-26T21:56:36.780+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withDatabaseDo(MongoConnector.scala:171)
[2023-01-26T21:56:36.780+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withCollectionDo(MongoConnector.scala:184)
[2023-01-26T21:56:36.780+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$2(MongoSpark.scala:121)
[2023-01-26T21:56:36.780+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$2$adapted(MongoSpark.scala:120)
[2023-01-26T21:56:36.780+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1011)
[2023-01-26T21:56:36.780+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1011)
[2023-01-26T21:56:36.781+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2268)
[2023-01-26T21:56:36.781+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
[2023-01-26T21:56:36.781+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.Task.run(Task.scala:136)
[2023-01-26T21:56:36.781+0000] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)
[2023-01-26T21:56:36.781+0000] {spark_submit.py:495} INFO - at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)
[2023-01-26T21:56:36.781+0000] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)
[2023-01-26T21:56:36.781+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[2023-01-26T21:56:36.781+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[2023-01-26T21:56:36.781+0000] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2023-01-26T21:56:36.781+0000] {spark_submit.py:495} INFO - 23/01/26 21:56:36 ERROR TaskSetManager: Task 0 in stage 31.0 failed 1 times; aborting job
[2023-01-26T21:56:36.930+0000] {spark_submit.py:495} INFO - [[34m2023-01-26 21:56:36,925[0m] {[34mconsumerSpark.py:[0m37} ERROR[0m - Error al escribir en Mongo: An error occurred while calling o302.save.
[2023-01-26T21:56:36.930+0000] {spark_submit.py:495} INFO - : org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 31.0 failed 1 times, most recent failure: Lost task 0.0 in stage 31.0 (TID 31) (f1cbb5df32e6 executor driver): com.mongodb.MongoTimeoutException: Timed out after 30000 ms while waiting to connect. Client view of cluster state is {type=UNKNOWN, servers=[{address=172.22.0.2:27017, type=UNKNOWN, state=CONNECTING, exception={com.mongodb.MongoSocketOpenException: Exception opening socket}, caused by {java.net.SocketTimeoutException: connect timed out}}]
[2023-01-26T21:56:36.930+0000] {spark_submit.py:495} INFO - at com.mongodb.internal.connection.BaseCluster.getDescription(BaseCluster.java:177)
[2023-01-26T21:56:36.930+0000] {spark_submit.py:495} INFO - at com.mongodb.internal.connection.SingleServerCluster.getDescription(SingleServerCluster.java:41)
[2023-01-26T21:56:36.930+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate.getConnectedClusterDescription(MongoClientDelegate.java:147)
[2023-01-26T21:56:36.930+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate.createClientSession(MongoClientDelegate.java:98)
[2023-01-26T21:56:36.930+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.getClientSession(MongoClientDelegate.java:278)
[2023-01-26T21:56:36.930+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:202)
[2023-01-26T21:56:36.930+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:441)
[2023-01-26T21:56:36.930+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:421)
[2023-01-26T21:56:36.931+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$4(MongoSpark.scala:138)
[2023-01-26T21:56:36.931+0000] {spark_submit.py:495} INFO - at scala.collection.Iterator.foreach(Iterator.scala:943)
[2023-01-26T21:56:36.931+0000] {spark_submit.py:495} INFO - at scala.collection.Iterator.foreach$(Iterator.scala:943)
[2023-01-26T21:56:36.931+0000] {spark_submit.py:495} INFO - at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
[2023-01-26T21:56:36.931+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$3(MongoSpark.scala:122)
[2023-01-26T21:56:36.931+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$3$adapted(MongoSpark.scala:121)
[2023-01-26T21:56:36.931+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.$anonfun$withCollectionDo$1(MongoConnector.scala:186)
[2023-01-26T21:56:36.931+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.$anonfun$withDatabaseDo$1(MongoConnector.scala:171)
[2023-01-26T21:56:36.931+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withMongoClientDo(MongoConnector.scala:154)
[2023-01-26T21:56:36.931+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withDatabaseDo(MongoConnector.scala:171)
[2023-01-26T21:56:36.931+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withCollectionDo(MongoConnector.scala:184)
[2023-01-26T21:56:36.931+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$2(MongoSpark.scala:121)
[2023-01-26T21:56:36.931+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$2$adapted(MongoSpark.scala:120)
[2023-01-26T21:56:36.932+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1011)
[2023-01-26T21:56:36.932+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1011)
[2023-01-26T21:56:36.932+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2268)
[2023-01-26T21:56:36.932+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
[2023-01-26T21:56:36.932+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.Task.run(Task.scala:136)
[2023-01-26T21:56:36.932+0000] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)
[2023-01-26T21:56:36.932+0000] {spark_submit.py:495} INFO - at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)
[2023-01-26T21:56:36.932+0000] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)
[2023-01-26T21:56:36.944+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[2023-01-26T21:56:36.944+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[2023-01-26T21:56:36.945+0000] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2023-01-26T21:56:36.945+0000] {spark_submit.py:495} INFO - 
[2023-01-26T21:56:36.945+0000] {spark_submit.py:495} INFO - Driver stacktrace:
[2023-01-26T21:56:36.945+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)
[2023-01-26T21:56:36.945+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)
[2023-01-26T21:56:36.945+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)
[2023-01-26T21:56:36.945+0000] {spark_submit.py:495} INFO - at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
[2023-01-26T21:56:36.946+0000] {spark_submit.py:495} INFO - at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
[2023-01-26T21:56:36.946+0000] {spark_submit.py:495} INFO - at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
[2023-01-26T21:56:36.946+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)
[2023-01-26T21:56:36.946+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)
[2023-01-26T21:56:36.946+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)
[2023-01-26T21:56:36.946+0000] {spark_submit.py:495} INFO - at scala.Option.foreach(Option.scala:407)
[2023-01-26T21:56:36.946+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)
[2023-01-26T21:56:36.946+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)
[2023-01-26T21:56:36.946+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)
[2023-01-26T21:56:36.946+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)
[2023-01-26T21:56:36.947+0000] {spark_submit.py:495} INFO - at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
[2023-01-26T21:56:36.947+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)
[2023-01-26T21:56:36.947+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.runJob(SparkContext.scala:2228)
[2023-01-26T21:56:36.947+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.runJob(SparkContext.scala:2249)
[2023-01-26T21:56:36.947+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.runJob(SparkContext.scala:2268)
[2023-01-26T21:56:36.947+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.runJob(SparkContext.scala:2293)
[2023-01-26T21:56:36.947+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$1(RDD.scala:1011)
[2023-01-26T21:56:36.947+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
[2023-01-26T21:56:36.947+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
[2023-01-26T21:56:36.947+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.withScope(RDD.scala:406)
[2023-01-26T21:56:36.947+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.foreachPartition(RDD.scala:1009)
[2023-01-26T21:56:36.948+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.save(MongoSpark.scala:120)
[2023-01-26T21:56:36.948+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.save(MongoSpark.scala:169)
[2023-01-26T21:56:36.948+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.sql.DefaultSource.createRelation(DefaultSource.scala:70)
[2023-01-26T21:56:36.948+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:47)
[2023-01-26T21:56:36.948+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)
[2023-01-26T21:56:36.948+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)
[2023-01-26T21:56:36.948+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)
[2023-01-26T21:56:36.948+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)
[2023-01-26T21:56:36.948+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)
[2023-01-26T21:56:36.948+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)
[2023-01-26T21:56:36.948+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)
[2023-01-26T21:56:36.948+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
[2023-01-26T21:56:36.949+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
[2023-01-26T21:56:36.949+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)
[2023-01-26T21:56:36.949+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)
[2023-01-26T21:56:36.949+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:584)
[2023-01-26T21:56:36.949+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:176)
[2023-01-26T21:56:36.949+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:584)
[2023-01-26T21:56:36.949+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)
[2023-01-26T21:56:36.949+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
[2023-01-26T21:56:36.949+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
[2023-01-26T21:56:36.949+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
[2023-01-26T21:56:36.949+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
[2023-01-26T21:56:36.949+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:560)
[2023-01-26T21:56:36.949+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)
[2023-01-26T21:56:36.949+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)
[2023-01-26T21:56:36.950+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)
[2023-01-26T21:56:36.950+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:116)
[2023-01-26T21:56:36.950+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:860)
[2023-01-26T21:56:36.950+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:390)
[2023-01-26T21:56:36.950+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:363)
[2023-01-26T21:56:36.950+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:247)
[2023-01-26T21:56:36.950+0000] {spark_submit.py:495} INFO - at jdk.internal.reflect.GeneratedMethodAccessor69.invoke(Unknown Source)
[2023-01-26T21:56:36.950+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
[2023-01-26T21:56:36.950+0000] {spark_submit.py:495} INFO - at java.base/java.lang.reflect.Method.invoke(Method.java:566)
[2023-01-26T21:56:36.950+0000] {spark_submit.py:495} INFO - at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
[2023-01-26T21:56:36.950+0000] {spark_submit.py:495} INFO - at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
[2023-01-26T21:56:36.950+0000] {spark_submit.py:495} INFO - at py4j.Gateway.invoke(Gateway.java:282)
[2023-01-26T21:56:36.951+0000] {spark_submit.py:495} INFO - at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
[2023-01-26T21:56:36.951+0000] {spark_submit.py:495} INFO - at py4j.commands.CallCommand.execute(CallCommand.java:79)
[2023-01-26T21:56:36.951+0000] {spark_submit.py:495} INFO - at py4j.ClientServerConnection.sendCommand(ClientServerConnection.java:244)
[2023-01-26T21:56:36.951+0000] {spark_submit.py:495} INFO - at py4j.CallbackClient.sendCommand(CallbackClient.java:384)
[2023-01-26T21:56:36.951+0000] {spark_submit.py:495} INFO - at py4j.CallbackClient.sendCommand(CallbackClient.java:356)
[2023-01-26T21:56:36.951+0000] {spark_submit.py:495} INFO - at py4j.reflection.PythonProxyHandler.invoke(PythonProxyHandler.java:106)
[2023-01-26T21:56:36.951+0000] {spark_submit.py:495} INFO - at com.sun.proxy.$Proxy32.call(Unknown Source)
[2023-01-26T21:56:36.951+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1(ForeachBatchSink.scala:51)
[2023-01-26T21:56:36.951+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1$adapted(ForeachBatchSink.scala:51)
[2023-01-26T21:56:36.951+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.sources.ForeachBatchSink.addBatch(ForeachBatchSink.scala:32)
[2023-01-26T21:56:36.951+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:666)
[2023-01-26T21:56:36.951+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)
[2023-01-26T21:56:36.951+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)
[2023-01-26T21:56:36.951+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)
[2023-01-26T21:56:36.952+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
[2023-01-26T21:56:36.952+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
[2023-01-26T21:56:36.952+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:664)
[2023-01-26T21:56:36.952+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:375)
[2023-01-26T21:56:36.952+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:373)
[2023-01-26T21:56:36.960+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:68)
[2023-01-26T21:56:36.960+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:664)
[2023-01-26T21:56:36.960+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:256)
[2023-01-26T21:56:36.960+0000] {spark_submit.py:495} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2023-01-26T21:56:36.961+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:375)
[2023-01-26T21:56:36.961+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:373)
[2023-01-26T21:56:36.961+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:68)
[2023-01-26T21:56:36.961+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:219)
[2023-01-26T21:56:36.961+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)
[2023-01-26T21:56:36.961+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:213)
[2023-01-26T21:56:36.961+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:307)
[2023-01-26T21:56:36.961+0000] {spark_submit.py:495} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2023-01-26T21:56:36.961+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
[2023-01-26T21:56:36.961+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:285)
[2023-01-26T21:56:36.961+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:208)
[2023-01-26T21:56:36.961+0000] {spark_submit.py:495} INFO - Caused by: com.mongodb.MongoTimeoutException: Timed out after 30000 ms while waiting to connect. Client view of cluster state is {type=UNKNOWN, servers=[{address=172.22.0.2:27017, type=UNKNOWN, state=CONNECTING, exception={com.mongodb.MongoSocketOpenException: Exception opening socket}, caused by {java.net.SocketTimeoutException: connect timed out}}]
[2023-01-26T21:56:36.962+0000] {spark_submit.py:495} INFO - at com.mongodb.internal.connection.BaseCluster.getDescription(BaseCluster.java:177)
[2023-01-26T21:56:36.962+0000] {spark_submit.py:495} INFO - at com.mongodb.internal.connection.SingleServerCluster.getDescription(SingleServerCluster.java:41)
[2023-01-26T21:56:36.962+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate.getConnectedClusterDescription(MongoClientDelegate.java:147)
[2023-01-26T21:56:36.962+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate.createClientSession(MongoClientDelegate.java:98)
[2023-01-26T21:56:36.962+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.getClientSession(MongoClientDelegate.java:278)
[2023-01-26T21:56:36.962+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:202)
[2023-01-26T21:56:36.962+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:441)
[2023-01-26T21:56:36.962+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:421)
[2023-01-26T21:56:36.962+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$4(MongoSpark.scala:138)
[2023-01-26T21:56:36.962+0000] {spark_submit.py:495} INFO - at scala.collection.Iterator.foreach(Iterator.scala:943)
[2023-01-26T21:56:36.962+0000] {spark_submit.py:495} INFO - at scala.collection.Iterator.foreach$(Iterator.scala:943)
[2023-01-26T21:56:36.962+0000] {spark_submit.py:495} INFO - at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
[2023-01-26T21:56:36.962+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$3(MongoSpark.scala:122)
[2023-01-26T21:56:36.962+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$3$adapted(MongoSpark.scala:121)
[2023-01-26T21:56:36.963+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.$anonfun$withCollectionDo$1(MongoConnector.scala:186)
[2023-01-26T21:56:36.963+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.$anonfun$withDatabaseDo$1(MongoConnector.scala:171)
[2023-01-26T21:56:36.963+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withMongoClientDo(MongoConnector.scala:154)
[2023-01-26T21:56:36.963+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withDatabaseDo(MongoConnector.scala:171)
[2023-01-26T21:56:36.963+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withCollectionDo(MongoConnector.scala:184)
[2023-01-26T21:56:36.963+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$2(MongoSpark.scala:121)
[2023-01-26T21:56:36.963+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$2$adapted(MongoSpark.scala:120)
[2023-01-26T21:56:36.963+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1011)
[2023-01-26T21:56:36.963+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1011)
[2023-01-26T21:56:36.963+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2268)
[2023-01-26T21:56:36.963+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
[2023-01-26T21:56:36.963+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.Task.run(Task.scala:136)
[2023-01-26T21:56:36.963+0000] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)
[2023-01-26T21:56:36.964+0000] {spark_submit.py:495} INFO - at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)
[2023-01-26T21:56:36.964+0000] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)
[2023-01-26T21:56:36.964+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[2023-01-26T21:56:36.964+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[2023-01-26T21:56:36.964+0000] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2023-01-26T21:56:36.964+0000] {spark_submit.py:495} INFO - [0m
[2023-01-26T21:56:37.283+0000] {spark_submit.py:495} INFO - [[34m2023-01-26 21:56:37,283[0m] {[34mclientserver.py:[0m577} INFO[0m - Received command c on object id p0[0m
[2023-01-26T21:57:07.424+0000] {spark_submit.py:495} INFO - 23/01/26 21:57:07 ERROR Executor: Exception in task 0.0 in stage 33.0 (TID 33)
[2023-01-26T21:57:07.424+0000] {spark_submit.py:495} INFO - com.mongodb.MongoTimeoutException: Timed out after 30000 ms while waiting to connect. Client view of cluster state is {type=UNKNOWN, servers=[{address=172.22.0.2:27017, type=UNKNOWN, state=CONNECTING, exception={com.mongodb.MongoSocketOpenException: Exception opening socket}, caused by {java.net.SocketTimeoutException: connect timed out}}]
[2023-01-26T21:57:07.425+0000] {spark_submit.py:495} INFO - at com.mongodb.internal.connection.BaseCluster.getDescription(BaseCluster.java:177)
[2023-01-26T21:57:07.425+0000] {spark_submit.py:495} INFO - at com.mongodb.internal.connection.SingleServerCluster.getDescription(SingleServerCluster.java:41)
[2023-01-26T21:57:07.426+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate.getConnectedClusterDescription(MongoClientDelegate.java:147)
[2023-01-26T21:57:07.426+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate.createClientSession(MongoClientDelegate.java:98)
[2023-01-26T21:57:07.426+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.getClientSession(MongoClientDelegate.java:278)
[2023-01-26T21:57:07.427+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:202)
[2023-01-26T21:57:07.427+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:441)
[2023-01-26T21:57:07.427+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:421)
[2023-01-26T21:57:07.428+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$4(MongoSpark.scala:138)
[2023-01-26T21:57:07.428+0000] {spark_submit.py:495} INFO - at scala.collection.Iterator.foreach(Iterator.scala:943)
[2023-01-26T21:57:07.428+0000] {spark_submit.py:495} INFO - at scala.collection.Iterator.foreach$(Iterator.scala:943)
[2023-01-26T21:57:07.429+0000] {spark_submit.py:495} INFO - at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
[2023-01-26T21:57:07.429+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$3(MongoSpark.scala:122)
[2023-01-26T21:57:07.429+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$3$adapted(MongoSpark.scala:121)
[2023-01-26T21:57:07.429+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.$anonfun$withCollectionDo$1(MongoConnector.scala:186)
[2023-01-26T21:57:07.429+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.$anonfun$withDatabaseDo$1(MongoConnector.scala:171)
[2023-01-26T21:57:07.430+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withMongoClientDo(MongoConnector.scala:154)
[2023-01-26T21:57:07.430+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withDatabaseDo(MongoConnector.scala:171)
[2023-01-26T21:57:07.430+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withCollectionDo(MongoConnector.scala:184)
[2023-01-26T21:57:07.430+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$2(MongoSpark.scala:121)
[2023-01-26T21:57:07.431+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$2$adapted(MongoSpark.scala:120)
[2023-01-26T21:57:07.431+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1011)
[2023-01-26T21:57:07.431+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1011)
[2023-01-26T21:57:07.432+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2268)
[2023-01-26T21:57:07.432+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
[2023-01-26T21:57:07.432+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.Task.run(Task.scala:136)
[2023-01-26T21:57:07.433+0000] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)
[2023-01-26T21:57:07.433+0000] {spark_submit.py:495} INFO - at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)
[2023-01-26T21:57:07.433+0000] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)
[2023-01-26T21:57:07.433+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[2023-01-26T21:57:07.434+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[2023-01-26T21:57:07.434+0000] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2023-01-26T21:57:07.434+0000] {spark_submit.py:495} INFO - 23/01/26 21:57:07 ERROR TaskSetManager: Task 0 in stage 33.0 failed 1 times; aborting job
[2023-01-26T21:57:07.601+0000] {spark_submit.py:495} INFO - [[34m2023-01-26 21:57:07,600[0m] {[34mconsumerSpark.py:[0m37} ERROR[0m - Error al escribir en Mongo: An error occurred while calling o311.save.
[2023-01-26T21:57:07.601+0000] {spark_submit.py:495} INFO - : org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 33.0 failed 1 times, most recent failure: Lost task 0.0 in stage 33.0 (TID 33) (f1cbb5df32e6 executor driver): com.mongodb.MongoTimeoutException: Timed out after 30000 ms while waiting to connect. Client view of cluster state is {type=UNKNOWN, servers=[{address=172.22.0.2:27017, type=UNKNOWN, state=CONNECTING, exception={com.mongodb.MongoSocketOpenException: Exception opening socket}, caused by {java.net.SocketTimeoutException: connect timed out}}]
[2023-01-26T21:57:07.601+0000] {spark_submit.py:495} INFO - at com.mongodb.internal.connection.BaseCluster.getDescription(BaseCluster.java:177)
[2023-01-26T21:57:07.601+0000] {spark_submit.py:495} INFO - at com.mongodb.internal.connection.SingleServerCluster.getDescription(SingleServerCluster.java:41)
[2023-01-26T21:57:07.602+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate.getConnectedClusterDescription(MongoClientDelegate.java:147)
[2023-01-26T21:57:07.602+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate.createClientSession(MongoClientDelegate.java:98)
[2023-01-26T21:57:07.602+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.getClientSession(MongoClientDelegate.java:278)
[2023-01-26T21:57:07.602+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:202)
[2023-01-26T21:57:07.602+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:441)
[2023-01-26T21:57:07.602+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:421)
[2023-01-26T21:57:07.603+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$4(MongoSpark.scala:138)
[2023-01-26T21:57:07.603+0000] {spark_submit.py:495} INFO - at scala.collection.Iterator.foreach(Iterator.scala:943)
[2023-01-26T21:57:07.603+0000] {spark_submit.py:495} INFO - at scala.collection.Iterator.foreach$(Iterator.scala:943)
[2023-01-26T21:57:07.603+0000] {spark_submit.py:495} INFO - at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
[2023-01-26T21:57:07.603+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$3(MongoSpark.scala:122)
[2023-01-26T21:57:07.603+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$3$adapted(MongoSpark.scala:121)
[2023-01-26T21:57:07.604+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.$anonfun$withCollectionDo$1(MongoConnector.scala:186)
[2023-01-26T21:57:07.604+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.$anonfun$withDatabaseDo$1(MongoConnector.scala:171)
[2023-01-26T21:57:07.604+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withMongoClientDo(MongoConnector.scala:154)
[2023-01-26T21:57:07.604+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withDatabaseDo(MongoConnector.scala:171)
[2023-01-26T21:57:07.604+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withCollectionDo(MongoConnector.scala:184)
[2023-01-26T21:57:07.604+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$2(MongoSpark.scala:121)
[2023-01-26T21:57:07.605+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$2$adapted(MongoSpark.scala:120)
[2023-01-26T21:57:07.605+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1011)
[2023-01-26T21:57:07.605+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1011)
[2023-01-26T21:57:07.605+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2268)
[2023-01-26T21:57:07.605+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
[2023-01-26T21:57:07.605+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.Task.run(Task.scala:136)
[2023-01-26T21:57:07.606+0000] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)
[2023-01-26T21:57:07.606+0000] {spark_submit.py:495} INFO - at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)
[2023-01-26T21:57:07.606+0000] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)
[2023-01-26T21:57:07.606+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[2023-01-26T21:57:07.606+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[2023-01-26T21:57:07.606+0000] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2023-01-26T21:57:07.607+0000] {spark_submit.py:495} INFO - 
[2023-01-26T21:57:07.607+0000] {spark_submit.py:495} INFO - Driver stacktrace:
[2023-01-26T21:57:07.607+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)
[2023-01-26T21:57:07.607+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)
[2023-01-26T21:57:07.607+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)
[2023-01-26T21:57:07.607+0000] {spark_submit.py:495} INFO - at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
[2023-01-26T21:57:07.608+0000] {spark_submit.py:495} INFO - at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
[2023-01-26T21:57:07.608+0000] {spark_submit.py:495} INFO - at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
[2023-01-26T21:57:07.608+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)
[2023-01-26T21:57:07.608+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)
[2023-01-26T21:57:07.608+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)
[2023-01-26T21:57:07.609+0000] {spark_submit.py:495} INFO - at scala.Option.foreach(Option.scala:407)
[2023-01-26T21:57:07.609+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)
[2023-01-26T21:57:07.609+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)
[2023-01-26T21:57:07.609+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)
[2023-01-26T21:57:07.609+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)
[2023-01-26T21:57:07.609+0000] {spark_submit.py:495} INFO - at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
[2023-01-26T21:57:07.610+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)
[2023-01-26T21:57:07.610+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.runJob(SparkContext.scala:2228)
[2023-01-26T21:57:07.610+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.runJob(SparkContext.scala:2249)
[2023-01-26T21:57:07.610+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.runJob(SparkContext.scala:2268)
[2023-01-26T21:57:07.610+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.runJob(SparkContext.scala:2293)
[2023-01-26T21:57:07.610+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$1(RDD.scala:1011)
[2023-01-26T21:57:07.611+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
[2023-01-26T21:57:07.611+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
[2023-01-26T21:57:07.611+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.withScope(RDD.scala:406)
[2023-01-26T21:57:07.611+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.foreachPartition(RDD.scala:1009)
[2023-01-26T21:57:07.611+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.save(MongoSpark.scala:120)
[2023-01-26T21:57:07.611+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.save(MongoSpark.scala:169)
[2023-01-26T21:57:07.612+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.sql.DefaultSource.createRelation(DefaultSource.scala:70)
[2023-01-26T21:57:07.612+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:47)
[2023-01-26T21:57:07.612+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)
[2023-01-26T21:57:07.612+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)
[2023-01-26T21:57:07.612+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)
[2023-01-26T21:57:07.612+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)
[2023-01-26T21:57:07.613+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)
[2023-01-26T21:57:07.613+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)
[2023-01-26T21:57:07.613+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)
[2023-01-26T21:57:07.613+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
[2023-01-26T21:57:07.613+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
[2023-01-26T21:57:07.613+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)
[2023-01-26T21:57:07.614+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)
[2023-01-26T21:57:07.614+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:584)
[2023-01-26T21:57:07.614+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:176)
[2023-01-26T21:57:07.614+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:584)
[2023-01-26T21:57:07.614+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)
[2023-01-26T21:57:07.614+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
[2023-01-26T21:57:07.615+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
[2023-01-26T21:57:07.615+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
[2023-01-26T21:57:07.615+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
[2023-01-26T21:57:07.615+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:560)
[2023-01-26T21:57:07.615+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)
[2023-01-26T21:57:07.615+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)
[2023-01-26T21:57:07.616+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)
[2023-01-26T21:57:07.616+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:116)
[2023-01-26T21:57:07.616+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:860)
[2023-01-26T21:57:07.616+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:390)
[2023-01-26T21:57:07.616+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:363)
[2023-01-26T21:57:07.617+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:247)
[2023-01-26T21:57:07.617+0000] {spark_submit.py:495} INFO - at jdk.internal.reflect.GeneratedMethodAccessor69.invoke(Unknown Source)
[2023-01-26T21:57:07.617+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
[2023-01-26T21:57:07.617+0000] {spark_submit.py:495} INFO - at java.base/java.lang.reflect.Method.invoke(Method.java:566)
[2023-01-26T21:57:07.617+0000] {spark_submit.py:495} INFO - at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
[2023-01-26T21:57:07.617+0000] {spark_submit.py:495} INFO - at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
[2023-01-26T21:57:07.618+0000] {spark_submit.py:495} INFO - at py4j.Gateway.invoke(Gateway.java:282)
[2023-01-26T21:57:07.618+0000] {spark_submit.py:495} INFO - at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
[2023-01-26T21:57:07.618+0000] {spark_submit.py:495} INFO - at py4j.commands.CallCommand.execute(CallCommand.java:79)
[2023-01-26T21:57:07.618+0000] {spark_submit.py:495} INFO - at py4j.ClientServerConnection.sendCommand(ClientServerConnection.java:244)
[2023-01-26T21:57:07.618+0000] {spark_submit.py:495} INFO - at py4j.CallbackClient.sendCommand(CallbackClient.java:384)
[2023-01-26T21:57:07.619+0000] {spark_submit.py:495} INFO - at py4j.CallbackClient.sendCommand(CallbackClient.java:356)
[2023-01-26T21:57:07.619+0000] {spark_submit.py:495} INFO - at py4j.reflection.PythonProxyHandler.invoke(PythonProxyHandler.java:106)
[2023-01-26T21:57:07.619+0000] {spark_submit.py:495} INFO - at com.sun.proxy.$Proxy32.call(Unknown Source)
[2023-01-26T21:57:07.619+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1(ForeachBatchSink.scala:51)
[2023-01-26T21:57:07.619+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1$adapted(ForeachBatchSink.scala:51)
[2023-01-26T21:57:07.619+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.sources.ForeachBatchSink.addBatch(ForeachBatchSink.scala:32)
[2023-01-26T21:57:07.620+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:666)
[2023-01-26T21:57:07.620+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)
[2023-01-26T21:57:07.620+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)
[2023-01-26T21:57:07.620+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)
[2023-01-26T21:57:07.620+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
[2023-01-26T21:57:07.620+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
[2023-01-26T21:57:07.621+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:664)
[2023-01-26T21:57:07.621+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:375)
[2023-01-26T21:57:07.621+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:373)
[2023-01-26T21:57:07.621+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:68)
[2023-01-26T21:57:07.621+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:664)
[2023-01-26T21:57:07.621+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:256)
[2023-01-26T21:57:07.622+0000] {spark_submit.py:495} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2023-01-26T21:57:07.622+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:375)
[2023-01-26T21:57:07.622+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:373)
[2023-01-26T21:57:07.622+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:68)
[2023-01-26T21:57:07.622+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:219)
[2023-01-26T21:57:07.622+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)
[2023-01-26T21:57:07.623+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:213)
[2023-01-26T21:57:07.623+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:307)
[2023-01-26T21:57:07.623+0000] {spark_submit.py:495} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2023-01-26T21:57:07.623+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
[2023-01-26T21:57:07.623+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:285)
[2023-01-26T21:57:07.624+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:208)
[2023-01-26T21:57:07.624+0000] {spark_submit.py:495} INFO - Caused by: com.mongodb.MongoTimeoutException: Timed out after 30000 ms while waiting to connect. Client view of cluster state is {type=UNKNOWN, servers=[{address=172.22.0.2:27017, type=UNKNOWN, state=CONNECTING, exception={com.mongodb.MongoSocketOpenException: Exception opening socket}, caused by {java.net.SocketTimeoutException: connect timed out}}]
[2023-01-26T21:57:07.624+0000] {spark_submit.py:495} INFO - at com.mongodb.internal.connection.BaseCluster.getDescription(BaseCluster.java:177)
[2023-01-26T21:57:07.624+0000] {spark_submit.py:495} INFO - at com.mongodb.internal.connection.SingleServerCluster.getDescription(SingleServerCluster.java:41)
[2023-01-26T21:57:07.624+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate.getConnectedClusterDescription(MongoClientDelegate.java:147)
[2023-01-26T21:57:07.624+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate.createClientSession(MongoClientDelegate.java:98)
[2023-01-26T21:57:07.625+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.getClientSession(MongoClientDelegate.java:278)
[2023-01-26T21:57:07.625+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:202)
[2023-01-26T21:57:07.625+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:441)
[2023-01-26T21:57:07.625+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:421)
[2023-01-26T21:57:07.625+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$4(MongoSpark.scala:138)
[2023-01-26T21:57:07.625+0000] {spark_submit.py:495} INFO - at scala.collection.Iterator.foreach(Iterator.scala:943)
[2023-01-26T21:57:07.626+0000] {spark_submit.py:495} INFO - at scala.collection.Iterator.foreach$(Iterator.scala:943)
[2023-01-26T21:57:07.626+0000] {spark_submit.py:495} INFO - at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
[2023-01-26T21:57:07.626+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$3(MongoSpark.scala:122)
[2023-01-26T21:57:07.626+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$3$adapted(MongoSpark.scala:121)
[2023-01-26T21:57:07.626+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.$anonfun$withCollectionDo$1(MongoConnector.scala:186)
[2023-01-26T21:57:07.626+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.$anonfun$withDatabaseDo$1(MongoConnector.scala:171)
[2023-01-26T21:57:07.627+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withMongoClientDo(MongoConnector.scala:154)
[2023-01-26T21:57:07.627+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withDatabaseDo(MongoConnector.scala:171)
[2023-01-26T21:57:07.627+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withCollectionDo(MongoConnector.scala:184)
[2023-01-26T21:57:07.627+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$2(MongoSpark.scala:121)
[2023-01-26T21:57:07.627+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$2$adapted(MongoSpark.scala:120)
[2023-01-26T21:57:07.627+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1011)
[2023-01-26T21:57:07.628+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1011)
[2023-01-26T21:57:07.628+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2268)
[2023-01-26T21:57:07.628+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
[2023-01-26T21:57:07.628+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.Task.run(Task.scala:136)
[2023-01-26T21:57:07.628+0000] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)
[2023-01-26T21:57:07.628+0000] {spark_submit.py:495} INFO - at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)
[2023-01-26T21:57:07.629+0000] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)
[2023-01-26T21:57:07.629+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[2023-01-26T21:57:07.629+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[2023-01-26T21:57:07.629+0000] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2023-01-26T21:57:07.629+0000] {spark_submit.py:495} INFO - [0m
[2023-01-26T23:15:11.182+0000] {spark_submit.py:495} INFO - [[34m2023-01-26 23:15:11,182[0m] {[34mclientserver.py:[0m577} INFO[0m - Received command c on object id p0[0m
[2023-01-26T23:15:15.656+0000] {spark_submit.py:495} INFO - -------------------------------------------
[2023-01-26T23:15:15.656+0000] {spark_submit.py:495} INFO - Batch: 12
[2023-01-26T23:15:15.656+0000] {spark_submit.py:495} INFO - -------------------------------------------
[2023-01-26T23:15:15.682+0000] {spark_submit.py:495} INFO - +-------------+--------------------+--------+-------------+--------------------+-------------------+------------+-----------------+--------------------+-------------+
[2023-01-26T23:15:15.682+0000] {spark_submit.py:495} INFO - |           id|       nameOfProduct|   price|ProductStatus|          SellerName|   RegistrationDate|QuantitySold|QuantityAvailable|      Free shipping?|Store ratings|
[2023-01-26T23:15:15.682+0000] {spark_submit.py:495} INFO - +-------------+--------------------+--------+-------------+--------------------+-------------------+------------+-----------------+--------------------+-------------+
[2023-01-26T23:15:15.683+0000] {spark_submit.py:495} INFO - |MLA1220964588|Procesador Gamer ...| 45899.0|          new|    OVERHARD DIGITAL|2015-01-08 12:19:41|          50|                1|Your shipment arr...|          4.5|
[2023-01-26T23:15:15.683+0000] {spark_submit.py:495} INFO - |MLA1157512489|Placa De Video Nv...|124999.0|          new|      PCREGISTRADAOK|2020-11-02 13:17:33|          50|                1|Your shipment arr...|          4.5|
[2023-01-26T23:15:15.683+0000] {spark_submit.py:495} INFO - |MLA1221192372|Disco Duro Extern...| 26299.0|          new|    OVERHARD DIGITAL|2015-01-08 12:19:41|         250|              100|Your shipment arr...|          4.5|
[2023-01-26T23:15:15.683+0000] {spark_submit.py:495} INFO - |MLA1163238194|Placa De Video Am...| 33999.0|          new|       MICYBERCOMPRA|2011-09-13 13:15:53|          50|                1|Your shipment arr...|          4.5|
[2023-01-26T23:15:15.683+0000] {spark_submit.py:495} INFO - |MLA1160204806|Memoria Ram Fury ...| 15499.0|          new|    OVERHARD DIGITAL|2015-01-08 12:19:41|         500|               50|Your shipment arr...|          4.5|
[2023-01-26T23:15:15.683+0000] {spark_submit.py:495} INFO - |MLA1115295971|Procesador Amd Ry...| 93999.0|          new|      PCREGISTRADAOK|2020-11-02 13:17:33|          25|                1|Your shipment arr...|          4.5|
[2023-01-26T23:15:15.683+0000] {spark_submit.py:495} INFO - |MLA1318244331|Disco Solido Ssd ...| 28799.0|          new|    OVERHARD DIGITAL|2015-01-08 12:19:41|           5|              150|Your shipment arr...|          4.5|
[2023-01-26T23:15:15.683+0000] {spark_submit.py:495} INFO - | MLA923637669|Placa De Video Ge...| 28699.0|          new|         INDIGOTRADE|2018-01-05 19:21:16|         500|               50|Your shipment arr...|          4.5|
[2023-01-26T23:15:15.683+0000] {spark_submit.py:495} INFO - |MLA1133481354|Disco Sólido Inte...| 13799.0|          new|           DALECLÍCK|2015-03-16 12:07:23|         500|                1|You have to pay t...|          4.5|
[2023-01-26T23:15:15.683+0000] {spark_submit.py:495} INFO - |MLA1139583771|Disco Sólido Inte...|  8339.0|          new|DIAMONDSYSTEMPADUAML|2015-07-01 12:32:22|         250|              250|You have to pay t...|          4.5|
[2023-01-26T23:15:15.683+0000] {spark_submit.py:495} INFO - | MLA882277177|Capturadora Video...|  2699.0|          new|         BSK-IMPORTS|2019-12-30 23:07:04|         500|               50|You have to pay t...|          3.0|
[2023-01-26T23:15:15.683+0000] {spark_submit.py:495} INFO - |MLA1148481665|Microprocesador P...| 58899.0|          new|            MALL WEB|2016-05-26 20:28:35|         150|                1|Your shipment arr...|          4.5|
[2023-01-26T23:15:15.684+0000] {spark_submit.py:495} INFO - |MLA1228990119|Memoria Ram Fury ...| 23255.0|          new|MARSTECHSRLMARSTE...|2020-08-05 14:27:59|          50|                1|Your shipment arr...|          4.5|
[2023-01-26T23:15:15.684+0000] {spark_submit.py:495} INFO - | MLA844489972|Disco Duro Intern...| 11599.0|          new|           DALECLÍCK|2015-03-16 12:07:23|        5000|              500|You have to pay t...|          5.0|
[2023-01-26T23:15:15.684+0000] {spark_submit.py:495} INFO - |MLA1183925519|Placa De Video Nv...|299999.0|          new|ESTUDIOSMPSRLESTU...|2020-11-29 18:36:32|          50|                1|Your shipment arr...|          4.5|
[2023-01-26T23:15:15.684+0000] {spark_submit.py:495} INFO - |MLA1168362675|Memoria Ram Fury ...| 26698.0|          new|MARSTECHSRLMARSTE...|2020-08-05 14:27:59|         150|                1|Your shipment arr...|          4.5|
[2023-01-26T23:15:15.684+0000] {spark_submit.py:495} INFO - |MLA1314590445|Disco Sólido Inte...|  7999.0|          new|       ELECTRO-SHOWS|2010-03-26 01:08:16|         250|              250|You have to pay t...|          4.5|
[2023-01-26T23:15:15.684+0000] {spark_submit.py:495} INFO - |MLA1163233412|Placa De Video Nv...| 34659.0|          new|      PCREGISTRADAOK|2020-11-02 13:17:33|           1|                1|Your shipment arr...|          4.5|
[2023-01-26T23:15:15.684+0000] {spark_submit.py:495} INFO - |MLA1302136118|Disco Duro Intern...| 18271.0|          new|DIAMONDSYSTEMSANJ...|2016-04-08 17:21:06|         100|               50|Your shipment arr...|          4.5|
[2023-01-26T23:15:15.684+0000] {spark_submit.py:495} INFO - |MLA1137238635|Fuente De Aliment...|  3235.0|          new|     DIAMONDSYSTEMML|2014-04-07 22:00:36|          25|                1|You have to pay t...|          3.5|
[2023-01-26T23:15:15.684+0000] {spark_submit.py:495} INFO - +-------------+--------------------+--------+-------------+--------------------+-------------------+------------+-----------------+--------------------+-------------+
[2023-01-26T23:15:15.684+0000] {spark_submit.py:495} INFO - only showing top 20 rows
[2023-01-26T23:15:15.684+0000] {spark_submit.py:495} INFO - 
[2023-01-26T23:15:41.887+0000] {spark_submit.py:495} INFO - 23/01/26 23:15:41 ERROR Executor: Exception in task 0.0 in stage 34.0 (TID 34)
[2023-01-26T23:15:41.887+0000] {spark_submit.py:495} INFO - com.mongodb.MongoTimeoutException: Timed out after 30000 ms while waiting to connect. Client view of cluster state is {type=UNKNOWN, servers=[{address=172.22.0.2:27017, type=UNKNOWN, state=CONNECTING, exception={com.mongodb.MongoSocketOpenException: Exception opening socket}, caused by {java.net.SocketTimeoutException: connect timed out}}]
[2023-01-26T23:15:41.887+0000] {spark_submit.py:495} INFO - at com.mongodb.internal.connection.BaseCluster.getDescription(BaseCluster.java:177)
[2023-01-26T23:15:41.887+0000] {spark_submit.py:495} INFO - at com.mongodb.internal.connection.SingleServerCluster.getDescription(SingleServerCluster.java:41)
[2023-01-26T23:15:41.887+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate.getConnectedClusterDescription(MongoClientDelegate.java:147)
[2023-01-26T23:15:41.887+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate.createClientSession(MongoClientDelegate.java:98)
[2023-01-26T23:15:41.887+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.getClientSession(MongoClientDelegate.java:278)
[2023-01-26T23:15:41.887+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:202)
[2023-01-26T23:15:41.887+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:441)
[2023-01-26T23:15:41.887+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:421)
[2023-01-26T23:15:41.888+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$4(MongoSpark.scala:138)
[2023-01-26T23:15:41.888+0000] {spark_submit.py:495} INFO - at scala.collection.Iterator.foreach(Iterator.scala:943)
[2023-01-26T23:15:41.888+0000] {spark_submit.py:495} INFO - at scala.collection.Iterator.foreach$(Iterator.scala:943)
[2023-01-26T23:15:41.888+0000] {spark_submit.py:495} INFO - at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
[2023-01-26T23:15:41.888+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$3(MongoSpark.scala:122)
[2023-01-26T23:15:41.888+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$3$adapted(MongoSpark.scala:121)
[2023-01-26T23:15:41.888+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.$anonfun$withCollectionDo$1(MongoConnector.scala:186)
[2023-01-26T23:15:41.888+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.$anonfun$withDatabaseDo$1(MongoConnector.scala:171)
[2023-01-26T23:15:41.888+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withMongoClientDo(MongoConnector.scala:154)
[2023-01-26T23:15:41.888+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withDatabaseDo(MongoConnector.scala:171)
[2023-01-26T23:15:41.888+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withCollectionDo(MongoConnector.scala:184)
[2023-01-26T23:15:41.888+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$2(MongoSpark.scala:121)
[2023-01-26T23:15:41.888+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$2$adapted(MongoSpark.scala:120)
[2023-01-26T23:15:41.888+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1011)
[2023-01-26T23:15:41.889+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1011)
[2023-01-26T23:15:41.889+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2268)
[2023-01-26T23:15:41.889+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
[2023-01-26T23:15:41.889+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.Task.run(Task.scala:136)
[2023-01-26T23:15:41.889+0000] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)
[2023-01-26T23:15:41.889+0000] {spark_submit.py:495} INFO - at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)
[2023-01-26T23:15:41.889+0000] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)
[2023-01-26T23:15:41.889+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[2023-01-26T23:15:41.889+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[2023-01-26T23:15:41.889+0000] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2023-01-26T23:15:41.891+0000] {spark_submit.py:495} INFO - 23/01/26 23:15:41 ERROR TaskSetManager: Task 0 in stage 34.0 failed 1 times; aborting job
[2023-01-26T23:15:42.048+0000] {spark_submit.py:495} INFO - [[34m2023-01-26 23:15:42,048[0m] {[34mconsumerSpark.py:[0m37} ERROR[0m - Error al escribir en Mongo: An error occurred while calling o320.save.
[2023-01-26T23:15:42.049+0000] {spark_submit.py:495} INFO - : org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 34.0 failed 1 times, most recent failure: Lost task 0.0 in stage 34.0 (TID 34) (f1cbb5df32e6 executor driver): com.mongodb.MongoTimeoutException: Timed out after 30000 ms while waiting to connect. Client view of cluster state is {type=UNKNOWN, servers=[{address=172.22.0.2:27017, type=UNKNOWN, state=CONNECTING, exception={com.mongodb.MongoSocketOpenException: Exception opening socket}, caused by {java.net.SocketTimeoutException: connect timed out}}]
[2023-01-26T23:15:42.049+0000] {spark_submit.py:495} INFO - at com.mongodb.internal.connection.BaseCluster.getDescription(BaseCluster.java:177)
[2023-01-26T23:15:42.049+0000] {spark_submit.py:495} INFO - at com.mongodb.internal.connection.SingleServerCluster.getDescription(SingleServerCluster.java:41)
[2023-01-26T23:15:42.051+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate.getConnectedClusterDescription(MongoClientDelegate.java:147)
[2023-01-26T23:15:42.051+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate.createClientSession(MongoClientDelegate.java:98)
[2023-01-26T23:15:42.051+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.getClientSession(MongoClientDelegate.java:278)
[2023-01-26T23:15:42.051+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:202)
[2023-01-26T23:15:42.051+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:441)
[2023-01-26T23:15:42.051+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:421)
[2023-01-26T23:15:42.051+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$4(MongoSpark.scala:138)
[2023-01-26T23:15:42.051+0000] {spark_submit.py:495} INFO - at scala.collection.Iterator.foreach(Iterator.scala:943)
[2023-01-26T23:15:42.051+0000] {spark_submit.py:495} INFO - at scala.collection.Iterator.foreach$(Iterator.scala:943)
[2023-01-26T23:15:42.051+0000] {spark_submit.py:495} INFO - at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
[2023-01-26T23:15:42.051+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$3(MongoSpark.scala:122)
[2023-01-26T23:15:42.051+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$3$adapted(MongoSpark.scala:121)
[2023-01-26T23:15:42.052+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.$anonfun$withCollectionDo$1(MongoConnector.scala:186)
[2023-01-26T23:15:42.052+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.$anonfun$withDatabaseDo$1(MongoConnector.scala:171)
[2023-01-26T23:15:42.052+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withMongoClientDo(MongoConnector.scala:154)
[2023-01-26T23:15:42.052+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withDatabaseDo(MongoConnector.scala:171)
[2023-01-26T23:15:42.052+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withCollectionDo(MongoConnector.scala:184)
[2023-01-26T23:15:42.052+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$2(MongoSpark.scala:121)
[2023-01-26T23:15:42.055+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$2$adapted(MongoSpark.scala:120)
[2023-01-26T23:15:42.055+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1011)
[2023-01-26T23:15:42.056+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1011)
[2023-01-26T23:15:42.056+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2268)
[2023-01-26T23:15:42.056+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
[2023-01-26T23:15:42.056+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.Task.run(Task.scala:136)
[2023-01-26T23:15:42.056+0000] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)
[2023-01-26T23:15:42.056+0000] {spark_submit.py:495} INFO - at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)
[2023-01-26T23:15:42.056+0000] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)
[2023-01-26T23:15:42.056+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[2023-01-26T23:15:42.056+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[2023-01-26T23:15:42.056+0000] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2023-01-26T23:15:42.057+0000] {spark_submit.py:495} INFO - 
[2023-01-26T23:15:42.057+0000] {spark_submit.py:495} INFO - Driver stacktrace:
[2023-01-26T23:15:42.057+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)
[2023-01-26T23:15:42.057+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)
[2023-01-26T23:15:42.057+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)
[2023-01-26T23:15:42.058+0000] {spark_submit.py:495} INFO - at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
[2023-01-26T23:15:42.058+0000] {spark_submit.py:495} INFO - at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
[2023-01-26T23:15:42.058+0000] {spark_submit.py:495} INFO - at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
[2023-01-26T23:15:42.058+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)
[2023-01-26T23:15:42.058+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)
[2023-01-26T23:15:42.058+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)
[2023-01-26T23:15:42.058+0000] {spark_submit.py:495} INFO - at scala.Option.foreach(Option.scala:407)
[2023-01-26T23:15:42.058+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)
[2023-01-26T23:15:42.058+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)
[2023-01-26T23:15:42.058+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)
[2023-01-26T23:15:42.058+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)
[2023-01-26T23:15:42.059+0000] {spark_submit.py:495} INFO - at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
[2023-01-26T23:15:42.059+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)
[2023-01-26T23:15:42.059+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.runJob(SparkContext.scala:2228)
[2023-01-26T23:15:42.059+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.runJob(SparkContext.scala:2249)
[2023-01-26T23:15:42.059+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.runJob(SparkContext.scala:2268)
[2023-01-26T23:15:42.059+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.runJob(SparkContext.scala:2293)
[2023-01-26T23:15:42.059+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$1(RDD.scala:1011)
[2023-01-26T23:15:42.059+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
[2023-01-26T23:15:42.059+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
[2023-01-26T23:15:42.059+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.withScope(RDD.scala:406)
[2023-01-26T23:15:42.059+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.foreachPartition(RDD.scala:1009)
[2023-01-26T23:15:42.059+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.save(MongoSpark.scala:120)
[2023-01-26T23:15:42.059+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.save(MongoSpark.scala:169)
[2023-01-26T23:15:42.060+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.sql.DefaultSource.createRelation(DefaultSource.scala:70)
[2023-01-26T23:15:42.060+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:47)
[2023-01-26T23:15:42.060+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)
[2023-01-26T23:15:42.060+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)
[2023-01-26T23:15:42.060+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)
[2023-01-26T23:15:42.060+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)
[2023-01-26T23:15:42.060+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)
[2023-01-26T23:15:42.060+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)
[2023-01-26T23:15:42.060+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)
[2023-01-26T23:15:42.060+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
[2023-01-26T23:15:42.060+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
[2023-01-26T23:15:42.060+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)
[2023-01-26T23:15:42.060+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)
[2023-01-26T23:15:42.060+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:584)
[2023-01-26T23:15:42.061+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:176)
[2023-01-26T23:15:42.061+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:584)
[2023-01-26T23:15:42.061+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)
[2023-01-26T23:15:42.061+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
[2023-01-26T23:15:42.061+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
[2023-01-26T23:15:42.061+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
[2023-01-26T23:15:42.061+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
[2023-01-26T23:15:42.061+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:560)
[2023-01-26T23:15:42.061+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)
[2023-01-26T23:15:42.061+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)
[2023-01-26T23:15:42.061+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)
[2023-01-26T23:15:42.061+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:116)
[2023-01-26T23:15:42.061+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:860)
[2023-01-26T23:15:42.061+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:390)
[2023-01-26T23:15:42.061+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:363)
[2023-01-26T23:15:42.062+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:247)
[2023-01-26T23:15:42.062+0000] {spark_submit.py:495} INFO - at jdk.internal.reflect.GeneratedMethodAccessor69.invoke(Unknown Source)
[2023-01-26T23:15:42.062+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
[2023-01-26T23:15:42.062+0000] {spark_submit.py:495} INFO - at java.base/java.lang.reflect.Method.invoke(Method.java:566)
[2023-01-26T23:15:42.062+0000] {spark_submit.py:495} INFO - at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
[2023-01-26T23:15:42.062+0000] {spark_submit.py:495} INFO - at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
[2023-01-26T23:15:42.062+0000] {spark_submit.py:495} INFO - at py4j.Gateway.invoke(Gateway.java:282)
[2023-01-26T23:15:42.062+0000] {spark_submit.py:495} INFO - at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
[2023-01-26T23:15:42.062+0000] {spark_submit.py:495} INFO - at py4j.commands.CallCommand.execute(CallCommand.java:79)
[2023-01-26T23:15:42.062+0000] {spark_submit.py:495} INFO - at py4j.ClientServerConnection.sendCommand(ClientServerConnection.java:244)
[2023-01-26T23:15:42.062+0000] {spark_submit.py:495} INFO - at py4j.CallbackClient.sendCommand(CallbackClient.java:384)
[2023-01-26T23:15:42.062+0000] {spark_submit.py:495} INFO - at py4j.CallbackClient.sendCommand(CallbackClient.java:356)
[2023-01-26T23:15:42.062+0000] {spark_submit.py:495} INFO - at py4j.reflection.PythonProxyHandler.invoke(PythonProxyHandler.java:106)
[2023-01-26T23:15:42.062+0000] {spark_submit.py:495} INFO - at com.sun.proxy.$Proxy32.call(Unknown Source)
[2023-01-26T23:15:42.062+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1(ForeachBatchSink.scala:51)
[2023-01-26T23:15:42.063+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1$adapted(ForeachBatchSink.scala:51)
[2023-01-26T23:15:42.063+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.sources.ForeachBatchSink.addBatch(ForeachBatchSink.scala:32)
[2023-01-26T23:15:42.063+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:666)
[2023-01-26T23:15:42.063+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)
[2023-01-26T23:15:42.063+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)
[2023-01-26T23:15:42.063+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)
[2023-01-26T23:15:42.063+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
[2023-01-26T23:15:42.063+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
[2023-01-26T23:15:42.063+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:664)
[2023-01-26T23:15:42.063+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:375)
[2023-01-26T23:15:42.063+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:373)
[2023-01-26T23:15:42.063+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:68)
[2023-01-26T23:15:42.063+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:664)
[2023-01-26T23:15:42.063+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:256)
[2023-01-26T23:15:42.063+0000] {spark_submit.py:495} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2023-01-26T23:15:42.064+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:375)
[2023-01-26T23:15:42.064+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:373)
[2023-01-26T23:15:42.064+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:68)
[2023-01-26T23:15:42.064+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:219)
[2023-01-26T23:15:42.064+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)
[2023-01-26T23:15:42.064+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:213)
[2023-01-26T23:15:42.064+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:307)
[2023-01-26T23:15:42.064+0000] {spark_submit.py:495} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2023-01-26T23:15:42.064+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
[2023-01-26T23:15:42.064+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:285)
[2023-01-26T23:15:42.064+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:208)
[2023-01-26T23:15:42.064+0000] {spark_submit.py:495} INFO - Caused by: com.mongodb.MongoTimeoutException: Timed out after 30000 ms while waiting to connect. Client view of cluster state is {type=UNKNOWN, servers=[{address=172.22.0.2:27017, type=UNKNOWN, state=CONNECTING, exception={com.mongodb.MongoSocketOpenException: Exception opening socket}, caused by {java.net.SocketTimeoutException: connect timed out}}]
[2023-01-26T23:15:42.064+0000] {spark_submit.py:495} INFO - at com.mongodb.internal.connection.BaseCluster.getDescription(BaseCluster.java:177)
[2023-01-26T23:15:42.064+0000] {spark_submit.py:495} INFO - at com.mongodb.internal.connection.SingleServerCluster.getDescription(SingleServerCluster.java:41)
[2023-01-26T23:15:42.065+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate.getConnectedClusterDescription(MongoClientDelegate.java:147)
[2023-01-26T23:15:42.065+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate.createClientSession(MongoClientDelegate.java:98)
[2023-01-26T23:15:42.065+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.getClientSession(MongoClientDelegate.java:278)
[2023-01-26T23:15:42.065+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:202)
[2023-01-26T23:15:42.065+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:441)
[2023-01-26T23:15:42.065+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:421)
[2023-01-26T23:15:42.065+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$4(MongoSpark.scala:138)
[2023-01-26T23:15:42.065+0000] {spark_submit.py:495} INFO - at scala.collection.Iterator.foreach(Iterator.scala:943)
[2023-01-26T23:15:42.065+0000] {spark_submit.py:495} INFO - at scala.collection.Iterator.foreach$(Iterator.scala:943)
[2023-01-26T23:15:42.065+0000] {spark_submit.py:495} INFO - at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
[2023-01-26T23:15:42.065+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$3(MongoSpark.scala:122)
[2023-01-26T23:15:42.065+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$3$adapted(MongoSpark.scala:121)
[2023-01-26T23:15:42.066+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.$anonfun$withCollectionDo$1(MongoConnector.scala:186)
[2023-01-26T23:15:42.066+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.$anonfun$withDatabaseDo$1(MongoConnector.scala:171)
[2023-01-26T23:15:42.066+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withMongoClientDo(MongoConnector.scala:154)
[2023-01-26T23:15:42.066+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withDatabaseDo(MongoConnector.scala:171)
[2023-01-26T23:15:42.066+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withCollectionDo(MongoConnector.scala:184)
[2023-01-26T23:15:42.066+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$2(MongoSpark.scala:121)
[2023-01-26T23:15:42.067+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$2$adapted(MongoSpark.scala:120)
[2023-01-26T23:15:42.067+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1011)
[2023-01-26T23:15:42.067+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1011)
[2023-01-26T23:15:42.067+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2268)
[2023-01-26T23:15:42.067+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
[2023-01-26T23:15:42.067+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.Task.run(Task.scala:136)
[2023-01-26T23:15:42.067+0000] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)
[2023-01-26T23:15:42.067+0000] {spark_submit.py:495} INFO - at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)
[2023-01-26T23:15:42.067+0000] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)
[2023-01-26T23:15:42.067+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[2023-01-26T23:15:42.068+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[2023-01-26T23:15:42.068+0000] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2023-01-26T23:15:42.068+0000] {spark_submit.py:495} INFO - [0m
[2023-01-26T23:15:42.128+0000] {spark_submit.py:495} INFO - [[34m2023-01-26 23:15:42,127[0m] {[34mclientserver.py:[0m577} INFO[0m - Received command c on object id p0[0m
[2023-01-26T23:16:12.181+0000] {spark_submit.py:495} INFO - 23/01/26 23:16:12 ERROR Executor: Exception in task 0.0 in stage 36.0 (TID 36)
[2023-01-26T23:16:12.182+0000] {spark_submit.py:495} INFO - com.mongodb.MongoTimeoutException: Timed out after 30000 ms while waiting to connect. Client view of cluster state is {type=UNKNOWN, servers=[{address=172.22.0.2:27017, type=UNKNOWN, state=CONNECTING, exception={com.mongodb.MongoSocketOpenException: Exception opening socket}, caused by {java.net.SocketTimeoutException: connect timed out}}]
[2023-01-26T23:16:12.182+0000] {spark_submit.py:495} INFO - at com.mongodb.internal.connection.BaseCluster.getDescription(BaseCluster.java:177)
[2023-01-26T23:16:12.182+0000] {spark_submit.py:495} INFO - at com.mongodb.internal.connection.SingleServerCluster.getDescription(SingleServerCluster.java:41)
[2023-01-26T23:16:12.183+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate.getConnectedClusterDescription(MongoClientDelegate.java:147)
[2023-01-26T23:16:12.183+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate.createClientSession(MongoClientDelegate.java:98)
[2023-01-26T23:16:12.183+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.getClientSession(MongoClientDelegate.java:278)
[2023-01-26T23:16:12.183+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:202)
[2023-01-26T23:16:12.183+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:441)
[2023-01-26T23:16:12.183+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:421)
[2023-01-26T23:16:12.183+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$4(MongoSpark.scala:138)
[2023-01-26T23:16:12.183+0000] {spark_submit.py:495} INFO - at scala.collection.Iterator.foreach(Iterator.scala:943)
[2023-01-26T23:16:12.183+0000] {spark_submit.py:495} INFO - at scala.collection.Iterator.foreach$(Iterator.scala:943)
[2023-01-26T23:16:12.183+0000] {spark_submit.py:495} INFO - at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
[2023-01-26T23:16:12.183+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$3(MongoSpark.scala:122)
[2023-01-26T23:16:12.183+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$3$adapted(MongoSpark.scala:121)
[2023-01-26T23:16:12.183+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.$anonfun$withCollectionDo$1(MongoConnector.scala:186)
[2023-01-26T23:16:12.184+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.$anonfun$withDatabaseDo$1(MongoConnector.scala:171)
[2023-01-26T23:16:12.184+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withMongoClientDo(MongoConnector.scala:154)
[2023-01-26T23:16:12.184+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withDatabaseDo(MongoConnector.scala:171)
[2023-01-26T23:16:12.184+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withCollectionDo(MongoConnector.scala:184)
[2023-01-26T23:16:12.184+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$2(MongoSpark.scala:121)
[2023-01-26T23:16:12.184+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$2$adapted(MongoSpark.scala:120)
[2023-01-26T23:16:12.184+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1011)
[2023-01-26T23:16:12.184+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1011)
[2023-01-26T23:16:12.184+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2268)
[2023-01-26T23:16:12.184+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
[2023-01-26T23:16:12.184+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.Task.run(Task.scala:136)
[2023-01-26T23:16:12.184+0000] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)
[2023-01-26T23:16:12.184+0000] {spark_submit.py:495} INFO - at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)
[2023-01-26T23:16:12.184+0000] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)
[2023-01-26T23:16:12.184+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[2023-01-26T23:16:12.185+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[2023-01-26T23:16:12.185+0000] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2023-01-26T23:16:12.186+0000] {spark_submit.py:495} INFO - 23/01/26 23:16:12 ERROR TaskSetManager: Task 0 in stage 36.0 failed 1 times; aborting job
[2023-01-26T23:16:12.308+0000] {spark_submit.py:495} INFO - [[34m2023-01-26 23:16:12,308[0m] {[34mconsumerSpark.py:[0m37} ERROR[0m - Error al escribir en Mongo: An error occurred while calling o329.save.
[2023-01-26T23:16:12.309+0000] {spark_submit.py:495} INFO - : org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 36.0 failed 1 times, most recent failure: Lost task 0.0 in stage 36.0 (TID 36) (f1cbb5df32e6 executor driver): com.mongodb.MongoTimeoutException: Timed out after 30000 ms while waiting to connect. Client view of cluster state is {type=UNKNOWN, servers=[{address=172.22.0.2:27017, type=UNKNOWN, state=CONNECTING, exception={com.mongodb.MongoSocketOpenException: Exception opening socket}, caused by {java.net.SocketTimeoutException: connect timed out}}]
[2023-01-26T23:16:12.309+0000] {spark_submit.py:495} INFO - at com.mongodb.internal.connection.BaseCluster.getDescription(BaseCluster.java:177)
[2023-01-26T23:16:12.309+0000] {spark_submit.py:495} INFO - at com.mongodb.internal.connection.SingleServerCluster.getDescription(SingleServerCluster.java:41)
[2023-01-26T23:16:12.309+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate.getConnectedClusterDescription(MongoClientDelegate.java:147)
[2023-01-26T23:16:12.309+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate.createClientSession(MongoClientDelegate.java:98)
[2023-01-26T23:16:12.309+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.getClientSession(MongoClientDelegate.java:278)
[2023-01-26T23:16:12.309+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:202)
[2023-01-26T23:16:12.309+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:441)
[2023-01-26T23:16:12.309+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:421)
[2023-01-26T23:16:12.309+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$4(MongoSpark.scala:138)
[2023-01-26T23:16:12.309+0000] {spark_submit.py:495} INFO - at scala.collection.Iterator.foreach(Iterator.scala:943)
[2023-01-26T23:16:12.309+0000] {spark_submit.py:495} INFO - at scala.collection.Iterator.foreach$(Iterator.scala:943)
[2023-01-26T23:16:12.310+0000] {spark_submit.py:495} INFO - at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
[2023-01-26T23:16:12.310+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$3(MongoSpark.scala:122)
[2023-01-26T23:16:12.310+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$3$adapted(MongoSpark.scala:121)
[2023-01-26T23:16:12.310+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.$anonfun$withCollectionDo$1(MongoConnector.scala:186)
[2023-01-26T23:16:12.310+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.$anonfun$withDatabaseDo$1(MongoConnector.scala:171)
[2023-01-26T23:16:12.310+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withMongoClientDo(MongoConnector.scala:154)
[2023-01-26T23:16:12.310+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withDatabaseDo(MongoConnector.scala:171)
[2023-01-26T23:16:12.310+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withCollectionDo(MongoConnector.scala:184)
[2023-01-26T23:16:12.310+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$2(MongoSpark.scala:121)
[2023-01-26T23:16:12.310+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$2$adapted(MongoSpark.scala:120)
[2023-01-26T23:16:12.310+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1011)
[2023-01-26T23:16:12.310+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1011)
[2023-01-26T23:16:12.310+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2268)
[2023-01-26T23:16:12.310+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
[2023-01-26T23:16:12.310+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.Task.run(Task.scala:136)
[2023-01-26T23:16:12.311+0000] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)
[2023-01-26T23:16:12.311+0000] {spark_submit.py:495} INFO - at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)
[2023-01-26T23:16:12.311+0000] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)
[2023-01-26T23:16:12.311+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[2023-01-26T23:16:12.311+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[2023-01-26T23:16:12.311+0000] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2023-01-26T23:16:12.311+0000] {spark_submit.py:495} INFO - 
[2023-01-26T23:16:12.311+0000] {spark_submit.py:495} INFO - Driver stacktrace:
[2023-01-26T23:16:12.311+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)
[2023-01-26T23:16:12.311+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)
[2023-01-26T23:16:12.311+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)
[2023-01-26T23:16:12.311+0000] {spark_submit.py:495} INFO - at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
[2023-01-26T23:16:12.311+0000] {spark_submit.py:495} INFO - at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
[2023-01-26T23:16:12.311+0000] {spark_submit.py:495} INFO - at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
[2023-01-26T23:16:12.312+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)
[2023-01-26T23:16:12.312+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)
[2023-01-26T23:16:12.312+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)
[2023-01-26T23:16:12.312+0000] {spark_submit.py:495} INFO - at scala.Option.foreach(Option.scala:407)
[2023-01-26T23:16:12.312+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)
[2023-01-26T23:16:12.312+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)
[2023-01-26T23:16:12.312+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)
[2023-01-26T23:16:12.312+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)
[2023-01-26T23:16:12.312+0000] {spark_submit.py:495} INFO - at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
[2023-01-26T23:16:12.312+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)
[2023-01-26T23:16:12.312+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.runJob(SparkContext.scala:2228)
[2023-01-26T23:16:12.312+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.runJob(SparkContext.scala:2249)
[2023-01-26T23:16:12.312+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.runJob(SparkContext.scala:2268)
[2023-01-26T23:16:12.312+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.runJob(SparkContext.scala:2293)
[2023-01-26T23:16:12.313+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$1(RDD.scala:1011)
[2023-01-26T23:16:12.313+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
[2023-01-26T23:16:12.313+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
[2023-01-26T23:16:12.313+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.withScope(RDD.scala:406)
[2023-01-26T23:16:12.313+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.foreachPartition(RDD.scala:1009)
[2023-01-26T23:16:12.313+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.save(MongoSpark.scala:120)
[2023-01-26T23:16:12.313+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.save(MongoSpark.scala:169)
[2023-01-26T23:16:12.313+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.sql.DefaultSource.createRelation(DefaultSource.scala:70)
[2023-01-26T23:16:12.313+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:47)
[2023-01-26T23:16:12.313+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)
[2023-01-26T23:16:12.313+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)
[2023-01-26T23:16:12.313+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)
[2023-01-26T23:16:12.313+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)
[2023-01-26T23:16:12.313+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)
[2023-01-26T23:16:12.313+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)
[2023-01-26T23:16:12.314+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)
[2023-01-26T23:16:12.314+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
[2023-01-26T23:16:12.314+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
[2023-01-26T23:16:12.314+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)
[2023-01-26T23:16:12.314+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)
[2023-01-26T23:16:12.314+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:584)
[2023-01-26T23:16:12.314+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:176)
[2023-01-26T23:16:12.314+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:584)
[2023-01-26T23:16:12.314+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)
[2023-01-26T23:16:12.314+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
[2023-01-26T23:16:12.314+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
[2023-01-26T23:16:12.314+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
[2023-01-26T23:16:12.314+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
[2023-01-26T23:16:12.314+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:560)
[2023-01-26T23:16:12.315+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)
[2023-01-26T23:16:12.315+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)
[2023-01-26T23:16:12.315+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)
[2023-01-26T23:16:12.315+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:116)
[2023-01-26T23:16:12.315+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:860)
[2023-01-26T23:16:12.315+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:390)
[2023-01-26T23:16:12.315+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:363)
[2023-01-26T23:16:12.315+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:247)
[2023-01-26T23:16:12.315+0000] {spark_submit.py:495} INFO - at jdk.internal.reflect.GeneratedMethodAccessor69.invoke(Unknown Source)
[2023-01-26T23:16:12.316+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
[2023-01-26T23:16:12.316+0000] {spark_submit.py:495} INFO - at java.base/java.lang.reflect.Method.invoke(Method.java:566)
[2023-01-26T23:16:12.316+0000] {spark_submit.py:495} INFO - at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
[2023-01-26T23:16:12.316+0000] {spark_submit.py:495} INFO - at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
[2023-01-26T23:16:12.316+0000] {spark_submit.py:495} INFO - at py4j.Gateway.invoke(Gateway.java:282)
[2023-01-26T23:16:12.316+0000] {spark_submit.py:495} INFO - at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
[2023-01-26T23:16:12.316+0000] {spark_submit.py:495} INFO - at py4j.commands.CallCommand.execute(CallCommand.java:79)
[2023-01-26T23:16:12.316+0000] {spark_submit.py:495} INFO - at py4j.ClientServerConnection.sendCommand(ClientServerConnection.java:244)
[2023-01-26T23:16:12.316+0000] {spark_submit.py:495} INFO - at py4j.CallbackClient.sendCommand(CallbackClient.java:384)
[2023-01-26T23:16:12.317+0000] {spark_submit.py:495} INFO - at py4j.CallbackClient.sendCommand(CallbackClient.java:356)
[2023-01-26T23:16:12.317+0000] {spark_submit.py:495} INFO - at py4j.reflection.PythonProxyHandler.invoke(PythonProxyHandler.java:106)
[2023-01-26T23:16:12.317+0000] {spark_submit.py:495} INFO - at com.sun.proxy.$Proxy32.call(Unknown Source)
[2023-01-26T23:16:12.317+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1(ForeachBatchSink.scala:51)
[2023-01-26T23:16:12.317+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1$adapted(ForeachBatchSink.scala:51)
[2023-01-26T23:16:12.317+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.sources.ForeachBatchSink.addBatch(ForeachBatchSink.scala:32)
[2023-01-26T23:16:12.317+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:666)
[2023-01-26T23:16:12.317+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)
[2023-01-26T23:16:12.317+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)
[2023-01-26T23:16:12.317+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)
[2023-01-26T23:16:12.317+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
[2023-01-26T23:16:12.317+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
[2023-01-26T23:16:12.317+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:664)
[2023-01-26T23:16:12.317+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:375)
[2023-01-26T23:16:12.317+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:373)
[2023-01-26T23:16:12.317+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:68)
[2023-01-26T23:16:12.318+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:664)
[2023-01-26T23:16:12.318+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:256)
[2023-01-26T23:16:12.318+0000] {spark_submit.py:495} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2023-01-26T23:16:12.318+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:375)
[2023-01-26T23:16:12.318+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:373)
[2023-01-26T23:16:12.318+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:68)
[2023-01-26T23:16:12.318+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:219)
[2023-01-26T23:16:12.318+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)
[2023-01-26T23:16:12.318+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:213)
[2023-01-26T23:16:12.318+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:307)
[2023-01-26T23:16:12.318+0000] {spark_submit.py:495} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2023-01-26T23:16:12.318+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
[2023-01-26T23:16:12.318+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:285)
[2023-01-26T23:16:12.318+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:208)
[2023-01-26T23:16:12.318+0000] {spark_submit.py:495} INFO - Caused by: com.mongodb.MongoTimeoutException: Timed out after 30000 ms while waiting to connect. Client view of cluster state is {type=UNKNOWN, servers=[{address=172.22.0.2:27017, type=UNKNOWN, state=CONNECTING, exception={com.mongodb.MongoSocketOpenException: Exception opening socket}, caused by {java.net.SocketTimeoutException: connect timed out}}]
[2023-01-26T23:16:12.319+0000] {spark_submit.py:495} INFO - at com.mongodb.internal.connection.BaseCluster.getDescription(BaseCluster.java:177)
[2023-01-26T23:16:12.319+0000] {spark_submit.py:495} INFO - at com.mongodb.internal.connection.SingleServerCluster.getDescription(SingleServerCluster.java:41)
[2023-01-26T23:16:12.319+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate.getConnectedClusterDescription(MongoClientDelegate.java:147)
[2023-01-26T23:16:12.319+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate.createClientSession(MongoClientDelegate.java:98)
[2023-01-26T23:16:12.319+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.getClientSession(MongoClientDelegate.java:278)
[2023-01-26T23:16:12.319+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:202)
[2023-01-26T23:16:12.319+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:441)
[2023-01-26T23:16:12.319+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:421)
[2023-01-26T23:16:12.319+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$4(MongoSpark.scala:138)
[2023-01-26T23:16:12.319+0000] {spark_submit.py:495} INFO - at scala.collection.Iterator.foreach(Iterator.scala:943)
[2023-01-26T23:16:12.319+0000] {spark_submit.py:495} INFO - at scala.collection.Iterator.foreach$(Iterator.scala:943)
[2023-01-26T23:16:12.319+0000] {spark_submit.py:495} INFO - at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
[2023-01-26T23:16:12.319+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$3(MongoSpark.scala:122)
[2023-01-26T23:16:12.319+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$3$adapted(MongoSpark.scala:121)
[2023-01-26T23:16:12.319+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.$anonfun$withCollectionDo$1(MongoConnector.scala:186)
[2023-01-26T23:16:12.320+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.$anonfun$withDatabaseDo$1(MongoConnector.scala:171)
[2023-01-26T23:16:12.320+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withMongoClientDo(MongoConnector.scala:154)
[2023-01-26T23:16:12.320+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withDatabaseDo(MongoConnector.scala:171)
[2023-01-26T23:16:12.320+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withCollectionDo(MongoConnector.scala:184)
[2023-01-26T23:16:12.320+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$2(MongoSpark.scala:121)
[2023-01-26T23:16:12.320+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$2$adapted(MongoSpark.scala:120)
[2023-01-26T23:16:12.320+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1011)
[2023-01-26T23:16:12.320+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1011)
[2023-01-26T23:16:12.320+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2268)
[2023-01-26T23:16:12.320+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
[2023-01-26T23:16:12.320+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.Task.run(Task.scala:136)
[2023-01-26T23:16:12.320+0000] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)
[2023-01-26T23:16:12.320+0000] {spark_submit.py:495} INFO - at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)
[2023-01-26T23:16:12.320+0000] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)
[2023-01-26T23:16:12.320+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[2023-01-26T23:16:12.321+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[2023-01-26T23:16:12.321+0000] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2023-01-26T23:16:12.321+0000] {spark_submit.py:495} INFO - [0m
[2023-01-26T23:26:19.233+0000] {spark_submit.py:495} INFO - [[34m2023-01-26 23:26:19,233[0m] {[34mclientserver.py:[0m577} INFO[0m - Received command c on object id p0[0m
[2023-01-26T23:26:21.250+0000] {spark_submit.py:495} INFO - -------------------------------------------
[2023-01-26T23:26:21.250+0000] {spark_submit.py:495} INFO - Batch: 13
[2023-01-26T23:26:21.250+0000] {spark_submit.py:495} INFO - -------------------------------------------
[2023-01-26T23:26:21.363+0000] {spark_submit.py:495} INFO - +-------------+--------------------+--------+-------------+--------------------+-------------------+------------+-----------------+--------------------+-------------+
[2023-01-26T23:26:21.364+0000] {spark_submit.py:495} INFO - |           id|       nameOfProduct|   price|ProductStatus|          SellerName|   RegistrationDate|QuantitySold|QuantityAvailable|      Free shipping?|Store ratings|
[2023-01-26T23:26:21.364+0000] {spark_submit.py:495} INFO - +-------------+--------------------+--------+-------------+--------------------+-------------------+------------+-----------------+--------------------+-------------+
[2023-01-26T23:26:21.364+0000] {spark_submit.py:495} INFO - |MLA1220964588|Procesador Gamer ...| 45899.0|          new|    OVERHARD DIGITAL|2015-01-08 12:19:41|          50|                1|Your shipment arr...|          4.5|
[2023-01-26T23:26:21.364+0000] {spark_submit.py:495} INFO - |MLA1157512489|Placa De Video Nv...|124999.0|          new|      PCREGISTRADAOK|2020-11-02 13:17:33|          50|                1|Your shipment arr...|          4.5|
[2023-01-26T23:26:21.364+0000] {spark_submit.py:495} INFO - |MLA1221192372|Disco Duro Extern...| 26299.0|          new|    OVERHARD DIGITAL|2015-01-08 12:19:41|         250|              100|Your shipment arr...|          4.5|
[2023-01-26T23:26:21.364+0000] {spark_submit.py:495} INFO - |MLA1163238194|Placa De Video Am...| 33999.0|          new|       MICYBERCOMPRA|2011-09-13 13:15:53|          50|                1|Your shipment arr...|          4.5|
[2023-01-26T23:26:21.364+0000] {spark_submit.py:495} INFO - |MLA1160204806|Memoria Ram Fury ...| 15499.0|          new|    OVERHARD DIGITAL|2015-01-08 12:19:41|         500|               50|Your shipment arr...|          4.5|
[2023-01-26T23:26:21.364+0000] {spark_submit.py:495} INFO - |MLA1115295971|Procesador Amd Ry...| 93999.0|          new|      PCREGISTRADAOK|2020-11-02 13:17:33|          25|                1|Your shipment arr...|          4.5|
[2023-01-26T23:26:21.364+0000] {spark_submit.py:495} INFO - |MLA1318244331|Disco Solido Ssd ...| 28799.0|          new|    OVERHARD DIGITAL|2015-01-08 12:19:41|           5|              150|Your shipment arr...|          4.5|
[2023-01-26T23:26:21.364+0000] {spark_submit.py:495} INFO - | MLA923637669|Placa De Video Ge...| 28699.0|          new|         INDIGOTRADE|2018-01-05 19:21:16|         500|               50|Your shipment arr...|          4.5|
[2023-01-26T23:26:21.364+0000] {spark_submit.py:495} INFO - |MLA1133481354|Disco Sólido Inte...| 13799.0|          new|           DALECLÍCK|2015-03-16 12:07:23|         500|                1|You have to pay t...|          4.5|
[2023-01-26T23:26:21.364+0000] {spark_submit.py:495} INFO - |MLA1139583771|Disco Sólido Inte...|  8339.0|          new|DIAMONDSYSTEMPADUAML|2015-07-01 12:32:22|         250|              250|You have to pay t...|          4.5|
[2023-01-26T23:26:21.365+0000] {spark_submit.py:495} INFO - | MLA882277177|Capturadora Video...|  2699.0|          new|         BSK-IMPORTS|2019-12-30 23:07:04|         500|               50|You have to pay t...|          3.0|
[2023-01-26T23:26:21.365+0000] {spark_submit.py:495} INFO - |MLA1148481665|Microprocesador P...| 58899.0|          new|            MALL WEB|2016-05-26 20:28:35|         150|                1|Your shipment arr...|          4.5|
[2023-01-26T23:26:21.365+0000] {spark_submit.py:495} INFO - |MLA1228990119|Memoria Ram Fury ...| 23255.0|          new|MARSTECHSRLMARSTE...|2020-08-05 14:27:59|          50|                1|Your shipment arr...|          4.5|
[2023-01-26T23:26:21.365+0000] {spark_submit.py:495} INFO - | MLA844489972|Disco Duro Intern...| 11599.0|          new|           DALECLÍCK|2015-03-16 12:07:23|        5000|              500|You have to pay t...|          5.0|
[2023-01-26T23:26:21.365+0000] {spark_submit.py:495} INFO - |MLA1183925519|Placa De Video Nv...|299999.0|          new|ESTUDIOSMPSRLESTU...|2020-11-29 18:36:32|          50|                1|Your shipment arr...|          4.5|
[2023-01-26T23:26:21.365+0000] {spark_submit.py:495} INFO - |MLA1168362675|Memoria Ram Fury ...| 26698.0|          new|MARSTECHSRLMARSTE...|2020-08-05 14:27:59|         150|                1|Your shipment arr...|          4.5|
[2023-01-26T23:26:21.365+0000] {spark_submit.py:495} INFO - |MLA1314590445|Disco Sólido Inte...|  7999.0|          new|       ELECTRO-SHOWS|2010-03-26 01:08:16|         250|              250|You have to pay t...|          4.5|
[2023-01-26T23:26:21.365+0000] {spark_submit.py:495} INFO - |MLA1163233412|Placa De Video Nv...| 34659.0|          new|      PCREGISTRADAOK|2020-11-02 13:17:33|           1|                1|Your shipment arr...|          4.5|
[2023-01-26T23:26:21.365+0000] {spark_submit.py:495} INFO - |MLA1302136118|Disco Duro Intern...| 18271.0|          new|DIAMONDSYSTEMSANJ...|2016-04-08 17:21:06|         100|               50|Your shipment arr...|          4.5|
[2023-01-26T23:26:21.365+0000] {spark_submit.py:495} INFO - |MLA1137238635|Fuente De Aliment...|  3235.0|          new|     DIAMONDSYSTEMML|2014-04-07 22:00:36|          25|                1|You have to pay t...|          3.5|
[2023-01-26T23:26:21.365+0000] {spark_submit.py:495} INFO - +-------------+--------------------+--------+-------------+--------------------+-------------------+------------+-----------------+--------------------+-------------+
[2023-01-26T23:26:21.365+0000] {spark_submit.py:495} INFO - only showing top 20 rows
[2023-01-26T23:26:21.366+0000] {spark_submit.py:495} INFO - 
[2023-01-26T23:26:50.091+0000] {spark_submit.py:495} INFO - 23/01/26 23:26:50 ERROR Executor: Exception in task 0.0 in stage 37.0 (TID 37)
[2023-01-26T23:26:50.091+0000] {spark_submit.py:495} INFO - com.mongodb.MongoTimeoutException: Timed out after 30000 ms while waiting to connect. Client view of cluster state is {type=UNKNOWN, servers=[{address=172.22.0.2:27017, type=UNKNOWN, state=CONNECTING, exception={com.mongodb.MongoSocketOpenException: Exception opening socket}, caused by {java.net.SocketTimeoutException: connect timed out}}]
[2023-01-26T23:26:50.092+0000] {spark_submit.py:495} INFO - at com.mongodb.internal.connection.BaseCluster.getDescription(BaseCluster.java:177)
[2023-01-26T23:26:50.092+0000] {spark_submit.py:495} INFO - at com.mongodb.internal.connection.SingleServerCluster.getDescription(SingleServerCluster.java:41)
[2023-01-26T23:26:50.092+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate.getConnectedClusterDescription(MongoClientDelegate.java:147)
[2023-01-26T23:26:50.093+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate.createClientSession(MongoClientDelegate.java:98)
[2023-01-26T23:26:50.093+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.getClientSession(MongoClientDelegate.java:278)
[2023-01-26T23:26:50.093+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:202)
[2023-01-26T23:26:50.094+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:441)
[2023-01-26T23:26:50.094+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:421)
[2023-01-26T23:26:50.094+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$4(MongoSpark.scala:138)
[2023-01-26T23:26:50.095+0000] {spark_submit.py:495} INFO - at scala.collection.Iterator.foreach(Iterator.scala:943)
[2023-01-26T23:26:50.095+0000] {spark_submit.py:495} INFO - at scala.collection.Iterator.foreach$(Iterator.scala:943)
[2023-01-26T23:26:50.095+0000] {spark_submit.py:495} INFO - at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
[2023-01-26T23:26:50.096+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$3(MongoSpark.scala:122)
[2023-01-26T23:26:50.096+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$3$adapted(MongoSpark.scala:121)
[2023-01-26T23:26:50.097+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.$anonfun$withCollectionDo$1(MongoConnector.scala:186)
[2023-01-26T23:26:50.097+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.$anonfun$withDatabaseDo$1(MongoConnector.scala:171)
[2023-01-26T23:26:50.097+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withMongoClientDo(MongoConnector.scala:154)
[2023-01-26T23:26:50.098+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withDatabaseDo(MongoConnector.scala:171)
[2023-01-26T23:26:50.098+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withCollectionDo(MongoConnector.scala:184)
[2023-01-26T23:26:50.098+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$2(MongoSpark.scala:121)
[2023-01-26T23:26:50.099+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$2$adapted(MongoSpark.scala:120)
[2023-01-26T23:26:50.099+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1011)
[2023-01-26T23:26:50.099+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1011)
[2023-01-26T23:26:50.100+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2268)
[2023-01-26T23:26:50.100+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
[2023-01-26T23:26:50.100+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.Task.run(Task.scala:136)
[2023-01-26T23:26:50.101+0000] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)
[2023-01-26T23:26:50.101+0000] {spark_submit.py:495} INFO - at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)
[2023-01-26T23:26:50.101+0000] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)
[2023-01-26T23:26:50.101+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[2023-01-26T23:26:50.102+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[2023-01-26T23:26:50.102+0000] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2023-01-26T23:26:50.102+0000] {spark_submit.py:495} INFO - 23/01/26 23:26:50 ERROR TaskSetManager: Task 0 in stage 37.0 failed 1 times; aborting job
[2023-01-26T23:26:50.285+0000] {spark_submit.py:495} INFO - [[34m2023-01-26 23:26:50,285[0m] {[34mconsumerSpark.py:[0m37} ERROR[0m - Error al escribir en Mongo: An error occurred while calling o338.save.
[2023-01-26T23:26:50.286+0000] {spark_submit.py:495} INFO - : org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 37.0 failed 1 times, most recent failure: Lost task 0.0 in stage 37.0 (TID 37) (f1cbb5df32e6 executor driver): com.mongodb.MongoTimeoutException: Timed out after 30000 ms while waiting to connect. Client view of cluster state is {type=UNKNOWN, servers=[{address=172.22.0.2:27017, type=UNKNOWN, state=CONNECTING, exception={com.mongodb.MongoSocketOpenException: Exception opening socket}, caused by {java.net.SocketTimeoutException: connect timed out}}]
[2023-01-26T23:26:50.286+0000] {spark_submit.py:495} INFO - at com.mongodb.internal.connection.BaseCluster.getDescription(BaseCluster.java:177)
[2023-01-26T23:26:50.287+0000] {spark_submit.py:495} INFO - at com.mongodb.internal.connection.SingleServerCluster.getDescription(SingleServerCluster.java:41)
[2023-01-26T23:26:50.287+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate.getConnectedClusterDescription(MongoClientDelegate.java:147)
[2023-01-26T23:26:50.288+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate.createClientSession(MongoClientDelegate.java:98)
[2023-01-26T23:26:50.288+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.getClientSession(MongoClientDelegate.java:278)
[2023-01-26T23:26:50.288+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:202)
[2023-01-26T23:26:50.289+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:441)
[2023-01-26T23:26:50.289+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:421)
[2023-01-26T23:26:50.290+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$4(MongoSpark.scala:138)
[2023-01-26T23:26:50.290+0000] {spark_submit.py:495} INFO - at scala.collection.Iterator.foreach(Iterator.scala:943)
[2023-01-26T23:26:50.290+0000] {spark_submit.py:495} INFO - at scala.collection.Iterator.foreach$(Iterator.scala:943)
[2023-01-26T23:26:50.291+0000] {spark_submit.py:495} INFO - at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
[2023-01-26T23:26:50.291+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$3(MongoSpark.scala:122)
[2023-01-26T23:26:50.291+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$3$adapted(MongoSpark.scala:121)
[2023-01-26T23:26:50.292+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.$anonfun$withCollectionDo$1(MongoConnector.scala:186)
[2023-01-26T23:26:50.292+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.$anonfun$withDatabaseDo$1(MongoConnector.scala:171)
[2023-01-26T23:26:50.293+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withMongoClientDo(MongoConnector.scala:154)
[2023-01-26T23:26:50.293+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withDatabaseDo(MongoConnector.scala:171)
[2023-01-26T23:26:50.293+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withCollectionDo(MongoConnector.scala:184)
[2023-01-26T23:26:50.294+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$2(MongoSpark.scala:121)
[2023-01-26T23:26:50.294+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$2$adapted(MongoSpark.scala:120)
[2023-01-26T23:26:50.294+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1011)
[2023-01-26T23:26:50.295+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1011)
[2023-01-26T23:26:50.295+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2268)
[2023-01-26T23:26:50.295+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
[2023-01-26T23:26:50.296+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.Task.run(Task.scala:136)
[2023-01-26T23:26:50.296+0000] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)
[2023-01-26T23:26:50.296+0000] {spark_submit.py:495} INFO - at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)
[2023-01-26T23:26:50.297+0000] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)
[2023-01-26T23:26:50.297+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[2023-01-26T23:26:50.297+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[2023-01-26T23:26:50.298+0000] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2023-01-26T23:26:50.298+0000] {spark_submit.py:495} INFO - 
[2023-01-26T23:26:50.298+0000] {spark_submit.py:495} INFO - Driver stacktrace:
[2023-01-26T23:26:50.299+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)
[2023-01-26T23:26:50.299+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)
[2023-01-26T23:26:50.299+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)
[2023-01-26T23:26:50.300+0000] {spark_submit.py:495} INFO - at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
[2023-01-26T23:26:50.300+0000] {spark_submit.py:495} INFO - at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
[2023-01-26T23:26:50.300+0000] {spark_submit.py:495} INFO - at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
[2023-01-26T23:26:50.301+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)
[2023-01-26T23:26:50.301+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)
[2023-01-26T23:26:50.301+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)
[2023-01-26T23:26:50.302+0000] {spark_submit.py:495} INFO - at scala.Option.foreach(Option.scala:407)
[2023-01-26T23:26:50.302+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)
[2023-01-26T23:26:50.302+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)
[2023-01-26T23:26:50.303+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)
[2023-01-26T23:26:50.303+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)
[2023-01-26T23:26:50.303+0000] {spark_submit.py:495} INFO - at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
[2023-01-26T23:26:50.304+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)
[2023-01-26T23:26:50.304+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.runJob(SparkContext.scala:2228)
[2023-01-26T23:26:50.304+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.runJob(SparkContext.scala:2249)
[2023-01-26T23:26:50.305+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.runJob(SparkContext.scala:2268)
[2023-01-26T23:26:50.305+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.runJob(SparkContext.scala:2293)
[2023-01-26T23:26:50.305+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$1(RDD.scala:1011)
[2023-01-26T23:26:50.306+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
[2023-01-26T23:26:50.306+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
[2023-01-26T23:26:50.306+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.withScope(RDD.scala:406)
[2023-01-26T23:26:50.307+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.foreachPartition(RDD.scala:1009)
[2023-01-26T23:26:50.307+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.save(MongoSpark.scala:120)
[2023-01-26T23:26:50.307+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.save(MongoSpark.scala:169)
[2023-01-26T23:26:50.308+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.sql.DefaultSource.createRelation(DefaultSource.scala:70)
[2023-01-26T23:26:50.308+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:47)
[2023-01-26T23:26:50.308+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)
[2023-01-26T23:26:50.309+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)
[2023-01-26T23:26:50.309+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)
[2023-01-26T23:26:50.310+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)
[2023-01-26T23:26:50.310+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)
[2023-01-26T23:26:50.310+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)
[2023-01-26T23:26:50.311+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)
[2023-01-26T23:26:50.311+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
[2023-01-26T23:26:50.311+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
[2023-01-26T23:26:50.312+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)
[2023-01-26T23:26:50.312+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)
[2023-01-26T23:26:50.312+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:584)
[2023-01-26T23:26:50.312+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:176)
[2023-01-26T23:26:50.313+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:584)
[2023-01-26T23:26:50.313+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)
[2023-01-26T23:26:50.313+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
[2023-01-26T23:26:50.313+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
[2023-01-26T23:26:50.314+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
[2023-01-26T23:26:50.314+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
[2023-01-26T23:26:50.314+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:560)
[2023-01-26T23:26:50.315+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)
[2023-01-26T23:26:50.315+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)
[2023-01-26T23:26:50.315+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)
[2023-01-26T23:26:50.315+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:116)
[2023-01-26T23:26:50.316+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:860)
[2023-01-26T23:26:50.316+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:390)
[2023-01-26T23:26:50.316+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:363)
[2023-01-26T23:26:50.316+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:247)
[2023-01-26T23:26:50.317+0000] {spark_submit.py:495} INFO - at jdk.internal.reflect.GeneratedMethodAccessor69.invoke(Unknown Source)
[2023-01-26T23:26:50.317+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
[2023-01-26T23:26:50.317+0000] {spark_submit.py:495} INFO - at java.base/java.lang.reflect.Method.invoke(Method.java:566)
[2023-01-26T23:26:50.317+0000] {spark_submit.py:495} INFO - at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
[2023-01-26T23:26:50.317+0000] {spark_submit.py:495} INFO - at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
[2023-01-26T23:26:50.318+0000] {spark_submit.py:495} INFO - at py4j.Gateway.invoke(Gateway.java:282)
[2023-01-26T23:26:50.318+0000] {spark_submit.py:495} INFO - at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
[2023-01-26T23:26:50.318+0000] {spark_submit.py:495} INFO - at py4j.commands.CallCommand.execute(CallCommand.java:79)
[2023-01-26T23:26:50.318+0000] {spark_submit.py:495} INFO - at py4j.ClientServerConnection.sendCommand(ClientServerConnection.java:244)
[2023-01-26T23:26:50.318+0000] {spark_submit.py:495} INFO - at py4j.CallbackClient.sendCommand(CallbackClient.java:384)
[2023-01-26T23:26:50.318+0000] {spark_submit.py:495} INFO - at py4j.CallbackClient.sendCommand(CallbackClient.java:356)
[2023-01-26T23:26:50.319+0000] {spark_submit.py:495} INFO - at py4j.reflection.PythonProxyHandler.invoke(PythonProxyHandler.java:106)
[2023-01-26T23:26:50.319+0000] {spark_submit.py:495} INFO - at com.sun.proxy.$Proxy32.call(Unknown Source)
[2023-01-26T23:26:50.319+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1(ForeachBatchSink.scala:51)
[2023-01-26T23:26:50.319+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1$adapted(ForeachBatchSink.scala:51)
[2023-01-26T23:26:50.319+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.sources.ForeachBatchSink.addBatch(ForeachBatchSink.scala:32)
[2023-01-26T23:26:50.319+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:666)
[2023-01-26T23:26:50.320+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)
[2023-01-26T23:26:50.320+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)
[2023-01-26T23:26:50.320+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)
[2023-01-26T23:26:50.320+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
[2023-01-26T23:26:50.320+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
[2023-01-26T23:26:50.320+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:664)
[2023-01-26T23:26:50.321+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:375)
[2023-01-26T23:26:50.321+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:373)
[2023-01-26T23:26:50.321+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:68)
[2023-01-26T23:26:50.321+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:664)
[2023-01-26T23:26:50.321+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:256)
[2023-01-26T23:26:50.321+0000] {spark_submit.py:495} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2023-01-26T23:26:50.322+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:375)
[2023-01-26T23:26:50.322+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:373)
[2023-01-26T23:26:50.322+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:68)
[2023-01-26T23:26:50.322+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:219)
[2023-01-26T23:26:50.322+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)
[2023-01-26T23:26:50.322+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:213)
[2023-01-26T23:26:50.322+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:307)
[2023-01-26T23:26:50.323+0000] {spark_submit.py:495} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2023-01-26T23:26:50.323+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
[2023-01-26T23:26:50.323+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:285)
[2023-01-26T23:26:50.323+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:208)
[2023-01-26T23:26:50.323+0000] {spark_submit.py:495} INFO - Caused by: com.mongodb.MongoTimeoutException: Timed out after 30000 ms while waiting to connect. Client view of cluster state is {type=UNKNOWN, servers=[{address=172.22.0.2:27017, type=UNKNOWN, state=CONNECTING, exception={com.mongodb.MongoSocketOpenException: Exception opening socket}, caused by {java.net.SocketTimeoutException: connect timed out}}]
[2023-01-26T23:26:50.323+0000] {spark_submit.py:495} INFO - at com.mongodb.internal.connection.BaseCluster.getDescription(BaseCluster.java:177)
[2023-01-26T23:26:50.323+0000] {spark_submit.py:495} INFO - at com.mongodb.internal.connection.SingleServerCluster.getDescription(SingleServerCluster.java:41)
[2023-01-26T23:26:50.324+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate.getConnectedClusterDescription(MongoClientDelegate.java:147)
[2023-01-26T23:26:50.324+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate.createClientSession(MongoClientDelegate.java:98)
[2023-01-26T23:26:50.324+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.getClientSession(MongoClientDelegate.java:278)
[2023-01-26T23:26:50.324+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:202)
[2023-01-26T23:26:50.324+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:441)
[2023-01-26T23:26:50.324+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:421)
[2023-01-26T23:26:50.324+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$4(MongoSpark.scala:138)
[2023-01-26T23:26:50.325+0000] {spark_submit.py:495} INFO - at scala.collection.Iterator.foreach(Iterator.scala:943)
[2023-01-26T23:26:50.325+0000] {spark_submit.py:495} INFO - at scala.collection.Iterator.foreach$(Iterator.scala:943)
[2023-01-26T23:26:50.325+0000] {spark_submit.py:495} INFO - at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
[2023-01-26T23:26:50.325+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$3(MongoSpark.scala:122)
[2023-01-26T23:26:50.325+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$3$adapted(MongoSpark.scala:121)
[2023-01-26T23:26:50.325+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.$anonfun$withCollectionDo$1(MongoConnector.scala:186)
[2023-01-26T23:26:50.325+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.$anonfun$withDatabaseDo$1(MongoConnector.scala:171)
[2023-01-26T23:26:50.325+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withMongoClientDo(MongoConnector.scala:154)
[2023-01-26T23:26:50.326+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withDatabaseDo(MongoConnector.scala:171)
[2023-01-26T23:26:50.326+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withCollectionDo(MongoConnector.scala:184)
[2023-01-26T23:26:50.326+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$2(MongoSpark.scala:121)
[2023-01-26T23:26:50.326+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$2$adapted(MongoSpark.scala:120)
[2023-01-26T23:26:50.326+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1011)
[2023-01-26T23:26:50.326+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1011)
[2023-01-26T23:26:50.327+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2268)
[2023-01-26T23:26:50.327+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
[2023-01-26T23:26:50.327+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.Task.run(Task.scala:136)
[2023-01-26T23:26:50.327+0000] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)
[2023-01-26T23:26:50.327+0000] {spark_submit.py:495} INFO - at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)
[2023-01-26T23:26:50.327+0000] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)
[2023-01-26T23:26:50.327+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[2023-01-26T23:26:50.327+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[2023-01-26T23:26:50.328+0000] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2023-01-26T23:26:50.328+0000] {spark_submit.py:495} INFO - [0m
[2023-01-26T23:26:50.403+0000] {spark_submit.py:495} INFO - [[34m2023-01-26 23:26:50,402[0m] {[34mclientserver.py:[0m577} INFO[0m - Received command c on object id p0[0m
[2023-01-26T23:27:20.456+0000] {spark_submit.py:495} INFO - 23/01/26 23:27:20 ERROR Executor: Exception in task 0.0 in stage 39.0 (TID 39)
[2023-01-26T23:27:20.457+0000] {spark_submit.py:495} INFO - com.mongodb.MongoTimeoutException: Timed out after 30000 ms while waiting to connect. Client view of cluster state is {type=UNKNOWN, servers=[{address=172.22.0.2:27017, type=UNKNOWN, state=CONNECTING, exception={com.mongodb.MongoSocketOpenException: Exception opening socket}, caused by {java.net.SocketTimeoutException: connect timed out}}]
[2023-01-26T23:27:20.457+0000] {spark_submit.py:495} INFO - at com.mongodb.internal.connection.BaseCluster.getDescription(BaseCluster.java:177)
[2023-01-26T23:27:20.457+0000] {spark_submit.py:495} INFO - at com.mongodb.internal.connection.SingleServerCluster.getDescription(SingleServerCluster.java:41)
[2023-01-26T23:27:20.457+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate.getConnectedClusterDescription(MongoClientDelegate.java:147)
[2023-01-26T23:27:20.458+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate.createClientSession(MongoClientDelegate.java:98)
[2023-01-26T23:27:20.458+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.getClientSession(MongoClientDelegate.java:278)
[2023-01-26T23:27:20.458+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:202)
[2023-01-26T23:27:20.458+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:441)
[2023-01-26T23:27:20.458+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:421)
[2023-01-26T23:27:20.458+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$4(MongoSpark.scala:138)
[2023-01-26T23:27:20.458+0000] {spark_submit.py:495} INFO - at scala.collection.Iterator.foreach(Iterator.scala:943)
[2023-01-26T23:27:20.458+0000] {spark_submit.py:495} INFO - at scala.collection.Iterator.foreach$(Iterator.scala:943)
[2023-01-26T23:27:20.458+0000] {spark_submit.py:495} INFO - at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
[2023-01-26T23:27:20.458+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$3(MongoSpark.scala:122)
[2023-01-26T23:27:20.458+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$3$adapted(MongoSpark.scala:121)
[2023-01-26T23:27:20.458+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.$anonfun$withCollectionDo$1(MongoConnector.scala:186)
[2023-01-26T23:27:20.458+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.$anonfun$withDatabaseDo$1(MongoConnector.scala:171)
[2023-01-26T23:27:20.459+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withMongoClientDo(MongoConnector.scala:154)
[2023-01-26T23:27:20.459+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withDatabaseDo(MongoConnector.scala:171)
[2023-01-26T23:27:20.459+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withCollectionDo(MongoConnector.scala:184)
[2023-01-26T23:27:20.459+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$2(MongoSpark.scala:121)
[2023-01-26T23:27:20.459+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$2$adapted(MongoSpark.scala:120)
[2023-01-26T23:27:20.464+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1011)
[2023-01-26T23:27:20.464+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1011)
[2023-01-26T23:27:20.464+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2268)
[2023-01-26T23:27:20.464+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
[2023-01-26T23:27:20.465+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.Task.run(Task.scala:136)
[2023-01-26T23:27:20.465+0000] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)
[2023-01-26T23:27:20.465+0000] {spark_submit.py:495} INFO - at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)
[2023-01-26T23:27:20.465+0000] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)
[2023-01-26T23:27:20.465+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[2023-01-26T23:27:20.465+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[2023-01-26T23:27:20.465+0000] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2023-01-26T23:27:20.467+0000] {spark_submit.py:495} INFO - 23/01/26 23:27:20 ERROR TaskSetManager: Task 0 in stage 39.0 failed 1 times; aborting job
[2023-01-26T23:27:20.609+0000] {spark_submit.py:495} INFO - [[34m2023-01-26 23:27:20,609[0m] {[34mconsumerSpark.py:[0m37} ERROR[0m - Error al escribir en Mongo: An error occurred while calling o347.save.
[2023-01-26T23:27:20.609+0000] {spark_submit.py:495} INFO - : org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 39.0 failed 1 times, most recent failure: Lost task 0.0 in stage 39.0 (TID 39) (f1cbb5df32e6 executor driver): com.mongodb.MongoTimeoutException: Timed out after 30000 ms while waiting to connect. Client view of cluster state is {type=UNKNOWN, servers=[{address=172.22.0.2:27017, type=UNKNOWN, state=CONNECTING, exception={com.mongodb.MongoSocketOpenException: Exception opening socket}, caused by {java.net.SocketTimeoutException: connect timed out}}]
[2023-01-26T23:27:20.609+0000] {spark_submit.py:495} INFO - at com.mongodb.internal.connection.BaseCluster.getDescription(BaseCluster.java:177)
[2023-01-26T23:27:20.609+0000] {spark_submit.py:495} INFO - at com.mongodb.internal.connection.SingleServerCluster.getDescription(SingleServerCluster.java:41)
[2023-01-26T23:27:20.609+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate.getConnectedClusterDescription(MongoClientDelegate.java:147)
[2023-01-26T23:27:20.609+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate.createClientSession(MongoClientDelegate.java:98)
[2023-01-26T23:27:20.610+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.getClientSession(MongoClientDelegate.java:278)
[2023-01-26T23:27:20.610+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:202)
[2023-01-26T23:27:20.610+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:441)
[2023-01-26T23:27:20.610+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:421)
[2023-01-26T23:27:20.610+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$4(MongoSpark.scala:138)
[2023-01-26T23:27:20.610+0000] {spark_submit.py:495} INFO - at scala.collection.Iterator.foreach(Iterator.scala:943)
[2023-01-26T23:27:20.610+0000] {spark_submit.py:495} INFO - at scala.collection.Iterator.foreach$(Iterator.scala:943)
[2023-01-26T23:27:20.610+0000] {spark_submit.py:495} INFO - at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
[2023-01-26T23:27:20.610+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$3(MongoSpark.scala:122)
[2023-01-26T23:27:20.610+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$3$adapted(MongoSpark.scala:121)
[2023-01-26T23:27:20.610+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.$anonfun$withCollectionDo$1(MongoConnector.scala:186)
[2023-01-26T23:27:20.610+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.$anonfun$withDatabaseDo$1(MongoConnector.scala:171)
[2023-01-26T23:27:20.610+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withMongoClientDo(MongoConnector.scala:154)
[2023-01-26T23:27:20.610+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withDatabaseDo(MongoConnector.scala:171)
[2023-01-26T23:27:20.610+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withCollectionDo(MongoConnector.scala:184)
[2023-01-26T23:27:20.611+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$2(MongoSpark.scala:121)
[2023-01-26T23:27:20.611+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$2$adapted(MongoSpark.scala:120)
[2023-01-26T23:27:20.611+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1011)
[2023-01-26T23:27:20.611+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1011)
[2023-01-26T23:27:20.611+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2268)
[2023-01-26T23:27:20.611+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
[2023-01-26T23:27:20.611+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.Task.run(Task.scala:136)
[2023-01-26T23:27:20.611+0000] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)
[2023-01-26T23:27:20.612+0000] {spark_submit.py:495} INFO - at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)
[2023-01-26T23:27:20.612+0000] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)
[2023-01-26T23:27:20.612+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[2023-01-26T23:27:20.612+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[2023-01-26T23:27:20.612+0000] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2023-01-26T23:27:20.612+0000] {spark_submit.py:495} INFO - 
[2023-01-26T23:27:20.612+0000] {spark_submit.py:495} INFO - Driver stacktrace:
[2023-01-26T23:27:20.612+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)
[2023-01-26T23:27:20.612+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)
[2023-01-26T23:27:20.612+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)
[2023-01-26T23:27:20.612+0000] {spark_submit.py:495} INFO - at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
[2023-01-26T23:27:20.613+0000] {spark_submit.py:495} INFO - at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
[2023-01-26T23:27:20.613+0000] {spark_submit.py:495} INFO - at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
[2023-01-26T23:27:20.613+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)
[2023-01-26T23:27:20.613+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)
[2023-01-26T23:27:20.613+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)
[2023-01-26T23:27:20.613+0000] {spark_submit.py:495} INFO - at scala.Option.foreach(Option.scala:407)
[2023-01-26T23:27:20.613+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)
[2023-01-26T23:27:20.613+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)
[2023-01-26T23:27:20.613+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)
[2023-01-26T23:27:20.613+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)
[2023-01-26T23:27:20.613+0000] {spark_submit.py:495} INFO - at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
[2023-01-26T23:27:20.613+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)
[2023-01-26T23:27:20.613+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.runJob(SparkContext.scala:2228)
[2023-01-26T23:27:20.614+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.runJob(SparkContext.scala:2249)
[2023-01-26T23:27:20.614+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.runJob(SparkContext.scala:2268)
[2023-01-26T23:27:20.614+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.runJob(SparkContext.scala:2293)
[2023-01-26T23:27:20.614+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$1(RDD.scala:1011)
[2023-01-26T23:27:20.614+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
[2023-01-26T23:27:20.614+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
[2023-01-26T23:27:20.614+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.withScope(RDD.scala:406)
[2023-01-26T23:27:20.614+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.foreachPartition(RDD.scala:1009)
[2023-01-26T23:27:20.614+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.save(MongoSpark.scala:120)
[2023-01-26T23:27:20.614+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.save(MongoSpark.scala:169)
[2023-01-26T23:27:20.614+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.sql.DefaultSource.createRelation(DefaultSource.scala:70)
[2023-01-26T23:27:20.614+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:47)
[2023-01-26T23:27:20.614+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)
[2023-01-26T23:27:20.615+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)
[2023-01-26T23:27:20.615+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)
[2023-01-26T23:27:20.615+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)
[2023-01-26T23:27:20.615+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)
[2023-01-26T23:27:20.615+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)
[2023-01-26T23:27:20.615+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)
[2023-01-26T23:27:20.615+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
[2023-01-26T23:27:20.615+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
[2023-01-26T23:27:20.615+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)
[2023-01-26T23:27:20.615+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)
[2023-01-26T23:27:20.615+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:584)
[2023-01-26T23:27:20.615+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:176)
[2023-01-26T23:27:20.615+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:584)
[2023-01-26T23:27:20.616+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)
[2023-01-26T23:27:20.616+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
[2023-01-26T23:27:20.616+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
[2023-01-26T23:27:20.616+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
[2023-01-26T23:27:20.616+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
[2023-01-26T23:27:20.616+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:560)
[2023-01-26T23:27:20.616+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)
[2023-01-26T23:27:20.616+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)
[2023-01-26T23:27:20.616+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)
[2023-01-26T23:27:20.616+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:116)
[2023-01-26T23:27:20.616+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:860)
[2023-01-26T23:27:20.616+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:390)
[2023-01-26T23:27:20.617+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:363)
[2023-01-26T23:27:20.617+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:247)
[2023-01-26T23:27:20.617+0000] {spark_submit.py:495} INFO - at jdk.internal.reflect.GeneratedMethodAccessor69.invoke(Unknown Source)
[2023-01-26T23:27:20.617+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
[2023-01-26T23:27:20.617+0000] {spark_submit.py:495} INFO - at java.base/java.lang.reflect.Method.invoke(Method.java:566)
[2023-01-26T23:27:20.617+0000] {spark_submit.py:495} INFO - at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
[2023-01-26T23:27:20.617+0000] {spark_submit.py:495} INFO - at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
[2023-01-26T23:27:20.617+0000] {spark_submit.py:495} INFO - at py4j.Gateway.invoke(Gateway.java:282)
[2023-01-26T23:27:20.617+0000] {spark_submit.py:495} INFO - at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
[2023-01-26T23:27:20.617+0000] {spark_submit.py:495} INFO - at py4j.commands.CallCommand.execute(CallCommand.java:79)
[2023-01-26T23:27:20.617+0000] {spark_submit.py:495} INFO - at py4j.ClientServerConnection.sendCommand(ClientServerConnection.java:244)
[2023-01-26T23:27:20.617+0000] {spark_submit.py:495} INFO - at py4j.CallbackClient.sendCommand(CallbackClient.java:384)
[2023-01-26T23:27:20.618+0000] {spark_submit.py:495} INFO - at py4j.CallbackClient.sendCommand(CallbackClient.java:356)
[2023-01-26T23:27:20.618+0000] {spark_submit.py:495} INFO - at py4j.reflection.PythonProxyHandler.invoke(PythonProxyHandler.java:106)
[2023-01-26T23:27:20.618+0000] {spark_submit.py:495} INFO - at com.sun.proxy.$Proxy32.call(Unknown Source)
[2023-01-26T23:27:20.618+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1(ForeachBatchSink.scala:51)
[2023-01-26T23:27:20.619+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1$adapted(ForeachBatchSink.scala:51)
[2023-01-26T23:27:20.620+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.sources.ForeachBatchSink.addBatch(ForeachBatchSink.scala:32)
[2023-01-26T23:27:20.620+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:666)
[2023-01-26T23:27:20.620+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)
[2023-01-26T23:27:20.620+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)
[2023-01-26T23:27:20.620+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)
[2023-01-26T23:27:20.620+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
[2023-01-26T23:27:20.620+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
[2023-01-26T23:27:20.620+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:664)
[2023-01-26T23:27:20.620+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:375)
[2023-01-26T23:27:20.620+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:373)
[2023-01-26T23:27:20.620+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:68)
[2023-01-26T23:27:20.620+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:664)
[2023-01-26T23:27:20.621+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:256)
[2023-01-26T23:27:20.621+0000] {spark_submit.py:495} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2023-01-26T23:27:20.621+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:375)
[2023-01-26T23:27:20.621+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:373)
[2023-01-26T23:27:20.621+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:68)
[2023-01-26T23:27:20.621+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:219)
[2023-01-26T23:27:20.621+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)
[2023-01-26T23:27:20.621+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:213)
[2023-01-26T23:27:20.621+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:307)
[2023-01-26T23:27:20.621+0000] {spark_submit.py:495} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2023-01-26T23:27:20.621+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
[2023-01-26T23:27:20.621+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:285)
[2023-01-26T23:27:20.622+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:208)
[2023-01-26T23:27:20.622+0000] {spark_submit.py:495} INFO - Caused by: com.mongodb.MongoTimeoutException: Timed out after 30000 ms while waiting to connect. Client view of cluster state is {type=UNKNOWN, servers=[{address=172.22.0.2:27017, type=UNKNOWN, state=CONNECTING, exception={com.mongodb.MongoSocketOpenException: Exception opening socket}, caused by {java.net.SocketTimeoutException: connect timed out}}]
[2023-01-26T23:27:20.622+0000] {spark_submit.py:495} INFO - at com.mongodb.internal.connection.BaseCluster.getDescription(BaseCluster.java:177)
[2023-01-26T23:27:20.622+0000] {spark_submit.py:495} INFO - at com.mongodb.internal.connection.SingleServerCluster.getDescription(SingleServerCluster.java:41)
[2023-01-26T23:27:20.622+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate.getConnectedClusterDescription(MongoClientDelegate.java:147)
[2023-01-26T23:27:20.622+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate.createClientSession(MongoClientDelegate.java:98)
[2023-01-26T23:27:20.622+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.getClientSession(MongoClientDelegate.java:278)
[2023-01-26T23:27:20.622+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:202)
[2023-01-26T23:27:20.622+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:441)
[2023-01-26T23:27:20.625+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:421)
[2023-01-26T23:27:20.626+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$4(MongoSpark.scala:138)
[2023-01-26T23:27:20.626+0000] {spark_submit.py:495} INFO - at scala.collection.Iterator.foreach(Iterator.scala:943)
[2023-01-26T23:27:20.627+0000] {spark_submit.py:495} INFO - at scala.collection.Iterator.foreach$(Iterator.scala:943)
[2023-01-26T23:27:20.627+0000] {spark_submit.py:495} INFO - at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
[2023-01-26T23:27:20.627+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$3(MongoSpark.scala:122)
[2023-01-26T23:27:20.627+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$3$adapted(MongoSpark.scala:121)
[2023-01-26T23:27:20.627+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.$anonfun$withCollectionDo$1(MongoConnector.scala:186)
[2023-01-26T23:27:20.627+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.$anonfun$withDatabaseDo$1(MongoConnector.scala:171)
[2023-01-26T23:27:20.627+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withMongoClientDo(MongoConnector.scala:154)
[2023-01-26T23:27:20.627+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withDatabaseDo(MongoConnector.scala:171)
[2023-01-26T23:27:20.627+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withCollectionDo(MongoConnector.scala:184)
[2023-01-26T23:27:20.627+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$2(MongoSpark.scala:121)
[2023-01-26T23:27:20.627+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$2$adapted(MongoSpark.scala:120)
[2023-01-26T23:27:20.629+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1011)
[2023-01-26T23:27:20.629+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1011)
[2023-01-26T23:27:20.629+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2268)
[2023-01-26T23:27:20.629+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
[2023-01-26T23:27:20.629+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.Task.run(Task.scala:136)
[2023-01-26T23:27:20.629+0000] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)
[2023-01-26T23:27:20.629+0000] {spark_submit.py:495} INFO - at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)
[2023-01-26T23:27:20.629+0000] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)
[2023-01-26T23:27:20.629+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[2023-01-26T23:27:20.630+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[2023-01-26T23:27:20.630+0000] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2023-01-26T23:27:20.630+0000] {spark_submit.py:495} INFO - [0m
[2023-01-26T23:30:52.432+0000] {spark_submit.py:495} INFO - [[34m2023-01-26 23:30:52,432[0m] {[34mclientserver.py:[0m577} INFO[0m - Received command c on object id p0[0m
[2023-01-26T23:30:55.970+0000] {spark_submit.py:495} INFO - -------------------------------------------
[2023-01-26T23:30:55.970+0000] {spark_submit.py:495} INFO - Batch: 14
[2023-01-26T23:30:55.970+0000] {spark_submit.py:495} INFO - -------------------------------------------
[2023-01-26T23:30:56.037+0000] {spark_submit.py:495} INFO - +-------------+--------------------+--------+-------------+--------------------+-------------------+------------+-----------------+--------------------+-------------+
[2023-01-26T23:30:56.037+0000] {spark_submit.py:495} INFO - |           id|       nameOfProduct|   price|ProductStatus|          SellerName|   RegistrationDate|QuantitySold|QuantityAvailable|      Free shipping?|Store ratings|
[2023-01-26T23:30:56.037+0000] {spark_submit.py:495} INFO - +-------------+--------------------+--------+-------------+--------------------+-------------------+------------+-----------------+--------------------+-------------+
[2023-01-26T23:30:56.037+0000] {spark_submit.py:495} INFO - |MLA1220964588|Procesador Gamer ...| 45899.0|          new|    OVERHARD DIGITAL|2015-01-08 12:19:41|          50|                1|Your shipment arr...|          4.5|
[2023-01-26T23:30:56.037+0000] {spark_submit.py:495} INFO - |MLA1157512489|Placa De Video Nv...|124999.0|          new|      PCREGISTRADAOK|2020-11-02 13:17:33|          50|                1|Your shipment arr...|          4.5|
[2023-01-26T23:30:56.037+0000] {spark_submit.py:495} INFO - |MLA1221192372|Disco Duro Extern...| 26299.0|          new|    OVERHARD DIGITAL|2015-01-08 12:19:41|         250|              100|Your shipment arr...|          4.5|
[2023-01-26T23:30:56.037+0000] {spark_submit.py:495} INFO - |MLA1163238194|Placa De Video Am...| 33999.0|          new|       MICYBERCOMPRA|2011-09-13 13:15:53|          50|                1|Your shipment arr...|          4.5|
[2023-01-26T23:30:56.037+0000] {spark_submit.py:495} INFO - |MLA1160204806|Memoria Ram Fury ...| 15499.0|          new|    OVERHARD DIGITAL|2015-01-08 12:19:41|         500|               50|Your shipment arr...|          4.5|
[2023-01-26T23:30:56.037+0000] {spark_submit.py:495} INFO - |MLA1115295971|Procesador Amd Ry...| 93999.0|          new|      PCREGISTRADAOK|2020-11-02 13:17:33|          25|                1|Your shipment arr...|          4.5|
[2023-01-26T23:30:56.037+0000] {spark_submit.py:495} INFO - |MLA1318244331|Disco Solido Ssd ...| 28799.0|          new|    OVERHARD DIGITAL|2015-01-08 12:19:41|           5|              150|Your shipment arr...|          4.5|
[2023-01-26T23:30:56.038+0000] {spark_submit.py:495} INFO - | MLA923637669|Placa De Video Ge...| 28699.0|          new|         INDIGOTRADE|2018-01-05 19:21:16|         500|               50|Your shipment arr...|          4.5|
[2023-01-26T23:30:56.038+0000] {spark_submit.py:495} INFO - |MLA1133481354|Disco Sólido Inte...| 13799.0|          new|           DALECLÍCK|2015-03-16 12:07:23|         500|                1|You have to pay t...|          4.5|
[2023-01-26T23:30:56.038+0000] {spark_submit.py:495} INFO - |MLA1139583771|Disco Sólido Inte...|  8339.0|          new|DIAMONDSYSTEMPADUAML|2015-07-01 12:32:22|         250|              250|You have to pay t...|          4.5|
[2023-01-26T23:30:56.038+0000] {spark_submit.py:495} INFO - | MLA882277177|Capturadora Video...|  2699.0|          new|         BSK-IMPORTS|2019-12-30 23:07:04|         500|               50|You have to pay t...|          3.0|
[2023-01-26T23:30:56.038+0000] {spark_submit.py:495} INFO - |MLA1148481665|Microprocesador P...| 58899.0|          new|            MALL WEB|2016-05-26 20:28:35|         150|                1|Your shipment arr...|          4.5|
[2023-01-26T23:30:56.038+0000] {spark_submit.py:495} INFO - |MLA1228990119|Memoria Ram Fury ...| 23255.0|          new|MARSTECHSRLMARSTE...|2020-08-05 14:27:59|          50|                1|Your shipment arr...|          4.5|
[2023-01-26T23:30:56.038+0000] {spark_submit.py:495} INFO - | MLA844489972|Disco Duro Intern...| 11599.0|          new|           DALECLÍCK|2015-03-16 12:07:23|        5000|              500|You have to pay t...|          5.0|
[2023-01-26T23:30:56.038+0000] {spark_submit.py:495} INFO - |MLA1183925519|Placa De Video Nv...|299999.0|          new|ESTUDIOSMPSRLESTU...|2020-11-29 18:36:32|          50|                1|Your shipment arr...|          4.5|
[2023-01-26T23:30:56.038+0000] {spark_submit.py:495} INFO - |MLA1168362675|Memoria Ram Fury ...| 26698.0|          new|MARSTECHSRLMARSTE...|2020-08-05 14:27:59|         150|                1|Your shipment arr...|          4.5|
[2023-01-26T23:30:56.038+0000] {spark_submit.py:495} INFO - |MLA1314590445|Disco Sólido Inte...|  7999.0|          new|       ELECTRO-SHOWS|2010-03-26 01:08:16|         250|              250|You have to pay t...|          4.5|
[2023-01-26T23:30:56.038+0000] {spark_submit.py:495} INFO - |MLA1163233412|Placa De Video Nv...| 34659.0|          new|      PCREGISTRADAOK|2020-11-02 13:17:33|           1|                1|Your shipment arr...|          4.5|
[2023-01-26T23:30:56.038+0000] {spark_submit.py:495} INFO - |MLA1302136118|Disco Duro Intern...| 18271.0|          new|DIAMONDSYSTEMSANJ...|2016-04-08 17:21:06|         100|               50|Your shipment arr...|          4.5|
[2023-01-26T23:30:56.038+0000] {spark_submit.py:495} INFO - |MLA1137238635|Fuente De Aliment...|  3235.0|          new|     DIAMONDSYSTEMML|2014-04-07 22:00:36|          25|                1|You have to pay t...|          3.5|
[2023-01-26T23:30:56.039+0000] {spark_submit.py:495} INFO - +-------------+--------------------+--------+-------------+--------------------+-------------------+------------+-----------------+--------------------+-------------+
[2023-01-26T23:30:56.039+0000] {spark_submit.py:495} INFO - only showing top 20 rows
[2023-01-26T23:30:56.039+0000] {spark_submit.py:495} INFO - 
[2023-01-26T23:31:23.283+0000] {spark_submit.py:495} INFO - 23/01/26 23:31:23 ERROR Executor: Exception in task 0.0 in stage 40.0 (TID 40)
[2023-01-26T23:31:23.284+0000] {spark_submit.py:495} INFO - com.mongodb.MongoTimeoutException: Timed out after 30000 ms while waiting to connect. Client view of cluster state is {type=UNKNOWN, servers=[{address=172.22.0.2:27017, type=UNKNOWN, state=CONNECTING, exception={com.mongodb.MongoSocketOpenException: Exception opening socket}, caused by {java.net.SocketTimeoutException: connect timed out}}]
[2023-01-26T23:31:23.284+0000] {spark_submit.py:495} INFO - at com.mongodb.internal.connection.BaseCluster.getDescription(BaseCluster.java:177)
[2023-01-26T23:31:23.285+0000] {spark_submit.py:495} INFO - at com.mongodb.internal.connection.SingleServerCluster.getDescription(SingleServerCluster.java:41)
[2023-01-26T23:31:23.285+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate.getConnectedClusterDescription(MongoClientDelegate.java:147)
[2023-01-26T23:31:23.286+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate.createClientSession(MongoClientDelegate.java:98)
[2023-01-26T23:31:23.286+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.getClientSession(MongoClientDelegate.java:278)
[2023-01-26T23:31:23.286+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:202)
[2023-01-26T23:31:23.287+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:441)
[2023-01-26T23:31:23.287+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:421)
[2023-01-26T23:31:23.288+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$4(MongoSpark.scala:138)
[2023-01-26T23:31:23.288+0000] {spark_submit.py:495} INFO - at scala.collection.Iterator.foreach(Iterator.scala:943)
[2023-01-26T23:31:23.288+0000] {spark_submit.py:495} INFO - at scala.collection.Iterator.foreach$(Iterator.scala:943)
[2023-01-26T23:31:23.289+0000] {spark_submit.py:495} INFO - at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
[2023-01-26T23:31:23.289+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$3(MongoSpark.scala:122)
[2023-01-26T23:31:23.289+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$3$adapted(MongoSpark.scala:121)
[2023-01-26T23:31:23.299+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.$anonfun$withCollectionDo$1(MongoConnector.scala:186)
[2023-01-26T23:31:23.299+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.$anonfun$withDatabaseDo$1(MongoConnector.scala:171)
[2023-01-26T23:31:23.300+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withMongoClientDo(MongoConnector.scala:154)
[2023-01-26T23:31:23.300+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withDatabaseDo(MongoConnector.scala:171)
[2023-01-26T23:31:23.300+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withCollectionDo(MongoConnector.scala:184)
[2023-01-26T23:31:23.301+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$2(MongoSpark.scala:121)
[2023-01-26T23:31:23.301+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$2$adapted(MongoSpark.scala:120)
[2023-01-26T23:31:23.301+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1011)
[2023-01-26T23:31:23.309+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1011)
[2023-01-26T23:31:23.309+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2268)
[2023-01-26T23:31:23.309+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
[2023-01-26T23:31:23.309+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.Task.run(Task.scala:136)
[2023-01-26T23:31:23.310+0000] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)
[2023-01-26T23:31:23.310+0000] {spark_submit.py:495} INFO - at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)
[2023-01-26T23:31:23.310+0000] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)
[2023-01-26T23:31:23.312+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[2023-01-26T23:31:23.312+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[2023-01-26T23:31:23.312+0000] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2023-01-26T23:31:23.316+0000] {spark_submit.py:495} INFO - 23/01/26 23:31:23 ERROR TaskSetManager: Task 0 in stage 40.0 failed 1 times; aborting job
[2023-01-26T23:31:23.428+0000] {spark_submit.py:495} INFO - [[34m2023-01-26 23:31:23,428[0m] {[34mconsumerSpark.py:[0m37} ERROR[0m - Error al escribir en Mongo: An error occurred while calling o356.save.
[2023-01-26T23:31:23.429+0000] {spark_submit.py:495} INFO - : org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 40.0 failed 1 times, most recent failure: Lost task 0.0 in stage 40.0 (TID 40) (f1cbb5df32e6 executor driver): com.mongodb.MongoTimeoutException: Timed out after 30000 ms while waiting to connect. Client view of cluster state is {type=UNKNOWN, servers=[{address=172.22.0.2:27017, type=UNKNOWN, state=CONNECTING, exception={com.mongodb.MongoSocketOpenException: Exception opening socket}, caused by {java.net.SocketTimeoutException: connect timed out}}]
[2023-01-26T23:31:23.429+0000] {spark_submit.py:495} INFO - at com.mongodb.internal.connection.BaseCluster.getDescription(BaseCluster.java:177)
[2023-01-26T23:31:23.429+0000] {spark_submit.py:495} INFO - at com.mongodb.internal.connection.SingleServerCluster.getDescription(SingleServerCluster.java:41)
[2023-01-26T23:31:23.429+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate.getConnectedClusterDescription(MongoClientDelegate.java:147)
[2023-01-26T23:31:23.429+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate.createClientSession(MongoClientDelegate.java:98)
[2023-01-26T23:31:23.429+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.getClientSession(MongoClientDelegate.java:278)
[2023-01-26T23:31:23.429+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:202)
[2023-01-26T23:31:23.429+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:441)
[2023-01-26T23:31:23.429+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:421)
[2023-01-26T23:31:23.429+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$4(MongoSpark.scala:138)
[2023-01-26T23:31:23.429+0000] {spark_submit.py:495} INFO - at scala.collection.Iterator.foreach(Iterator.scala:943)
[2023-01-26T23:31:23.429+0000] {spark_submit.py:495} INFO - at scala.collection.Iterator.foreach$(Iterator.scala:943)
[2023-01-26T23:31:23.430+0000] {spark_submit.py:495} INFO - at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
[2023-01-26T23:31:23.430+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$3(MongoSpark.scala:122)
[2023-01-26T23:31:23.430+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$3$adapted(MongoSpark.scala:121)
[2023-01-26T23:31:23.430+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.$anonfun$withCollectionDo$1(MongoConnector.scala:186)
[2023-01-26T23:31:23.430+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.$anonfun$withDatabaseDo$1(MongoConnector.scala:171)
[2023-01-26T23:31:23.430+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withMongoClientDo(MongoConnector.scala:154)
[2023-01-26T23:31:23.430+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withDatabaseDo(MongoConnector.scala:171)
[2023-01-26T23:31:23.430+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withCollectionDo(MongoConnector.scala:184)
[2023-01-26T23:31:23.430+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$2(MongoSpark.scala:121)
[2023-01-26T23:31:23.430+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$2$adapted(MongoSpark.scala:120)
[2023-01-26T23:31:23.430+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1011)
[2023-01-26T23:31:23.430+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1011)
[2023-01-26T23:31:23.430+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2268)
[2023-01-26T23:31:23.431+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
[2023-01-26T23:31:23.431+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.Task.run(Task.scala:136)
[2023-01-26T23:31:23.431+0000] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)
[2023-01-26T23:31:23.431+0000] {spark_submit.py:495} INFO - at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)
[2023-01-26T23:31:23.431+0000] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)
[2023-01-26T23:31:23.431+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[2023-01-26T23:31:23.431+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[2023-01-26T23:31:23.431+0000] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2023-01-26T23:31:23.431+0000] {spark_submit.py:495} INFO - 
[2023-01-26T23:31:23.431+0000] {spark_submit.py:495} INFO - Driver stacktrace:
[2023-01-26T23:31:23.431+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)
[2023-01-26T23:31:23.431+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)
[2023-01-26T23:31:23.431+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)
[2023-01-26T23:31:23.431+0000] {spark_submit.py:495} INFO - at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
[2023-01-26T23:31:23.431+0000] {spark_submit.py:495} INFO - at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
[2023-01-26T23:31:23.432+0000] {spark_submit.py:495} INFO - at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
[2023-01-26T23:31:23.432+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)
[2023-01-26T23:31:23.432+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)
[2023-01-26T23:31:23.432+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)
[2023-01-26T23:31:23.432+0000] {spark_submit.py:495} INFO - at scala.Option.foreach(Option.scala:407)
[2023-01-26T23:31:23.432+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)
[2023-01-26T23:31:23.432+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)
[2023-01-26T23:31:23.432+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)
[2023-01-26T23:31:23.432+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)
[2023-01-26T23:31:23.432+0000] {spark_submit.py:495} INFO - at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
[2023-01-26T23:31:23.432+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)
[2023-01-26T23:31:23.432+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.runJob(SparkContext.scala:2228)
[2023-01-26T23:31:23.432+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.runJob(SparkContext.scala:2249)
[2023-01-26T23:31:23.432+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.runJob(SparkContext.scala:2268)
[2023-01-26T23:31:23.433+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.runJob(SparkContext.scala:2293)
[2023-01-26T23:31:23.433+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$1(RDD.scala:1011)
[2023-01-26T23:31:23.433+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
[2023-01-26T23:31:23.433+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
[2023-01-26T23:31:23.433+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.withScope(RDD.scala:406)
[2023-01-26T23:31:23.433+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.foreachPartition(RDD.scala:1009)
[2023-01-26T23:31:23.433+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.save(MongoSpark.scala:120)
[2023-01-26T23:31:23.433+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.save(MongoSpark.scala:169)
[2023-01-26T23:31:23.433+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.sql.DefaultSource.createRelation(DefaultSource.scala:70)
[2023-01-26T23:31:23.433+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:47)
[2023-01-26T23:31:23.433+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)
[2023-01-26T23:31:23.433+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)
[2023-01-26T23:31:23.433+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)
[2023-01-26T23:31:23.433+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)
[2023-01-26T23:31:23.433+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)
[2023-01-26T23:31:23.434+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)
[2023-01-26T23:31:23.434+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)
[2023-01-26T23:31:23.434+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
[2023-01-26T23:31:23.434+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
[2023-01-26T23:31:23.434+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)
[2023-01-26T23:31:23.434+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)
[2023-01-26T23:31:23.434+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:584)
[2023-01-26T23:31:23.434+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:176)
[2023-01-26T23:31:23.434+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:584)
[2023-01-26T23:31:23.434+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)
[2023-01-26T23:31:23.434+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
[2023-01-26T23:31:23.434+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
[2023-01-26T23:31:23.434+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
[2023-01-26T23:31:23.434+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
[2023-01-26T23:31:23.434+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:560)
[2023-01-26T23:31:23.435+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)
[2023-01-26T23:31:23.435+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)
[2023-01-26T23:31:23.435+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)
[2023-01-26T23:31:23.435+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:116)
[2023-01-26T23:31:23.435+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:860)
[2023-01-26T23:31:23.435+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:390)
[2023-01-26T23:31:23.435+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:363)
[2023-01-26T23:31:23.435+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:247)
[2023-01-26T23:31:23.435+0000] {spark_submit.py:495} INFO - at jdk.internal.reflect.GeneratedMethodAccessor69.invoke(Unknown Source)
[2023-01-26T23:31:23.435+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
[2023-01-26T23:31:23.435+0000] {spark_submit.py:495} INFO - at java.base/java.lang.reflect.Method.invoke(Method.java:566)
[2023-01-26T23:31:23.435+0000] {spark_submit.py:495} INFO - at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
[2023-01-26T23:31:23.435+0000] {spark_submit.py:495} INFO - at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
[2023-01-26T23:31:23.435+0000] {spark_submit.py:495} INFO - at py4j.Gateway.invoke(Gateway.java:282)
[2023-01-26T23:31:23.435+0000] {spark_submit.py:495} INFO - at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
[2023-01-26T23:31:23.436+0000] {spark_submit.py:495} INFO - at py4j.commands.CallCommand.execute(CallCommand.java:79)
[2023-01-26T23:31:23.436+0000] {spark_submit.py:495} INFO - at py4j.ClientServerConnection.sendCommand(ClientServerConnection.java:244)
[2023-01-26T23:31:23.436+0000] {spark_submit.py:495} INFO - at py4j.CallbackClient.sendCommand(CallbackClient.java:384)
[2023-01-26T23:31:23.436+0000] {spark_submit.py:495} INFO - at py4j.CallbackClient.sendCommand(CallbackClient.java:356)
[2023-01-26T23:31:23.436+0000] {spark_submit.py:495} INFO - at py4j.reflection.PythonProxyHandler.invoke(PythonProxyHandler.java:106)
[2023-01-26T23:31:23.436+0000] {spark_submit.py:495} INFO - at com.sun.proxy.$Proxy32.call(Unknown Source)
[2023-01-26T23:31:23.436+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1(ForeachBatchSink.scala:51)
[2023-01-26T23:31:23.436+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1$adapted(ForeachBatchSink.scala:51)
[2023-01-26T23:31:23.436+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.sources.ForeachBatchSink.addBatch(ForeachBatchSink.scala:32)
[2023-01-26T23:31:23.436+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:666)
[2023-01-26T23:31:23.436+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)
[2023-01-26T23:31:23.436+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)
[2023-01-26T23:31:23.436+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)
[2023-01-26T23:31:23.436+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
[2023-01-26T23:31:23.436+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
[2023-01-26T23:31:23.437+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:664)
[2023-01-26T23:31:23.437+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:375)
[2023-01-26T23:31:23.437+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:373)
[2023-01-26T23:31:23.437+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:68)
[2023-01-26T23:31:23.437+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:664)
[2023-01-26T23:31:23.437+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:256)
[2023-01-26T23:31:23.437+0000] {spark_submit.py:495} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2023-01-26T23:31:23.437+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:375)
[2023-01-26T23:31:23.437+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:373)
[2023-01-26T23:31:23.437+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:68)
[2023-01-26T23:31:23.437+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:219)
[2023-01-26T23:31:23.437+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)
[2023-01-26T23:31:23.437+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:213)
[2023-01-26T23:31:23.437+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:307)
[2023-01-26T23:31:23.437+0000] {spark_submit.py:495} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2023-01-26T23:31:23.437+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
[2023-01-26T23:31:23.438+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:285)
[2023-01-26T23:31:23.438+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:208)
[2023-01-26T23:31:23.438+0000] {spark_submit.py:495} INFO - Caused by: com.mongodb.MongoTimeoutException: Timed out after 30000 ms while waiting to connect. Client view of cluster state is {type=UNKNOWN, servers=[{address=172.22.0.2:27017, type=UNKNOWN, state=CONNECTING, exception={com.mongodb.MongoSocketOpenException: Exception opening socket}, caused by {java.net.SocketTimeoutException: connect timed out}}]
[2023-01-26T23:31:23.438+0000] {spark_submit.py:495} INFO - at com.mongodb.internal.connection.BaseCluster.getDescription(BaseCluster.java:177)
[2023-01-26T23:31:23.438+0000] {spark_submit.py:495} INFO - at com.mongodb.internal.connection.SingleServerCluster.getDescription(SingleServerCluster.java:41)
[2023-01-26T23:31:23.438+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate.getConnectedClusterDescription(MongoClientDelegate.java:147)
[2023-01-26T23:31:23.438+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate.createClientSession(MongoClientDelegate.java:98)
[2023-01-26T23:31:23.438+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.getClientSession(MongoClientDelegate.java:278)
[2023-01-26T23:31:23.438+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:202)
[2023-01-26T23:31:23.438+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:441)
[2023-01-26T23:31:23.438+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:421)
[2023-01-26T23:31:23.438+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$4(MongoSpark.scala:138)
[2023-01-26T23:31:23.438+0000] {spark_submit.py:495} INFO - at scala.collection.Iterator.foreach(Iterator.scala:943)
[2023-01-26T23:31:23.438+0000] {spark_submit.py:495} INFO - at scala.collection.Iterator.foreach$(Iterator.scala:943)
[2023-01-26T23:31:23.438+0000] {spark_submit.py:495} INFO - at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
[2023-01-26T23:31:23.439+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$3(MongoSpark.scala:122)
[2023-01-26T23:31:23.439+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$3$adapted(MongoSpark.scala:121)
[2023-01-26T23:31:23.439+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.$anonfun$withCollectionDo$1(MongoConnector.scala:186)
[2023-01-26T23:31:23.439+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.$anonfun$withDatabaseDo$1(MongoConnector.scala:171)
[2023-01-26T23:31:23.439+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withMongoClientDo(MongoConnector.scala:154)
[2023-01-26T23:31:23.439+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withDatabaseDo(MongoConnector.scala:171)
[2023-01-26T23:31:23.439+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withCollectionDo(MongoConnector.scala:184)
[2023-01-26T23:31:23.439+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$2(MongoSpark.scala:121)
[2023-01-26T23:31:23.439+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$2$adapted(MongoSpark.scala:120)
[2023-01-26T23:31:23.439+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1011)
[2023-01-26T23:31:23.439+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1011)
[2023-01-26T23:31:23.439+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2268)
[2023-01-26T23:31:23.439+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
[2023-01-26T23:31:23.439+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.Task.run(Task.scala:136)
[2023-01-26T23:31:23.439+0000] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)
[2023-01-26T23:31:23.440+0000] {spark_submit.py:495} INFO - at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)
[2023-01-26T23:31:23.440+0000] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)
[2023-01-26T23:31:23.440+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[2023-01-26T23:31:23.440+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[2023-01-26T23:31:23.440+0000] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2023-01-26T23:31:23.440+0000] {spark_submit.py:495} INFO - [0m
[2023-01-26T23:31:23.536+0000] {spark_submit.py:495} INFO - [[34m2023-01-26 23:31:23,536[0m] {[34mclientserver.py:[0m577} INFO[0m - Received command c on object id p0[0m
[2023-01-26T23:31:53.604+0000] {spark_submit.py:495} INFO - 23/01/26 23:31:53 ERROR Executor: Exception in task 0.0 in stage 42.0 (TID 42)
[2023-01-26T23:31:53.604+0000] {spark_submit.py:495} INFO - com.mongodb.MongoTimeoutException: Timed out after 30000 ms while waiting to connect. Client view of cluster state is {type=UNKNOWN, servers=[{address=172.22.0.2:27017, type=UNKNOWN, state=CONNECTING, exception={com.mongodb.MongoSocketOpenException: Exception opening socket}, caused by {java.net.SocketTimeoutException: connect timed out}}]
[2023-01-26T23:31:53.605+0000] {spark_submit.py:495} INFO - at com.mongodb.internal.connection.BaseCluster.getDescription(BaseCluster.java:177)
[2023-01-26T23:31:53.605+0000] {spark_submit.py:495} INFO - at com.mongodb.internal.connection.SingleServerCluster.getDescription(SingleServerCluster.java:41)
[2023-01-26T23:31:53.606+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate.getConnectedClusterDescription(MongoClientDelegate.java:147)
[2023-01-26T23:31:53.606+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate.createClientSession(MongoClientDelegate.java:98)
[2023-01-26T23:31:53.606+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.getClientSession(MongoClientDelegate.java:278)
[2023-01-26T23:31:53.607+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:202)
[2023-01-26T23:31:53.607+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:441)
[2023-01-26T23:31:53.607+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:421)
[2023-01-26T23:31:53.608+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$4(MongoSpark.scala:138)
[2023-01-26T23:31:53.608+0000] {spark_submit.py:495} INFO - at scala.collection.Iterator.foreach(Iterator.scala:943)
[2023-01-26T23:31:53.609+0000] {spark_submit.py:495} INFO - at scala.collection.Iterator.foreach$(Iterator.scala:943)
[2023-01-26T23:31:53.609+0000] {spark_submit.py:495} INFO - at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
[2023-01-26T23:31:53.609+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$3(MongoSpark.scala:122)
[2023-01-26T23:31:53.610+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$3$adapted(MongoSpark.scala:121)
[2023-01-26T23:31:53.610+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.$anonfun$withCollectionDo$1(MongoConnector.scala:186)
[2023-01-26T23:31:53.610+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.$anonfun$withDatabaseDo$1(MongoConnector.scala:171)
[2023-01-26T23:31:53.611+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withMongoClientDo(MongoConnector.scala:154)
[2023-01-26T23:31:53.611+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withDatabaseDo(MongoConnector.scala:171)
[2023-01-26T23:31:53.612+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withCollectionDo(MongoConnector.scala:184)
[2023-01-26T23:31:53.613+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$2(MongoSpark.scala:121)
[2023-01-26T23:31:53.613+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$2$adapted(MongoSpark.scala:120)
[2023-01-26T23:31:53.614+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1011)
[2023-01-26T23:31:53.614+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1011)
[2023-01-26T23:31:53.614+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2268)
[2023-01-26T23:31:53.615+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
[2023-01-26T23:31:53.615+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.Task.run(Task.scala:136)
[2023-01-26T23:31:53.615+0000] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)
[2023-01-26T23:31:53.616+0000] {spark_submit.py:495} INFO - at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)
[2023-01-26T23:31:53.616+0000] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)
[2023-01-26T23:31:53.617+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[2023-01-26T23:31:53.617+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[2023-01-26T23:31:53.617+0000] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2023-01-26T23:31:53.618+0000] {spark_submit.py:495} INFO - 23/01/26 23:31:53 ERROR TaskSetManager: Task 0 in stage 42.0 failed 1 times; aborting job
[2023-01-26T23:31:53.785+0000] {spark_submit.py:495} INFO - [[34m2023-01-26 23:31:53,784[0m] {[34mconsumerSpark.py:[0m37} ERROR[0m - Error al escribir en Mongo: An error occurred while calling o365.save.
[2023-01-26T23:31:53.785+0000] {spark_submit.py:495} INFO - : org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 42.0 failed 1 times, most recent failure: Lost task 0.0 in stage 42.0 (TID 42) (f1cbb5df32e6 executor driver): com.mongodb.MongoTimeoutException: Timed out after 30000 ms while waiting to connect. Client view of cluster state is {type=UNKNOWN, servers=[{address=172.22.0.2:27017, type=UNKNOWN, state=CONNECTING, exception={com.mongodb.MongoSocketOpenException: Exception opening socket}, caused by {java.net.SocketTimeoutException: connect timed out}}]
[2023-01-26T23:31:53.785+0000] {spark_submit.py:495} INFO - at com.mongodb.internal.connection.BaseCluster.getDescription(BaseCluster.java:177)
[2023-01-26T23:31:53.786+0000] {spark_submit.py:495} INFO - at com.mongodb.internal.connection.SingleServerCluster.getDescription(SingleServerCluster.java:41)
[2023-01-26T23:31:53.786+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate.getConnectedClusterDescription(MongoClientDelegate.java:147)
[2023-01-26T23:31:53.786+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate.createClientSession(MongoClientDelegate.java:98)
[2023-01-26T23:31:53.786+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.getClientSession(MongoClientDelegate.java:278)
[2023-01-26T23:31:53.786+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:202)
[2023-01-26T23:31:53.786+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:441)
[2023-01-26T23:31:53.787+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:421)
[2023-01-26T23:31:53.787+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$4(MongoSpark.scala:138)
[2023-01-26T23:31:53.787+0000] {spark_submit.py:495} INFO - at scala.collection.Iterator.foreach(Iterator.scala:943)
[2023-01-26T23:31:53.787+0000] {spark_submit.py:495} INFO - at scala.collection.Iterator.foreach$(Iterator.scala:943)
[2023-01-26T23:31:53.787+0000] {spark_submit.py:495} INFO - at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
[2023-01-26T23:31:53.787+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$3(MongoSpark.scala:122)
[2023-01-26T23:31:53.788+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$3$adapted(MongoSpark.scala:121)
[2023-01-26T23:31:53.788+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.$anonfun$withCollectionDo$1(MongoConnector.scala:186)
[2023-01-26T23:31:53.788+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.$anonfun$withDatabaseDo$1(MongoConnector.scala:171)
[2023-01-26T23:31:53.788+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withMongoClientDo(MongoConnector.scala:154)
[2023-01-26T23:31:53.788+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withDatabaseDo(MongoConnector.scala:171)
[2023-01-26T23:31:53.789+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withCollectionDo(MongoConnector.scala:184)
[2023-01-26T23:31:53.789+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$2(MongoSpark.scala:121)
[2023-01-26T23:31:53.789+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$2$adapted(MongoSpark.scala:120)
[2023-01-26T23:31:53.789+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1011)
[2023-01-26T23:31:53.790+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1011)
[2023-01-26T23:31:53.790+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2268)
[2023-01-26T23:31:53.790+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
[2023-01-26T23:31:53.790+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.Task.run(Task.scala:136)
[2023-01-26T23:31:53.790+0000] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)
[2023-01-26T23:31:53.791+0000] {spark_submit.py:495} INFO - at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)
[2023-01-26T23:31:53.791+0000] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)
[2023-01-26T23:31:53.791+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[2023-01-26T23:31:53.792+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[2023-01-26T23:31:53.792+0000] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2023-01-26T23:31:53.792+0000] {spark_submit.py:495} INFO - 
[2023-01-26T23:31:53.793+0000] {spark_submit.py:495} INFO - Driver stacktrace:
[2023-01-26T23:31:53.793+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)
[2023-01-26T23:31:53.793+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)
[2023-01-26T23:31:53.793+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)
[2023-01-26T23:31:53.793+0000] {spark_submit.py:495} INFO - at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
[2023-01-26T23:31:53.793+0000] {spark_submit.py:495} INFO - at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
[2023-01-26T23:31:53.794+0000] {spark_submit.py:495} INFO - at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
[2023-01-26T23:31:53.794+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)
[2023-01-26T23:31:53.794+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)
[2023-01-26T23:31:53.794+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)
[2023-01-26T23:31:53.797+0000] {spark_submit.py:495} INFO - at scala.Option.foreach(Option.scala:407)
[2023-01-26T23:31:53.798+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)
[2023-01-26T23:31:53.798+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)
[2023-01-26T23:31:53.798+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)
[2023-01-26T23:31:53.798+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)
[2023-01-26T23:31:53.799+0000] {spark_submit.py:495} INFO - at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
[2023-01-26T23:31:53.799+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)
[2023-01-26T23:31:53.799+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.runJob(SparkContext.scala:2228)
[2023-01-26T23:31:53.799+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.runJob(SparkContext.scala:2249)
[2023-01-26T23:31:53.800+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.runJob(SparkContext.scala:2268)
[2023-01-26T23:31:53.800+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.runJob(SparkContext.scala:2293)
[2023-01-26T23:31:53.800+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$1(RDD.scala:1011)
[2023-01-26T23:31:53.800+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
[2023-01-26T23:31:53.800+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
[2023-01-26T23:31:53.800+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.withScope(RDD.scala:406)
[2023-01-26T23:31:53.800+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.foreachPartition(RDD.scala:1009)
[2023-01-26T23:31:53.801+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.save(MongoSpark.scala:120)
[2023-01-26T23:31:53.801+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.save(MongoSpark.scala:169)
[2023-01-26T23:31:53.801+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.sql.DefaultSource.createRelation(DefaultSource.scala:70)
[2023-01-26T23:31:53.801+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:47)
[2023-01-26T23:31:53.801+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)
[2023-01-26T23:31:53.801+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)
[2023-01-26T23:31:53.801+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)
[2023-01-26T23:31:53.802+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)
[2023-01-26T23:31:53.802+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)
[2023-01-26T23:31:53.802+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)
[2023-01-26T23:31:53.802+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)
[2023-01-26T23:31:53.802+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
[2023-01-26T23:31:53.802+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
[2023-01-26T23:31:53.802+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)
[2023-01-26T23:31:53.803+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)
[2023-01-26T23:31:53.803+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:584)
[2023-01-26T23:31:53.803+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:176)
[2023-01-26T23:31:53.803+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:584)
[2023-01-26T23:31:53.803+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)
[2023-01-26T23:31:53.803+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
[2023-01-26T23:31:53.803+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
[2023-01-26T23:31:53.803+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
[2023-01-26T23:31:53.804+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
[2023-01-26T23:31:53.804+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:560)
[2023-01-26T23:31:53.804+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)
[2023-01-26T23:31:53.804+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)
[2023-01-26T23:31:53.804+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)
[2023-01-26T23:31:53.804+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:116)
[2023-01-26T23:31:53.804+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:860)
[2023-01-26T23:31:53.805+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:390)
[2023-01-26T23:31:53.805+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:363)
[2023-01-26T23:31:53.805+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:247)
[2023-01-26T23:31:53.805+0000] {spark_submit.py:495} INFO - at jdk.internal.reflect.GeneratedMethodAccessor69.invoke(Unknown Source)
[2023-01-26T23:31:53.805+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
[2023-01-26T23:31:53.805+0000] {spark_submit.py:495} INFO - at java.base/java.lang.reflect.Method.invoke(Method.java:566)
[2023-01-26T23:31:53.805+0000] {spark_submit.py:495} INFO - at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
[2023-01-26T23:31:53.805+0000] {spark_submit.py:495} INFO - at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
[2023-01-26T23:31:53.805+0000] {spark_submit.py:495} INFO - at py4j.Gateway.invoke(Gateway.java:282)
[2023-01-26T23:31:53.806+0000] {spark_submit.py:495} INFO - at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
[2023-01-26T23:31:53.806+0000] {spark_submit.py:495} INFO - at py4j.commands.CallCommand.execute(CallCommand.java:79)
[2023-01-26T23:31:53.806+0000] {spark_submit.py:495} INFO - at py4j.ClientServerConnection.sendCommand(ClientServerConnection.java:244)
[2023-01-26T23:31:53.806+0000] {spark_submit.py:495} INFO - at py4j.CallbackClient.sendCommand(CallbackClient.java:384)
[2023-01-26T23:31:53.806+0000] {spark_submit.py:495} INFO - at py4j.CallbackClient.sendCommand(CallbackClient.java:356)
[2023-01-26T23:31:53.806+0000] {spark_submit.py:495} INFO - at py4j.reflection.PythonProxyHandler.invoke(PythonProxyHandler.java:106)
[2023-01-26T23:31:53.806+0000] {spark_submit.py:495} INFO - at com.sun.proxy.$Proxy32.call(Unknown Source)
[2023-01-26T23:31:53.806+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1(ForeachBatchSink.scala:51)
[2023-01-26T23:31:53.807+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1$adapted(ForeachBatchSink.scala:51)
[2023-01-26T23:31:53.807+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.sources.ForeachBatchSink.addBatch(ForeachBatchSink.scala:32)
[2023-01-26T23:31:53.807+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:666)
[2023-01-26T23:31:53.807+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)
[2023-01-26T23:31:53.807+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)
[2023-01-26T23:31:53.807+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)
[2023-01-26T23:31:53.807+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
[2023-01-26T23:31:53.807+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
[2023-01-26T23:31:53.807+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:664)
[2023-01-26T23:31:53.808+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:375)
[2023-01-26T23:31:53.808+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:373)
[2023-01-26T23:31:53.808+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:68)
[2023-01-26T23:31:53.808+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:664)
[2023-01-26T23:31:53.808+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:256)
[2023-01-26T23:31:53.808+0000] {spark_submit.py:495} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2023-01-26T23:31:53.809+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:375)
[2023-01-26T23:31:53.809+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:373)
[2023-01-26T23:31:53.809+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:68)
[2023-01-26T23:31:53.809+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:219)
[2023-01-26T23:31:53.809+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)
[2023-01-26T23:31:53.809+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:213)
[2023-01-26T23:31:53.809+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:307)
[2023-01-26T23:31:53.810+0000] {spark_submit.py:495} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2023-01-26T23:31:53.810+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
[2023-01-26T23:31:53.810+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:285)
[2023-01-26T23:31:53.810+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:208)
[2023-01-26T23:31:53.810+0000] {spark_submit.py:495} INFO - Caused by: com.mongodb.MongoTimeoutException: Timed out after 30000 ms while waiting to connect. Client view of cluster state is {type=UNKNOWN, servers=[{address=172.22.0.2:27017, type=UNKNOWN, state=CONNECTING, exception={com.mongodb.MongoSocketOpenException: Exception opening socket}, caused by {java.net.SocketTimeoutException: connect timed out}}]
[2023-01-26T23:31:53.810+0000] {spark_submit.py:495} INFO - at com.mongodb.internal.connection.BaseCluster.getDescription(BaseCluster.java:177)
[2023-01-26T23:31:53.810+0000] {spark_submit.py:495} INFO - at com.mongodb.internal.connection.SingleServerCluster.getDescription(SingleServerCluster.java:41)
[2023-01-26T23:31:53.810+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate.getConnectedClusterDescription(MongoClientDelegate.java:147)
[2023-01-26T23:31:53.811+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate.createClientSession(MongoClientDelegate.java:98)
[2023-01-26T23:31:53.811+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.getClientSession(MongoClientDelegate.java:278)
[2023-01-26T23:31:53.811+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:202)
[2023-01-26T23:31:53.811+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:441)
[2023-01-26T23:31:53.811+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:421)
[2023-01-26T23:31:53.811+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$4(MongoSpark.scala:138)
[2023-01-26T23:31:53.811+0000] {spark_submit.py:495} INFO - at scala.collection.Iterator.foreach(Iterator.scala:943)
[2023-01-26T23:31:53.811+0000] {spark_submit.py:495} INFO - at scala.collection.Iterator.foreach$(Iterator.scala:943)
[2023-01-26T23:31:53.811+0000] {spark_submit.py:495} INFO - at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
[2023-01-26T23:31:53.812+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$3(MongoSpark.scala:122)
[2023-01-26T23:31:53.812+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$3$adapted(MongoSpark.scala:121)
[2023-01-26T23:31:53.812+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.$anonfun$withCollectionDo$1(MongoConnector.scala:186)
[2023-01-26T23:31:53.812+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.$anonfun$withDatabaseDo$1(MongoConnector.scala:171)
[2023-01-26T23:31:53.812+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withMongoClientDo(MongoConnector.scala:154)
[2023-01-26T23:31:53.812+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withDatabaseDo(MongoConnector.scala:171)
[2023-01-26T23:31:53.812+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withCollectionDo(MongoConnector.scala:184)
[2023-01-26T23:31:53.812+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$2(MongoSpark.scala:121)
[2023-01-26T23:31:53.812+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$2$adapted(MongoSpark.scala:120)
[2023-01-26T23:31:53.812+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1011)
[2023-01-26T23:31:53.813+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1011)
[2023-01-26T23:31:53.813+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2268)
[2023-01-26T23:31:53.813+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
[2023-01-26T23:31:53.813+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.Task.run(Task.scala:136)
[2023-01-26T23:31:53.813+0000] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)
[2023-01-26T23:31:53.813+0000] {spark_submit.py:495} INFO - at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)
[2023-01-26T23:31:53.813+0000] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)
[2023-01-26T23:31:53.813+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[2023-01-26T23:31:53.813+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[2023-01-26T23:31:53.813+0000] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2023-01-26T23:31:53.814+0000] {spark_submit.py:495} INFO - [0m
[2023-01-26T23:35:46.576+0000] {spark_submit.py:495} INFO - [[34m2023-01-26 23:35:46,576[0m] {[34mclientserver.py:[0m577} INFO[0m - Received command c on object id p0[0m
[2023-01-26T23:35:51.484+0000] {spark_submit.py:495} INFO - -------------------------------------------
[2023-01-26T23:35:51.484+0000] {spark_submit.py:495} INFO - Batch: 15
[2023-01-26T23:35:51.486+0000] {spark_submit.py:495} INFO - -------------------------------------------
[2023-01-26T23:35:51.514+0000] {spark_submit.py:495} INFO - +-------------+--------------------+--------+-------------+--------------------+-------------------+------------+-----------------+--------------------+-------------+
[2023-01-26T23:35:51.514+0000] {spark_submit.py:495} INFO - |           id|       nameOfProduct|   price|ProductStatus|          SellerName|   RegistrationDate|QuantitySold|QuantityAvailable|      Free shipping?|Store ratings|
[2023-01-26T23:35:51.515+0000] {spark_submit.py:495} INFO - +-------------+--------------------+--------+-------------+--------------------+-------------------+------------+-----------------+--------------------+-------------+
[2023-01-26T23:35:51.515+0000] {spark_submit.py:495} INFO - |MLA1220964588|Procesador Gamer ...| 45899.0|          new|    OVERHARD DIGITAL|2015-01-08 12:19:41|          50|                1|Your shipment arr...|          4.5|
[2023-01-26T23:35:51.515+0000] {spark_submit.py:495} INFO - |MLA1157512489|Placa De Video Nv...|124999.0|          new|      PCREGISTRADAOK|2020-11-02 13:17:33|          50|                1|Your shipment arr...|          4.5|
[2023-01-26T23:35:51.515+0000] {spark_submit.py:495} INFO - |MLA1221192372|Disco Duro Extern...| 26299.0|          new|    OVERHARD DIGITAL|2015-01-08 12:19:41|         250|              100|Your shipment arr...|          4.5|
[2023-01-26T23:35:51.515+0000] {spark_submit.py:495} INFO - |MLA1163238194|Placa De Video Am...| 33999.0|          new|       MICYBERCOMPRA|2011-09-13 13:15:53|          50|                1|Your shipment arr...|          4.5|
[2023-01-26T23:35:51.515+0000] {spark_submit.py:495} INFO - |MLA1160204806|Memoria Ram Fury ...| 15499.0|          new|    OVERHARD DIGITAL|2015-01-08 12:19:41|         500|               50|Your shipment arr...|          4.5|
[2023-01-26T23:35:51.515+0000] {spark_submit.py:495} INFO - |MLA1115295971|Procesador Amd Ry...| 93999.0|          new|      PCREGISTRADAOK|2020-11-02 13:17:33|          25|                1|Your shipment arr...|          4.5|
[2023-01-26T23:35:51.515+0000] {spark_submit.py:495} INFO - |MLA1318244331|Disco Solido Ssd ...| 28799.0|          new|    OVERHARD DIGITAL|2015-01-08 12:19:41|           5|              150|Your shipment arr...|          4.5|
[2023-01-26T23:35:51.515+0000] {spark_submit.py:495} INFO - | MLA923637669|Placa De Video Ge...| 28699.0|          new|         INDIGOTRADE|2018-01-05 19:21:16|         500|               50|Your shipment arr...|          4.5|
[2023-01-26T23:35:51.515+0000] {spark_submit.py:495} INFO - |MLA1133481354|Disco Sólido Inte...| 13799.0|          new|           DALECLÍCK|2015-03-16 12:07:23|         500|                1|You have to pay t...|          4.5|
[2023-01-26T23:35:51.515+0000] {spark_submit.py:495} INFO - |MLA1139583771|Disco Sólido Inte...|  8339.0|          new|DIAMONDSYSTEMPADUAML|2015-07-01 12:32:22|         250|              250|You have to pay t...|          4.5|
[2023-01-26T23:35:51.516+0000] {spark_submit.py:495} INFO - | MLA882277177|Capturadora Video...|  2699.0|          new|         BSK-IMPORTS|2019-12-30 23:07:04|         500|               50|You have to pay t...|          3.0|
[2023-01-26T23:35:51.516+0000] {spark_submit.py:495} INFO - |MLA1148481665|Microprocesador P...| 58899.0|          new|            MALL WEB|2016-05-26 20:28:35|         150|                1|Your shipment arr...|          4.5|
[2023-01-26T23:35:51.516+0000] {spark_submit.py:495} INFO - |MLA1228990119|Memoria Ram Fury ...| 23255.0|          new|MARSTECHSRLMARSTE...|2020-08-05 14:27:59|          50|                1|Your shipment arr...|          4.5|
[2023-01-26T23:35:51.516+0000] {spark_submit.py:495} INFO - | MLA844489972|Disco Duro Intern...| 11599.0|          new|           DALECLÍCK|2015-03-16 12:07:23|        5000|              500|You have to pay t...|          5.0|
[2023-01-26T23:35:51.516+0000] {spark_submit.py:495} INFO - |MLA1183925519|Placa De Video Nv...|299999.0|          new|ESTUDIOSMPSRLESTU...|2020-11-29 18:36:32|          50|                1|Your shipment arr...|          4.5|
[2023-01-26T23:35:51.516+0000] {spark_submit.py:495} INFO - |MLA1168362675|Memoria Ram Fury ...| 26698.0|          new|MARSTECHSRLMARSTE...|2020-08-05 14:27:59|         150|                1|Your shipment arr...|          4.5|
[2023-01-26T23:35:51.516+0000] {spark_submit.py:495} INFO - |MLA1314590445|Disco Sólido Inte...|  7999.0|          new|       ELECTRO-SHOWS|2010-03-26 01:08:16|         250|              250|You have to pay t...|          4.5|
[2023-01-26T23:35:51.516+0000] {spark_submit.py:495} INFO - |MLA1163233412|Placa De Video Nv...| 34659.0|          new|      PCREGISTRADAOK|2020-11-02 13:17:33|           1|                1|Your shipment arr...|          4.5|
[2023-01-26T23:35:51.516+0000] {spark_submit.py:495} INFO - |MLA1302136118|Disco Duro Intern...| 18271.0|          new|DIAMONDSYSTEMSANJ...|2016-04-08 17:21:06|         100|               50|Your shipment arr...|          4.5|
[2023-01-26T23:35:51.516+0000] {spark_submit.py:495} INFO - |MLA1137238635|Fuente De Aliment...|  3235.0|          new|     DIAMONDSYSTEMML|2014-04-07 22:00:36|          25|                1|You have to pay t...|          3.5|
[2023-01-26T23:35:51.516+0000] {spark_submit.py:495} INFO - +-------------+--------------------+--------+-------------+--------------------+-------------------+------------+-----------------+--------------------+-------------+
[2023-01-26T23:35:51.516+0000] {spark_submit.py:495} INFO - only showing top 20 rows
[2023-01-26T23:35:51.517+0000] {spark_submit.py:495} INFO - 
[2023-01-26T23:36:17.356+0000] {spark_submit.py:495} INFO - 23/01/26 23:36:17 ERROR Executor: Exception in task 0.0 in stage 43.0 (TID 43)
[2023-01-26T23:36:17.356+0000] {spark_submit.py:495} INFO - com.mongodb.MongoTimeoutException: Timed out after 30000 ms while waiting to connect. Client view of cluster state is {type=UNKNOWN, servers=[{address=172.22.0.2:27017, type=UNKNOWN, state=CONNECTING, exception={com.mongodb.MongoSocketOpenException: Exception opening socket}, caused by {java.net.SocketTimeoutException: connect timed out}}]
[2023-01-26T23:36:17.356+0000] {spark_submit.py:495} INFO - at com.mongodb.internal.connection.BaseCluster.getDescription(BaseCluster.java:177)
[2023-01-26T23:36:17.356+0000] {spark_submit.py:495} INFO - at com.mongodb.internal.connection.SingleServerCluster.getDescription(SingleServerCluster.java:41)
[2023-01-26T23:36:17.356+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate.getConnectedClusterDescription(MongoClientDelegate.java:147)
[2023-01-26T23:36:17.356+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate.createClientSession(MongoClientDelegate.java:98)
[2023-01-26T23:36:17.356+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.getClientSession(MongoClientDelegate.java:278)
[2023-01-26T23:36:17.356+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:202)
[2023-01-26T23:36:17.357+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:441)
[2023-01-26T23:36:17.357+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:421)
[2023-01-26T23:36:17.357+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$4(MongoSpark.scala:138)
[2023-01-26T23:36:17.357+0000] {spark_submit.py:495} INFO - at scala.collection.Iterator.foreach(Iterator.scala:943)
[2023-01-26T23:36:17.357+0000] {spark_submit.py:495} INFO - at scala.collection.Iterator.foreach$(Iterator.scala:943)
[2023-01-26T23:36:17.357+0000] {spark_submit.py:495} INFO - at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
[2023-01-26T23:36:17.357+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$3(MongoSpark.scala:122)
[2023-01-26T23:36:17.357+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$3$adapted(MongoSpark.scala:121)
[2023-01-26T23:36:17.357+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.$anonfun$withCollectionDo$1(MongoConnector.scala:186)
[2023-01-26T23:36:17.357+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.$anonfun$withDatabaseDo$1(MongoConnector.scala:171)
[2023-01-26T23:36:17.357+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withMongoClientDo(MongoConnector.scala:154)
[2023-01-26T23:36:17.357+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withDatabaseDo(MongoConnector.scala:171)
[2023-01-26T23:36:17.358+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withCollectionDo(MongoConnector.scala:184)
[2023-01-26T23:36:17.358+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$2(MongoSpark.scala:121)
[2023-01-26T23:36:17.358+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$2$adapted(MongoSpark.scala:120)
[2023-01-26T23:36:17.358+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1011)
[2023-01-26T23:36:17.358+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1011)
[2023-01-26T23:36:17.358+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2268)
[2023-01-26T23:36:17.358+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
[2023-01-26T23:36:17.358+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.Task.run(Task.scala:136)
[2023-01-26T23:36:17.358+0000] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)
[2023-01-26T23:36:17.358+0000] {spark_submit.py:495} INFO - at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)
[2023-01-26T23:36:17.358+0000] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)
[2023-01-26T23:36:17.359+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[2023-01-26T23:36:17.359+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[2023-01-26T23:36:17.359+0000] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2023-01-26T23:36:17.359+0000] {spark_submit.py:495} INFO - 23/01/26 23:36:17 ERROR TaskSetManager: Task 0 in stage 43.0 failed 1 times; aborting job
[2023-01-26T23:36:17.509+0000] {spark_submit.py:495} INFO - [[34m2023-01-26 23:36:17,509[0m] {[34mconsumerSpark.py:[0m37} ERROR[0m - Error al escribir en Mongo: An error occurred while calling o374.save.
[2023-01-26T23:36:17.510+0000] {spark_submit.py:495} INFO - : org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 43.0 failed 1 times, most recent failure: Lost task 0.0 in stage 43.0 (TID 43) (f1cbb5df32e6 executor driver): com.mongodb.MongoTimeoutException: Timed out after 30000 ms while waiting to connect. Client view of cluster state is {type=UNKNOWN, servers=[{address=172.22.0.2:27017, type=UNKNOWN, state=CONNECTING, exception={com.mongodb.MongoSocketOpenException: Exception opening socket}, caused by {java.net.SocketTimeoutException: connect timed out}}]
[2023-01-26T23:36:17.510+0000] {spark_submit.py:495} INFO - at com.mongodb.internal.connection.BaseCluster.getDescription(BaseCluster.java:177)
[2023-01-26T23:36:17.511+0000] {spark_submit.py:495} INFO - at com.mongodb.internal.connection.SingleServerCluster.getDescription(SingleServerCluster.java:41)
[2023-01-26T23:36:17.512+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate.getConnectedClusterDescription(MongoClientDelegate.java:147)
[2023-01-26T23:36:17.512+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate.createClientSession(MongoClientDelegate.java:98)
[2023-01-26T23:36:17.513+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.getClientSession(MongoClientDelegate.java:278)
[2023-01-26T23:36:17.513+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:202)
[2023-01-26T23:36:17.514+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:441)
[2023-01-26T23:36:17.514+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:421)
[2023-01-26T23:36:17.514+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$4(MongoSpark.scala:138)
[2023-01-26T23:36:17.515+0000] {spark_submit.py:495} INFO - at scala.collection.Iterator.foreach(Iterator.scala:943)
[2023-01-26T23:36:17.515+0000] {spark_submit.py:495} INFO - at scala.collection.Iterator.foreach$(Iterator.scala:943)
[2023-01-26T23:36:17.516+0000] {spark_submit.py:495} INFO - at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
[2023-01-26T23:36:17.516+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$3(MongoSpark.scala:122)
[2023-01-26T23:36:17.516+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$3$adapted(MongoSpark.scala:121)
[2023-01-26T23:36:17.517+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.$anonfun$withCollectionDo$1(MongoConnector.scala:186)
[2023-01-26T23:36:17.517+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.$anonfun$withDatabaseDo$1(MongoConnector.scala:171)
[2023-01-26T23:36:17.517+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withMongoClientDo(MongoConnector.scala:154)
[2023-01-26T23:36:17.518+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withDatabaseDo(MongoConnector.scala:171)
[2023-01-26T23:36:17.518+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withCollectionDo(MongoConnector.scala:184)
[2023-01-26T23:36:17.519+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$2(MongoSpark.scala:121)
[2023-01-26T23:36:17.519+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$2$adapted(MongoSpark.scala:120)
[2023-01-26T23:36:17.519+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1011)
[2023-01-26T23:36:17.520+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1011)
[2023-01-26T23:36:17.520+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2268)
[2023-01-26T23:36:17.521+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
[2023-01-26T23:36:17.521+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.Task.run(Task.scala:136)
[2023-01-26T23:36:17.521+0000] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)
[2023-01-26T23:36:17.522+0000] {spark_submit.py:495} INFO - at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)
[2023-01-26T23:36:17.522+0000] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)
[2023-01-26T23:36:17.523+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[2023-01-26T23:36:17.523+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[2023-01-26T23:36:17.523+0000] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2023-01-26T23:36:17.524+0000] {spark_submit.py:495} INFO - 
[2023-01-26T23:36:17.524+0000] {spark_submit.py:495} INFO - Driver stacktrace:
[2023-01-26T23:36:17.524+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)
[2023-01-26T23:36:17.525+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)
[2023-01-26T23:36:17.525+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)
[2023-01-26T23:36:17.526+0000] {spark_submit.py:495} INFO - at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
[2023-01-26T23:36:17.526+0000] {spark_submit.py:495} INFO - at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
[2023-01-26T23:36:17.526+0000] {spark_submit.py:495} INFO - at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
[2023-01-26T23:36:17.527+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)
[2023-01-26T23:36:17.527+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)
[2023-01-26T23:36:17.527+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)
[2023-01-26T23:36:17.528+0000] {spark_submit.py:495} INFO - at scala.Option.foreach(Option.scala:407)
[2023-01-26T23:36:17.528+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)
[2023-01-26T23:36:17.528+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)
[2023-01-26T23:36:17.529+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)
[2023-01-26T23:36:17.529+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)
[2023-01-26T23:36:17.530+0000] {spark_submit.py:495} INFO - at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
[2023-01-26T23:36:17.530+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)
[2023-01-26T23:36:17.530+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.runJob(SparkContext.scala:2228)
[2023-01-26T23:36:17.531+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.runJob(SparkContext.scala:2249)
[2023-01-26T23:36:17.531+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.runJob(SparkContext.scala:2268)
[2023-01-26T23:36:17.532+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.runJob(SparkContext.scala:2293)
[2023-01-26T23:36:17.532+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$1(RDD.scala:1011)
[2023-01-26T23:36:17.532+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
[2023-01-26T23:36:17.532+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
[2023-01-26T23:36:17.533+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.withScope(RDD.scala:406)
[2023-01-26T23:36:17.533+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.foreachPartition(RDD.scala:1009)
[2023-01-26T23:36:17.533+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.save(MongoSpark.scala:120)
[2023-01-26T23:36:17.533+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.save(MongoSpark.scala:169)
[2023-01-26T23:36:17.534+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.sql.DefaultSource.createRelation(DefaultSource.scala:70)
[2023-01-26T23:36:17.534+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:47)
[2023-01-26T23:36:17.534+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)
[2023-01-26T23:36:17.534+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)
[2023-01-26T23:36:17.535+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)
[2023-01-26T23:36:17.535+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)
[2023-01-26T23:36:17.535+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)
[2023-01-26T23:36:17.535+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)
[2023-01-26T23:36:17.535+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)
[2023-01-26T23:36:17.536+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
[2023-01-26T23:36:17.536+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
[2023-01-26T23:36:17.536+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)
[2023-01-26T23:36:17.536+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)
[2023-01-26T23:36:17.537+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:584)
[2023-01-26T23:36:17.537+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:176)
[2023-01-26T23:36:17.537+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:584)
[2023-01-26T23:36:17.537+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)
[2023-01-26T23:36:17.537+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
[2023-01-26T23:36:17.538+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
[2023-01-26T23:36:17.538+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
[2023-01-26T23:36:17.538+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
[2023-01-26T23:36:17.538+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:560)
[2023-01-26T23:36:17.538+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)
[2023-01-26T23:36:17.538+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)
[2023-01-26T23:36:17.539+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)
[2023-01-26T23:36:17.539+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:116)
[2023-01-26T23:36:17.539+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:860)
[2023-01-26T23:36:17.539+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:390)
[2023-01-26T23:36:17.539+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:363)
[2023-01-26T23:36:17.540+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:247)
[2023-01-26T23:36:17.540+0000] {spark_submit.py:495} INFO - at jdk.internal.reflect.GeneratedMethodAccessor69.invoke(Unknown Source)
[2023-01-26T23:36:17.540+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
[2023-01-26T23:36:17.540+0000] {spark_submit.py:495} INFO - at java.base/java.lang.reflect.Method.invoke(Method.java:566)
[2023-01-26T23:36:17.540+0000] {spark_submit.py:495} INFO - at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
[2023-01-26T23:36:17.540+0000] {spark_submit.py:495} INFO - at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
[2023-01-26T23:36:17.541+0000] {spark_submit.py:495} INFO - at py4j.Gateway.invoke(Gateway.java:282)
[2023-01-26T23:36:17.541+0000] {spark_submit.py:495} INFO - at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
[2023-01-26T23:36:17.541+0000] {spark_submit.py:495} INFO - at py4j.commands.CallCommand.execute(CallCommand.java:79)
[2023-01-26T23:36:17.541+0000] {spark_submit.py:495} INFO - at py4j.ClientServerConnection.sendCommand(ClientServerConnection.java:244)
[2023-01-26T23:36:17.541+0000] {spark_submit.py:495} INFO - at py4j.CallbackClient.sendCommand(CallbackClient.java:384)
[2023-01-26T23:36:17.541+0000] {spark_submit.py:495} INFO - at py4j.CallbackClient.sendCommand(CallbackClient.java:356)
[2023-01-26T23:36:17.542+0000] {spark_submit.py:495} INFO - at py4j.reflection.PythonProxyHandler.invoke(PythonProxyHandler.java:106)
[2023-01-26T23:36:17.542+0000] {spark_submit.py:495} INFO - at com.sun.proxy.$Proxy32.call(Unknown Source)
[2023-01-26T23:36:17.542+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1(ForeachBatchSink.scala:51)
[2023-01-26T23:36:17.542+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1$adapted(ForeachBatchSink.scala:51)
[2023-01-26T23:36:17.542+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.sources.ForeachBatchSink.addBatch(ForeachBatchSink.scala:32)
[2023-01-26T23:36:17.542+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:666)
[2023-01-26T23:36:17.542+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)
[2023-01-26T23:36:17.543+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)
[2023-01-26T23:36:17.543+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)
[2023-01-26T23:36:17.543+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
[2023-01-26T23:36:17.543+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
[2023-01-26T23:36:17.543+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:664)
[2023-01-26T23:36:17.543+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:375)
[2023-01-26T23:36:17.543+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:373)
[2023-01-26T23:36:17.544+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:68)
[2023-01-26T23:36:17.544+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:664)
[2023-01-26T23:36:17.544+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:256)
[2023-01-26T23:36:17.544+0000] {spark_submit.py:495} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2023-01-26T23:36:17.544+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:375)
[2023-01-26T23:36:17.544+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:373)
[2023-01-26T23:36:17.544+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:68)
[2023-01-26T23:36:17.544+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:219)
[2023-01-26T23:36:17.545+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)
[2023-01-26T23:36:17.545+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:213)
[2023-01-26T23:36:17.545+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:307)
[2023-01-26T23:36:17.545+0000] {spark_submit.py:495} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2023-01-26T23:36:17.545+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
[2023-01-26T23:36:17.545+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:285)
[2023-01-26T23:36:17.545+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:208)
[2023-01-26T23:36:17.546+0000] {spark_submit.py:495} INFO - Caused by: com.mongodb.MongoTimeoutException: Timed out after 30000 ms while waiting to connect. Client view of cluster state is {type=UNKNOWN, servers=[{address=172.22.0.2:27017, type=UNKNOWN, state=CONNECTING, exception={com.mongodb.MongoSocketOpenException: Exception opening socket}, caused by {java.net.SocketTimeoutException: connect timed out}}]
[2023-01-26T23:36:17.546+0000] {spark_submit.py:495} INFO - at com.mongodb.internal.connection.BaseCluster.getDescription(BaseCluster.java:177)
[2023-01-26T23:36:17.546+0000] {spark_submit.py:495} INFO - at com.mongodb.internal.connection.SingleServerCluster.getDescription(SingleServerCluster.java:41)
[2023-01-26T23:36:17.546+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate.getConnectedClusterDescription(MongoClientDelegate.java:147)
[2023-01-26T23:36:17.546+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate.createClientSession(MongoClientDelegate.java:98)
[2023-01-26T23:36:17.546+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.getClientSession(MongoClientDelegate.java:278)
[2023-01-26T23:36:17.546+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:202)
[2023-01-26T23:36:17.547+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:441)
[2023-01-26T23:36:17.547+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:421)
[2023-01-26T23:36:17.547+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$4(MongoSpark.scala:138)
[2023-01-26T23:36:17.547+0000] {spark_submit.py:495} INFO - at scala.collection.Iterator.foreach(Iterator.scala:943)
[2023-01-26T23:36:17.547+0000] {spark_submit.py:495} INFO - at scala.collection.Iterator.foreach$(Iterator.scala:943)
[2023-01-26T23:36:17.547+0000] {spark_submit.py:495} INFO - at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
[2023-01-26T23:36:17.547+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$3(MongoSpark.scala:122)
[2023-01-26T23:36:17.547+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$3$adapted(MongoSpark.scala:121)
[2023-01-26T23:36:17.547+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.$anonfun$withCollectionDo$1(MongoConnector.scala:186)
[2023-01-26T23:36:17.548+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.$anonfun$withDatabaseDo$1(MongoConnector.scala:171)
[2023-01-26T23:36:17.548+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withMongoClientDo(MongoConnector.scala:154)
[2023-01-26T23:36:17.548+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withDatabaseDo(MongoConnector.scala:171)
[2023-01-26T23:36:17.548+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withCollectionDo(MongoConnector.scala:184)
[2023-01-26T23:36:17.548+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$2(MongoSpark.scala:121)
[2023-01-26T23:36:17.548+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$2$adapted(MongoSpark.scala:120)
[2023-01-26T23:36:17.548+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1011)
[2023-01-26T23:36:17.548+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1011)
[2023-01-26T23:36:17.549+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2268)
[2023-01-26T23:36:17.549+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
[2023-01-26T23:36:17.549+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.Task.run(Task.scala:136)
[2023-01-26T23:36:17.549+0000] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)
[2023-01-26T23:36:17.549+0000] {spark_submit.py:495} INFO - at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)
[2023-01-26T23:36:17.549+0000] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)
[2023-01-26T23:36:17.549+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[2023-01-26T23:36:17.549+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[2023-01-26T23:36:17.549+0000] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2023-01-26T23:36:17.550+0000] {spark_submit.py:495} INFO - [0m
[2023-01-26T23:37:50.170+0000] {spark_submit.py:495} INFO - [[34m2023-01-26 23:37:50,170[0m] {[34mclientserver.py:[0m577} INFO[0m - Received command c on object id p0[0m
[2023-01-26T23:37:50.789+0000] {spark_submit.py:495} INFO - -------------------------------------------
[2023-01-26T23:37:50.790+0000] {spark_submit.py:495} INFO - Batch: 16
[2023-01-26T23:37:50.790+0000] {spark_submit.py:495} INFO - -------------------------------------------
[2023-01-26T23:37:50.819+0000] {spark_submit.py:495} INFO - +-------------+--------------------+--------+-------------+--------------------+-------------------+------------+-----------------+--------------------+-------------+
[2023-01-26T23:37:50.819+0000] {spark_submit.py:495} INFO - |           id|       nameOfProduct|   price|ProductStatus|          SellerName|   RegistrationDate|QuantitySold|QuantityAvailable|      Free shipping?|Store ratings|
[2023-01-26T23:37:50.819+0000] {spark_submit.py:495} INFO - +-------------+--------------------+--------+-------------+--------------------+-------------------+------------+-----------------+--------------------+-------------+
[2023-01-26T23:37:50.820+0000] {spark_submit.py:495} INFO - |MLA1220964588|Procesador Gamer ...| 45899.0|          new|    OVERHARD DIGITAL|2015-01-08 12:19:41|          50|                1|Your shipment arr...|          4.5|
[2023-01-26T23:37:50.820+0000] {spark_submit.py:495} INFO - |MLA1157512489|Placa De Video Nv...|124999.0|          new|      PCREGISTRADAOK|2020-11-02 13:17:33|          50|                1|Your shipment arr...|          4.5|
[2023-01-26T23:37:50.820+0000] {spark_submit.py:495} INFO - |MLA1221192372|Disco Duro Extern...| 26299.0|          new|    OVERHARD DIGITAL|2015-01-08 12:19:41|         250|              100|Your shipment arr...|          4.5|
[2023-01-26T23:37:50.820+0000] {spark_submit.py:495} INFO - |MLA1163238194|Placa De Video Am...| 33999.0|          new|       MICYBERCOMPRA|2011-09-13 13:15:53|          50|                1|Your shipment arr...|          4.5|
[2023-01-26T23:37:50.820+0000] {spark_submit.py:495} INFO - |MLA1160204806|Memoria Ram Fury ...| 15499.0|          new|    OVERHARD DIGITAL|2015-01-08 12:19:41|         500|               50|Your shipment arr...|          4.5|
[2023-01-26T23:37:50.820+0000] {spark_submit.py:495} INFO - |MLA1115295971|Procesador Amd Ry...| 93999.0|          new|      PCREGISTRADAOK|2020-11-02 13:17:33|          25|                1|Your shipment arr...|          4.5|
[2023-01-26T23:37:50.820+0000] {spark_submit.py:495} INFO - |MLA1318244331|Disco Solido Ssd ...| 28799.0|          new|    OVERHARD DIGITAL|2015-01-08 12:19:41|           5|              150|Your shipment arr...|          4.5|
[2023-01-26T23:37:50.820+0000] {spark_submit.py:495} INFO - | MLA923637669|Placa De Video Ge...| 28699.0|          new|         INDIGOTRADE|2018-01-05 19:21:16|         500|               50|Your shipment arr...|          4.5|
[2023-01-26T23:37:50.820+0000] {spark_submit.py:495} INFO - |MLA1133481354|Disco Sólido Inte...| 13799.0|          new|           DALECLÍCK|2015-03-16 12:07:23|         500|                1|You have to pay t...|          4.5|
[2023-01-26T23:37:50.820+0000] {spark_submit.py:495} INFO - |MLA1139583771|Disco Sólido Inte...|  8339.0|          new|DIAMONDSYSTEMPADUAML|2015-07-01 12:32:22|         250|              250|You have to pay t...|          4.5|
[2023-01-26T23:37:50.820+0000] {spark_submit.py:495} INFO - | MLA882277177|Capturadora Video...|  2699.0|          new|         BSK-IMPORTS|2019-12-30 23:07:04|         500|               50|You have to pay t...|          3.0|
[2023-01-26T23:37:50.820+0000] {spark_submit.py:495} INFO - |MLA1148481665|Microprocesador P...| 58899.0|          new|            MALL WEB|2016-05-26 20:28:35|         150|                1|Your shipment arr...|          4.5|
[2023-01-26T23:37:50.820+0000] {spark_submit.py:495} INFO - |MLA1228990119|Memoria Ram Fury ...| 23255.0|          new|MARSTECHSRLMARSTE...|2020-08-05 14:27:59|          50|                1|Your shipment arr...|          4.5|
[2023-01-26T23:37:50.821+0000] {spark_submit.py:495} INFO - | MLA844489972|Disco Duro Intern...| 11599.0|          new|           DALECLÍCK|2015-03-16 12:07:23|        5000|              500|You have to pay t...|          5.0|
[2023-01-26T23:37:50.821+0000] {spark_submit.py:495} INFO - |MLA1183925519|Placa De Video Nv...|299999.0|          new|ESTUDIOSMPSRLESTU...|2020-11-29 18:36:32|          50|                1|Your shipment arr...|          4.5|
[2023-01-26T23:37:50.821+0000] {spark_submit.py:495} INFO - |MLA1168362675|Memoria Ram Fury ...| 26698.0|          new|MARSTECHSRLMARSTE...|2020-08-05 14:27:59|         150|                1|Your shipment arr...|          4.5|
[2023-01-26T23:37:50.821+0000] {spark_submit.py:495} INFO - |MLA1314590445|Disco Sólido Inte...|  7999.0|          new|       ELECTRO-SHOWS|2010-03-26 01:08:16|         250|              250|You have to pay t...|          4.5|
[2023-01-26T23:37:50.821+0000] {spark_submit.py:495} INFO - |MLA1163233412|Placa De Video Nv...| 34659.0|          new|      PCREGISTRADAOK|2020-11-02 13:17:33|           1|                1|Your shipment arr...|          4.5|
[2023-01-26T23:37:50.821+0000] {spark_submit.py:495} INFO - |MLA1302136118|Disco Duro Intern...| 18271.0|          new|DIAMONDSYSTEMSANJ...|2016-04-08 17:21:06|         100|               50|Your shipment arr...|          4.5|
[2023-01-26T23:37:50.821+0000] {spark_submit.py:495} INFO - |MLA1137238635|Fuente De Aliment...|  3235.0|          new|     DIAMONDSYSTEMML|2014-04-07 22:00:36|          25|                1|You have to pay t...|          3.5|
[2023-01-26T23:37:50.821+0000] {spark_submit.py:495} INFO - +-------------+--------------------+--------+-------------+--------------------+-------------------+------------+-----------------+--------------------+-------------+
[2023-01-26T23:37:50.821+0000] {spark_submit.py:495} INFO - only showing top 20 rows
[2023-01-26T23:37:50.822+0000] {spark_submit.py:495} INFO - 
[2023-01-26T23:38:20.830+0000] {spark_submit.py:495} INFO - 23/01/26 23:38:20 ERROR Executor: Exception in task 0.0 in stage 46.0 (TID 46)
[2023-01-26T23:38:20.830+0000] {spark_submit.py:495} INFO - com.mongodb.MongoTimeoutException: Timed out after 30000 ms while waiting to connect. Client view of cluster state is {type=UNKNOWN, servers=[{address=172.22.0.2:27017, type=UNKNOWN, state=CONNECTING, exception={com.mongodb.MongoSocketOpenException: Exception opening socket}, caused by {java.net.SocketTimeoutException: connect timed out}}]
[2023-01-26T23:38:20.831+0000] {spark_submit.py:495} INFO - at com.mongodb.internal.connection.BaseCluster.getDescription(BaseCluster.java:177)
[2023-01-26T23:38:20.831+0000] {spark_submit.py:495} INFO - at com.mongodb.internal.connection.SingleServerCluster.getDescription(SingleServerCluster.java:41)
[2023-01-26T23:38:20.831+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate.getConnectedClusterDescription(MongoClientDelegate.java:147)
[2023-01-26T23:38:20.831+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate.createClientSession(MongoClientDelegate.java:98)
[2023-01-26T23:38:20.831+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.getClientSession(MongoClientDelegate.java:278)
[2023-01-26T23:38:20.832+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:202)
[2023-01-26T23:38:20.832+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:441)
[2023-01-26T23:38:20.832+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:421)
[2023-01-26T23:38:20.832+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$4(MongoSpark.scala:138)
[2023-01-26T23:38:20.832+0000] {spark_submit.py:495} INFO - at scala.collection.Iterator.foreach(Iterator.scala:943)
[2023-01-26T23:38:20.832+0000] {spark_submit.py:495} INFO - at scala.collection.Iterator.foreach$(Iterator.scala:943)
[2023-01-26T23:38:20.832+0000] {spark_submit.py:495} INFO - at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
[2023-01-26T23:38:20.833+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$3(MongoSpark.scala:122)
[2023-01-26T23:38:20.833+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$3$adapted(MongoSpark.scala:121)
[2023-01-26T23:38:20.833+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.$anonfun$withCollectionDo$1(MongoConnector.scala:186)
[2023-01-26T23:38:20.833+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.$anonfun$withDatabaseDo$1(MongoConnector.scala:171)
[2023-01-26T23:38:20.833+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withMongoClientDo(MongoConnector.scala:154)
[2023-01-26T23:38:20.833+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withDatabaseDo(MongoConnector.scala:171)
[2023-01-26T23:38:20.833+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withCollectionDo(MongoConnector.scala:184)
[2023-01-26T23:38:20.834+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$2(MongoSpark.scala:121)
[2023-01-26T23:38:20.834+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$2$adapted(MongoSpark.scala:120)
[2023-01-26T23:38:20.834+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1011)
[2023-01-26T23:38:20.834+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1011)
[2023-01-26T23:38:20.834+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2268)
[2023-01-26T23:38:20.834+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
[2023-01-26T23:38:20.834+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.Task.run(Task.scala:136)
[2023-01-26T23:38:20.835+0000] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)
[2023-01-26T23:38:20.835+0000] {spark_submit.py:495} INFO - at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)
[2023-01-26T23:38:20.835+0000] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)
[2023-01-26T23:38:20.835+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[2023-01-26T23:38:20.835+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[2023-01-26T23:38:20.835+0000] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2023-01-26T23:38:20.835+0000] {spark_submit.py:495} INFO - 23/01/26 23:38:20 ERROR TaskSetManager: Task 0 in stage 46.0 failed 1 times; aborting job
[2023-01-26T23:38:20.957+0000] {spark_submit.py:495} INFO - [[34m2023-01-26 23:38:20,957[0m] {[34mconsumerSpark.py:[0m37} ERROR[0m - Error al escribir en Mongo: An error occurred while calling o383.save.
[2023-01-26T23:38:20.958+0000] {spark_submit.py:495} INFO - : org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 46.0 failed 1 times, most recent failure: Lost task 0.0 in stage 46.0 (TID 46) (f1cbb5df32e6 executor driver): com.mongodb.MongoTimeoutException: Timed out after 30000 ms while waiting to connect. Client view of cluster state is {type=UNKNOWN, servers=[{address=172.22.0.2:27017, type=UNKNOWN, state=CONNECTING, exception={com.mongodb.MongoSocketOpenException: Exception opening socket}, caused by {java.net.SocketTimeoutException: connect timed out}}]
[2023-01-26T23:38:20.959+0000] {spark_submit.py:495} INFO - at com.mongodb.internal.connection.BaseCluster.getDescription(BaseCluster.java:177)
[2023-01-26T23:38:20.959+0000] {spark_submit.py:495} INFO - at com.mongodb.internal.connection.SingleServerCluster.getDescription(SingleServerCluster.java:41)
[2023-01-26T23:38:20.959+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate.getConnectedClusterDescription(MongoClientDelegate.java:147)
[2023-01-26T23:38:20.960+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate.createClientSession(MongoClientDelegate.java:98)
[2023-01-26T23:38:20.960+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.getClientSession(MongoClientDelegate.java:278)
[2023-01-26T23:38:20.960+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:202)
[2023-01-26T23:38:20.961+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:441)
[2023-01-26T23:38:20.961+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:421)
[2023-01-26T23:38:20.961+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$4(MongoSpark.scala:138)
[2023-01-26T23:38:20.962+0000] {spark_submit.py:495} INFO - at scala.collection.Iterator.foreach(Iterator.scala:943)
[2023-01-26T23:38:20.962+0000] {spark_submit.py:495} INFO - at scala.collection.Iterator.foreach$(Iterator.scala:943)
[2023-01-26T23:38:20.962+0000] {spark_submit.py:495} INFO - at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
[2023-01-26T23:38:20.963+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$3(MongoSpark.scala:122)
[2023-01-26T23:38:20.964+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$3$adapted(MongoSpark.scala:121)
[2023-01-26T23:38:20.964+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.$anonfun$withCollectionDo$1(MongoConnector.scala:186)
[2023-01-26T23:38:20.965+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.$anonfun$withDatabaseDo$1(MongoConnector.scala:171)
[2023-01-26T23:38:20.965+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withMongoClientDo(MongoConnector.scala:154)
[2023-01-26T23:38:20.965+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withDatabaseDo(MongoConnector.scala:171)
[2023-01-26T23:38:20.966+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withCollectionDo(MongoConnector.scala:184)
[2023-01-26T23:38:20.966+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$2(MongoSpark.scala:121)
[2023-01-26T23:38:20.966+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$2$adapted(MongoSpark.scala:120)
[2023-01-26T23:38:20.967+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1011)
[2023-01-26T23:38:20.967+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1011)
[2023-01-26T23:38:20.968+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2268)
[2023-01-26T23:38:20.968+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
[2023-01-26T23:38:20.968+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.Task.run(Task.scala:136)
[2023-01-26T23:38:20.969+0000] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)
[2023-01-26T23:38:20.969+0000] {spark_submit.py:495} INFO - at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)
[2023-01-26T23:38:20.970+0000] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)
[2023-01-26T23:38:20.970+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[2023-01-26T23:38:20.970+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[2023-01-26T23:38:20.971+0000] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2023-01-26T23:38:20.971+0000] {spark_submit.py:495} INFO - 
[2023-01-26T23:38:20.971+0000] {spark_submit.py:495} INFO - Driver stacktrace:
[2023-01-26T23:38:20.972+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)
[2023-01-26T23:38:20.972+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)
[2023-01-26T23:38:20.973+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)
[2023-01-26T23:38:20.973+0000] {spark_submit.py:495} INFO - at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
[2023-01-26T23:38:20.973+0000] {spark_submit.py:495} INFO - at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
[2023-01-26T23:38:20.974+0000] {spark_submit.py:495} INFO - at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
[2023-01-26T23:38:20.974+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)
[2023-01-26T23:38:20.974+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)
[2023-01-26T23:38:20.975+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)
[2023-01-26T23:38:20.975+0000] {spark_submit.py:495} INFO - at scala.Option.foreach(Option.scala:407)
[2023-01-26T23:38:20.975+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)
[2023-01-26T23:38:20.976+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)
[2023-01-26T23:38:20.976+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)
[2023-01-26T23:38:20.977+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)
[2023-01-26T23:38:20.977+0000] {spark_submit.py:495} INFO - at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
[2023-01-26T23:38:20.977+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)
[2023-01-26T23:38:20.978+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.runJob(SparkContext.scala:2228)
[2023-01-26T23:38:20.978+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.runJob(SparkContext.scala:2249)
[2023-01-26T23:38:20.980+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.runJob(SparkContext.scala:2268)
[2023-01-26T23:38:20.981+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.runJob(SparkContext.scala:2293)
[2023-01-26T23:38:20.981+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$1(RDD.scala:1011)
[2023-01-26T23:38:20.982+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
[2023-01-26T23:38:20.982+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
[2023-01-26T23:38:20.982+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.withScope(RDD.scala:406)
[2023-01-26T23:38:20.983+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.foreachPartition(RDD.scala:1009)
[2023-01-26T23:38:20.983+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.save(MongoSpark.scala:120)
[2023-01-26T23:38:20.984+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.save(MongoSpark.scala:169)
[2023-01-26T23:38:20.984+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.sql.DefaultSource.createRelation(DefaultSource.scala:70)
[2023-01-26T23:38:20.984+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:47)
[2023-01-26T23:38:20.985+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)
[2023-01-26T23:38:20.985+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)
[2023-01-26T23:38:20.986+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)
[2023-01-26T23:38:20.986+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)
[2023-01-26T23:38:20.986+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)
[2023-01-26T23:38:20.987+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)
[2023-01-26T23:38:20.987+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)
[2023-01-26T23:38:20.987+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
[2023-01-26T23:38:20.988+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
[2023-01-26T23:38:20.988+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)
[2023-01-26T23:38:20.988+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)
[2023-01-26T23:38:20.988+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:584)
[2023-01-26T23:38:20.990+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:176)
[2023-01-26T23:38:20.990+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:584)
[2023-01-26T23:38:20.990+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)
[2023-01-26T23:38:20.991+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
[2023-01-26T23:38:20.991+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
[2023-01-26T23:38:20.991+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
[2023-01-26T23:38:20.992+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
[2023-01-26T23:38:20.992+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:560)
[2023-01-26T23:38:20.992+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)
[2023-01-26T23:38:20.992+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)
[2023-01-26T23:38:20.993+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)
[2023-01-26T23:38:20.993+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:116)
[2023-01-26T23:38:20.993+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:860)
[2023-01-26T23:38:20.993+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:390)
[2023-01-26T23:38:20.993+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:363)
[2023-01-26T23:38:20.993+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:247)
[2023-01-26T23:38:20.994+0000] {spark_submit.py:495} INFO - at jdk.internal.reflect.GeneratedMethodAccessor69.invoke(Unknown Source)
[2023-01-26T23:38:20.994+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
[2023-01-26T23:38:20.994+0000] {spark_submit.py:495} INFO - at java.base/java.lang.reflect.Method.invoke(Method.java:566)
[2023-01-26T23:38:20.994+0000] {spark_submit.py:495} INFO - at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
[2023-01-26T23:38:20.994+0000] {spark_submit.py:495} INFO - at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
[2023-01-26T23:38:20.995+0000] {spark_submit.py:495} INFO - at py4j.Gateway.invoke(Gateway.java:282)
[2023-01-26T23:38:20.995+0000] {spark_submit.py:495} INFO - at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
[2023-01-26T23:38:20.995+0000] {spark_submit.py:495} INFO - at py4j.commands.CallCommand.execute(CallCommand.java:79)
[2023-01-26T23:38:20.995+0000] {spark_submit.py:495} INFO - at py4j.ClientServerConnection.sendCommand(ClientServerConnection.java:244)
[2023-01-26T23:38:20.995+0000] {spark_submit.py:495} INFO - at py4j.CallbackClient.sendCommand(CallbackClient.java:384)
[2023-01-26T23:38:20.995+0000] {spark_submit.py:495} INFO - at py4j.CallbackClient.sendCommand(CallbackClient.java:356)
[2023-01-26T23:38:20.996+0000] {spark_submit.py:495} INFO - at py4j.reflection.PythonProxyHandler.invoke(PythonProxyHandler.java:106)
[2023-01-26T23:38:20.996+0000] {spark_submit.py:495} INFO - at com.sun.proxy.$Proxy32.call(Unknown Source)
[2023-01-26T23:38:20.996+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1(ForeachBatchSink.scala:51)
[2023-01-26T23:38:20.996+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1$adapted(ForeachBatchSink.scala:51)
[2023-01-26T23:38:20.997+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.sources.ForeachBatchSink.addBatch(ForeachBatchSink.scala:32)
[2023-01-26T23:38:20.997+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:666)
[2023-01-26T23:38:20.997+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)
[2023-01-26T23:38:20.997+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)
[2023-01-26T23:38:20.997+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)
[2023-01-26T23:38:20.998+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
[2023-01-26T23:38:20.998+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
[2023-01-26T23:38:20.998+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:664)
[2023-01-26T23:38:20.998+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:375)
[2023-01-26T23:38:20.998+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:373)
[2023-01-26T23:38:20.998+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:68)
[2023-01-26T23:38:20.998+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:664)
[2023-01-26T23:38:20.998+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:256)
[2023-01-26T23:38:20.999+0000] {spark_submit.py:495} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2023-01-26T23:38:20.999+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:375)
[2023-01-26T23:38:20.999+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:373)
[2023-01-26T23:38:20.999+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:68)
[2023-01-26T23:38:20.999+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:219)
[2023-01-26T23:38:20.999+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)
[2023-01-26T23:38:20.999+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:213)
[2023-01-26T23:38:21.002+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:307)
[2023-01-26T23:38:21.002+0000] {spark_submit.py:495} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2023-01-26T23:38:21.002+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
[2023-01-26T23:38:21.003+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:285)
[2023-01-26T23:38:21.003+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:208)
[2023-01-26T23:38:21.003+0000] {spark_submit.py:495} INFO - Caused by: com.mongodb.MongoTimeoutException: Timed out after 30000 ms while waiting to connect. Client view of cluster state is {type=UNKNOWN, servers=[{address=172.22.0.2:27017, type=UNKNOWN, state=CONNECTING, exception={com.mongodb.MongoSocketOpenException: Exception opening socket}, caused by {java.net.SocketTimeoutException: connect timed out}}]
[2023-01-26T23:38:21.003+0000] {spark_submit.py:495} INFO - at com.mongodb.internal.connection.BaseCluster.getDescription(BaseCluster.java:177)
[2023-01-26T23:38:21.003+0000] {spark_submit.py:495} INFO - at com.mongodb.internal.connection.SingleServerCluster.getDescription(SingleServerCluster.java:41)
[2023-01-26T23:38:21.003+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate.getConnectedClusterDescription(MongoClientDelegate.java:147)
[2023-01-26T23:38:21.003+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate.createClientSession(MongoClientDelegate.java:98)
[2023-01-26T23:38:21.003+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.getClientSession(MongoClientDelegate.java:278)
[2023-01-26T23:38:21.003+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:202)
[2023-01-26T23:38:21.004+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:441)
[2023-01-26T23:38:21.004+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:421)
[2023-01-26T23:38:21.004+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$4(MongoSpark.scala:138)
[2023-01-26T23:38:21.004+0000] {spark_submit.py:495} INFO - at scala.collection.Iterator.foreach(Iterator.scala:943)
[2023-01-26T23:38:21.004+0000] {spark_submit.py:495} INFO - at scala.collection.Iterator.foreach$(Iterator.scala:943)
[2023-01-26T23:38:21.004+0000] {spark_submit.py:495} INFO - at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
[2023-01-26T23:38:21.004+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$3(MongoSpark.scala:122)
[2023-01-26T23:38:21.004+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$3$adapted(MongoSpark.scala:121)
[2023-01-26T23:38:21.004+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.$anonfun$withCollectionDo$1(MongoConnector.scala:186)
[2023-01-26T23:38:21.005+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.$anonfun$withDatabaseDo$1(MongoConnector.scala:171)
[2023-01-26T23:38:21.005+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withMongoClientDo(MongoConnector.scala:154)
[2023-01-26T23:38:21.005+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withDatabaseDo(MongoConnector.scala:171)
[2023-01-26T23:38:21.005+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withCollectionDo(MongoConnector.scala:184)
[2023-01-26T23:38:21.005+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$2(MongoSpark.scala:121)
[2023-01-26T23:38:21.005+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$2$adapted(MongoSpark.scala:120)
[2023-01-26T23:38:21.005+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1011)
[2023-01-26T23:38:21.005+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1011)
[2023-01-26T23:38:21.005+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2268)
[2023-01-26T23:38:21.006+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
[2023-01-26T23:38:21.006+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.Task.run(Task.scala:136)
[2023-01-26T23:38:21.006+0000] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)
[2023-01-26T23:38:21.006+0000] {spark_submit.py:495} INFO - at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)
[2023-01-26T23:38:21.006+0000] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)
[2023-01-26T23:38:21.006+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[2023-01-26T23:38:21.006+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[2023-01-26T23:38:21.006+0000] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2023-01-26T23:38:21.007+0000] {spark_submit.py:495} INFO - [0m
[2023-01-26T23:38:21.063+0000] {spark_submit.py:495} INFO - [[34m2023-01-26 23:38:21,063[0m] {[34mclientserver.py:[0m577} INFO[0m - Received command c on object id p0[0m
[2023-01-26T23:38:51.108+0000] {spark_submit.py:495} INFO - 23/01/26 23:38:51 ERROR Executor: Exception in task 0.0 in stage 47.0 (TID 47)
[2023-01-26T23:38:51.109+0000] {spark_submit.py:495} INFO - com.mongodb.MongoTimeoutException: Timed out after 30000 ms while waiting to connect. Client view of cluster state is {type=UNKNOWN, servers=[{address=172.22.0.2:27017, type=UNKNOWN, state=CONNECTING, exception={com.mongodb.MongoSocketOpenException: Exception opening socket}, caused by {java.net.SocketTimeoutException: connect timed out}}]
[2023-01-26T23:38:51.109+0000] {spark_submit.py:495} INFO - at com.mongodb.internal.connection.BaseCluster.getDescription(BaseCluster.java:177)
[2023-01-26T23:38:51.110+0000] {spark_submit.py:495} INFO - at com.mongodb.internal.connection.SingleServerCluster.getDescription(SingleServerCluster.java:41)
[2023-01-26T23:38:51.110+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate.getConnectedClusterDescription(MongoClientDelegate.java:147)
[2023-01-26T23:38:51.110+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate.createClientSession(MongoClientDelegate.java:98)
[2023-01-26T23:38:51.111+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.getClientSession(MongoClientDelegate.java:278)
[2023-01-26T23:38:51.111+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:202)
[2023-01-26T23:38:51.112+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:441)
[2023-01-26T23:38:51.112+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:421)
[2023-01-26T23:38:51.112+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$4(MongoSpark.scala:138)
[2023-01-26T23:38:51.113+0000] {spark_submit.py:495} INFO - at scala.collection.Iterator.foreach(Iterator.scala:943)
[2023-01-26T23:38:51.113+0000] {spark_submit.py:495} INFO - at scala.collection.Iterator.foreach$(Iterator.scala:943)
[2023-01-26T23:38:51.113+0000] {spark_submit.py:495} INFO - at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
[2023-01-26T23:38:51.114+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$3(MongoSpark.scala:122)
[2023-01-26T23:38:51.114+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$3$adapted(MongoSpark.scala:121)
[2023-01-26T23:38:51.114+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.$anonfun$withCollectionDo$1(MongoConnector.scala:186)
[2023-01-26T23:38:51.115+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.$anonfun$withDatabaseDo$1(MongoConnector.scala:171)
[2023-01-26T23:38:51.115+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withMongoClientDo(MongoConnector.scala:154)
[2023-01-26T23:38:51.116+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withDatabaseDo(MongoConnector.scala:171)
[2023-01-26T23:38:51.116+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withCollectionDo(MongoConnector.scala:184)
[2023-01-26T23:38:51.116+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$2(MongoSpark.scala:121)
[2023-01-26T23:38:51.117+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$2$adapted(MongoSpark.scala:120)
[2023-01-26T23:38:51.117+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1011)
[2023-01-26T23:38:51.118+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1011)
[2023-01-26T23:38:51.118+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2268)
[2023-01-26T23:38:51.119+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
[2023-01-26T23:38:51.119+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.Task.run(Task.scala:136)
[2023-01-26T23:38:51.120+0000] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)
[2023-01-26T23:38:51.120+0000] {spark_submit.py:495} INFO - at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)
[2023-01-26T23:38:51.120+0000] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)
[2023-01-26T23:38:51.121+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[2023-01-26T23:38:51.121+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[2023-01-26T23:38:51.121+0000] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2023-01-26T23:38:51.122+0000] {spark_submit.py:495} INFO - 23/01/26 23:38:51 ERROR TaskSetManager: Task 0 in stage 47.0 failed 1 times; aborting job
[2023-01-26T23:38:51.249+0000] {spark_submit.py:495} INFO - [[34m2023-01-26 23:38:51,248[0m] {[34mconsumerSpark.py:[0m37} ERROR[0m - Error al escribir en Mongo: An error occurred while calling o392.save.
[2023-01-26T23:38:51.249+0000] {spark_submit.py:495} INFO - : org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 47.0 failed 1 times, most recent failure: Lost task 0.0 in stage 47.0 (TID 47) (f1cbb5df32e6 executor driver): com.mongodb.MongoTimeoutException: Timed out after 30000 ms while waiting to connect. Client view of cluster state is {type=UNKNOWN, servers=[{address=172.22.0.2:27017, type=UNKNOWN, state=CONNECTING, exception={com.mongodb.MongoSocketOpenException: Exception opening socket}, caused by {java.net.SocketTimeoutException: connect timed out}}]
[2023-01-26T23:38:51.249+0000] {spark_submit.py:495} INFO - at com.mongodb.internal.connection.BaseCluster.getDescription(BaseCluster.java:177)
[2023-01-26T23:38:51.249+0000] {spark_submit.py:495} INFO - at com.mongodb.internal.connection.SingleServerCluster.getDescription(SingleServerCluster.java:41)
[2023-01-26T23:38:51.249+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate.getConnectedClusterDescription(MongoClientDelegate.java:147)
[2023-01-26T23:38:51.249+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate.createClientSession(MongoClientDelegate.java:98)
[2023-01-26T23:38:51.249+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.getClientSession(MongoClientDelegate.java:278)
[2023-01-26T23:38:51.249+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:202)
[2023-01-26T23:38:51.249+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:441)
[2023-01-26T23:38:51.249+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:421)
[2023-01-26T23:38:51.249+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$4(MongoSpark.scala:138)
[2023-01-26T23:38:51.250+0000] {spark_submit.py:495} INFO - at scala.collection.Iterator.foreach(Iterator.scala:943)
[2023-01-26T23:38:51.250+0000] {spark_submit.py:495} INFO - at scala.collection.Iterator.foreach$(Iterator.scala:943)
[2023-01-26T23:38:51.250+0000] {spark_submit.py:495} INFO - at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
[2023-01-26T23:38:51.250+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$3(MongoSpark.scala:122)
[2023-01-26T23:38:51.250+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$3$adapted(MongoSpark.scala:121)
[2023-01-26T23:38:51.250+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.$anonfun$withCollectionDo$1(MongoConnector.scala:186)
[2023-01-26T23:38:51.250+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.$anonfun$withDatabaseDo$1(MongoConnector.scala:171)
[2023-01-26T23:38:51.250+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withMongoClientDo(MongoConnector.scala:154)
[2023-01-26T23:38:51.250+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withDatabaseDo(MongoConnector.scala:171)
[2023-01-26T23:38:51.250+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withCollectionDo(MongoConnector.scala:184)
[2023-01-26T23:38:51.250+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$2(MongoSpark.scala:121)
[2023-01-26T23:38:51.250+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$2$adapted(MongoSpark.scala:120)
[2023-01-26T23:38:51.250+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1011)
[2023-01-26T23:38:51.250+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1011)
[2023-01-26T23:38:51.250+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2268)
[2023-01-26T23:38:51.251+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
[2023-01-26T23:38:51.251+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.Task.run(Task.scala:136)
[2023-01-26T23:38:51.251+0000] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)
[2023-01-26T23:38:51.251+0000] {spark_submit.py:495} INFO - at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)
[2023-01-26T23:38:51.251+0000] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)
[2023-01-26T23:38:51.251+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[2023-01-26T23:38:51.251+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[2023-01-26T23:38:51.251+0000] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2023-01-26T23:38:51.251+0000] {spark_submit.py:495} INFO - 
[2023-01-26T23:38:51.251+0000] {spark_submit.py:495} INFO - Driver stacktrace:
[2023-01-26T23:38:51.251+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)
[2023-01-26T23:38:51.251+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)
[2023-01-26T23:38:51.251+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)
[2023-01-26T23:38:51.251+0000] {spark_submit.py:495} INFO - at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
[2023-01-26T23:38:51.252+0000] {spark_submit.py:495} INFO - at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
[2023-01-26T23:38:51.252+0000] {spark_submit.py:495} INFO - at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
[2023-01-26T23:38:51.252+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)
[2023-01-26T23:38:51.252+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)
[2023-01-26T23:38:51.252+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)
[2023-01-26T23:38:51.252+0000] {spark_submit.py:495} INFO - at scala.Option.foreach(Option.scala:407)
[2023-01-26T23:38:51.252+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)
[2023-01-26T23:38:51.252+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)
[2023-01-26T23:38:51.252+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)
[2023-01-26T23:38:51.252+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)
[2023-01-26T23:38:51.252+0000] {spark_submit.py:495} INFO - at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
[2023-01-26T23:38:51.252+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)
[2023-01-26T23:38:51.252+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.runJob(SparkContext.scala:2228)
[2023-01-26T23:38:51.252+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.runJob(SparkContext.scala:2249)
[2023-01-26T23:38:51.253+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.runJob(SparkContext.scala:2268)
[2023-01-26T23:38:51.253+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.runJob(SparkContext.scala:2293)
[2023-01-26T23:38:51.253+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$1(RDD.scala:1011)
[2023-01-26T23:38:51.253+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
[2023-01-26T23:38:51.253+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
[2023-01-26T23:38:51.253+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.withScope(RDD.scala:406)
[2023-01-26T23:38:51.253+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.foreachPartition(RDD.scala:1009)
[2023-01-26T23:38:51.253+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.save(MongoSpark.scala:120)
[2023-01-26T23:38:51.253+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.save(MongoSpark.scala:169)
[2023-01-26T23:38:51.253+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.sql.DefaultSource.createRelation(DefaultSource.scala:70)
[2023-01-26T23:38:51.253+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:47)
[2023-01-26T23:38:51.253+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)
[2023-01-26T23:38:51.253+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)
[2023-01-26T23:38:51.253+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)
[2023-01-26T23:38:51.253+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)
[2023-01-26T23:38:51.253+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)
[2023-01-26T23:38:51.254+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)
[2023-01-26T23:38:51.254+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)
[2023-01-26T23:38:51.254+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
[2023-01-26T23:38:51.254+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
[2023-01-26T23:38:51.254+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)
[2023-01-26T23:38:51.254+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)
[2023-01-26T23:38:51.254+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:584)
[2023-01-26T23:38:51.254+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:176)
[2023-01-26T23:38:51.254+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:584)
[2023-01-26T23:38:51.254+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)
[2023-01-26T23:38:51.254+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
[2023-01-26T23:38:51.254+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
[2023-01-26T23:38:51.254+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
[2023-01-26T23:38:51.255+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
[2023-01-26T23:38:51.255+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:560)
[2023-01-26T23:38:51.255+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)
[2023-01-26T23:38:51.255+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)
[2023-01-26T23:38:51.255+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)
[2023-01-26T23:38:51.255+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:116)
[2023-01-26T23:38:51.255+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:860)
[2023-01-26T23:38:51.255+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:390)
[2023-01-26T23:38:51.255+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:363)
[2023-01-26T23:38:51.255+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:247)
[2023-01-26T23:38:51.255+0000] {spark_submit.py:495} INFO - at jdk.internal.reflect.GeneratedMethodAccessor69.invoke(Unknown Source)
[2023-01-26T23:38:51.255+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
[2023-01-26T23:38:51.255+0000] {spark_submit.py:495} INFO - at java.base/java.lang.reflect.Method.invoke(Method.java:566)
[2023-01-26T23:38:51.255+0000] {spark_submit.py:495} INFO - at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
[2023-01-26T23:38:51.255+0000] {spark_submit.py:495} INFO - at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
[2023-01-26T23:38:51.256+0000] {spark_submit.py:495} INFO - at py4j.Gateway.invoke(Gateway.java:282)
[2023-01-26T23:38:51.256+0000] {spark_submit.py:495} INFO - at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
[2023-01-26T23:38:51.256+0000] {spark_submit.py:495} INFO - at py4j.commands.CallCommand.execute(CallCommand.java:79)
[2023-01-26T23:38:51.256+0000] {spark_submit.py:495} INFO - at py4j.ClientServerConnection.sendCommand(ClientServerConnection.java:244)
[2023-01-26T23:38:51.256+0000] {spark_submit.py:495} INFO - at py4j.CallbackClient.sendCommand(CallbackClient.java:384)
[2023-01-26T23:38:51.256+0000] {spark_submit.py:495} INFO - at py4j.CallbackClient.sendCommand(CallbackClient.java:356)
[2023-01-26T23:38:51.256+0000] {spark_submit.py:495} INFO - at py4j.reflection.PythonProxyHandler.invoke(PythonProxyHandler.java:106)
[2023-01-26T23:38:51.256+0000] {spark_submit.py:495} INFO - at com.sun.proxy.$Proxy32.call(Unknown Source)
[2023-01-26T23:38:51.256+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1(ForeachBatchSink.scala:51)
[2023-01-26T23:38:51.256+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1$adapted(ForeachBatchSink.scala:51)
[2023-01-26T23:38:51.257+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.sources.ForeachBatchSink.addBatch(ForeachBatchSink.scala:32)
[2023-01-26T23:38:51.257+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:666)
[2023-01-26T23:38:51.257+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)
[2023-01-26T23:38:51.257+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)
[2023-01-26T23:38:51.257+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)
[2023-01-26T23:38:51.257+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
[2023-01-26T23:38:51.257+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
[2023-01-26T23:38:51.257+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:664)
[2023-01-26T23:38:51.257+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:375)
[2023-01-26T23:38:51.257+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:373)
[2023-01-26T23:38:51.257+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:68)
[2023-01-26T23:38:51.257+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:664)
[2023-01-26T23:38:51.257+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:256)
[2023-01-26T23:38:51.257+0000] {spark_submit.py:495} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2023-01-26T23:38:51.258+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:375)
[2023-01-26T23:38:51.258+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:373)
[2023-01-26T23:38:51.258+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:68)
[2023-01-26T23:38:51.258+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:219)
[2023-01-26T23:38:51.258+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)
[2023-01-26T23:38:51.258+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:213)
[2023-01-26T23:38:51.258+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:307)
[2023-01-26T23:38:51.258+0000] {spark_submit.py:495} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2023-01-26T23:38:51.258+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
[2023-01-26T23:38:51.258+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:285)
[2023-01-26T23:38:51.258+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:208)
[2023-01-26T23:38:51.258+0000] {spark_submit.py:495} INFO - Caused by: com.mongodb.MongoTimeoutException: Timed out after 30000 ms while waiting to connect. Client view of cluster state is {type=UNKNOWN, servers=[{address=172.22.0.2:27017, type=UNKNOWN, state=CONNECTING, exception={com.mongodb.MongoSocketOpenException: Exception opening socket}, caused by {java.net.SocketTimeoutException: connect timed out}}]
[2023-01-26T23:38:51.258+0000] {spark_submit.py:495} INFO - at com.mongodb.internal.connection.BaseCluster.getDescription(BaseCluster.java:177)
[2023-01-26T23:38:51.258+0000] {spark_submit.py:495} INFO - at com.mongodb.internal.connection.SingleServerCluster.getDescription(SingleServerCluster.java:41)
[2023-01-26T23:38:51.258+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate.getConnectedClusterDescription(MongoClientDelegate.java:147)
[2023-01-26T23:38:51.259+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate.createClientSession(MongoClientDelegate.java:98)
[2023-01-26T23:38:51.259+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.getClientSession(MongoClientDelegate.java:278)
[2023-01-26T23:38:51.259+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:202)
[2023-01-26T23:38:51.259+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:441)
[2023-01-26T23:38:51.259+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:421)
[2023-01-26T23:38:51.259+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$4(MongoSpark.scala:138)
[2023-01-26T23:38:51.259+0000] {spark_submit.py:495} INFO - at scala.collection.Iterator.foreach(Iterator.scala:943)
[2023-01-26T23:38:51.259+0000] {spark_submit.py:495} INFO - at scala.collection.Iterator.foreach$(Iterator.scala:943)
[2023-01-26T23:38:51.259+0000] {spark_submit.py:495} INFO - at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
[2023-01-26T23:38:51.259+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$3(MongoSpark.scala:122)
[2023-01-26T23:38:51.259+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$3$adapted(MongoSpark.scala:121)
[2023-01-26T23:38:51.259+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.$anonfun$withCollectionDo$1(MongoConnector.scala:186)
[2023-01-26T23:38:51.259+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.$anonfun$withDatabaseDo$1(MongoConnector.scala:171)
[2023-01-26T23:38:51.259+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withMongoClientDo(MongoConnector.scala:154)
[2023-01-26T23:38:51.260+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withDatabaseDo(MongoConnector.scala:171)
[2023-01-26T23:38:51.260+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withCollectionDo(MongoConnector.scala:184)
[2023-01-26T23:38:51.260+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$2(MongoSpark.scala:121)
[2023-01-26T23:38:51.260+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$2$adapted(MongoSpark.scala:120)
[2023-01-26T23:38:51.260+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1011)
[2023-01-26T23:38:51.260+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1011)
[2023-01-26T23:38:51.260+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2268)
[2023-01-26T23:38:51.260+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
[2023-01-26T23:38:51.260+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.Task.run(Task.scala:136)
[2023-01-26T23:38:51.260+0000] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)
[2023-01-26T23:38:51.260+0000] {spark_submit.py:495} INFO - at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)
[2023-01-26T23:38:51.260+0000] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)
[2023-01-26T23:38:51.260+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[2023-01-26T23:38:51.260+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[2023-01-26T23:38:51.260+0000] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2023-01-26T23:38:51.261+0000] {spark_submit.py:495} INFO - [0m
[2023-01-26T23:51:40.479+0000] {spark_submit.py:495} INFO - [[34m2023-01-26 23:51:40,477[0m] {[34mclientserver.py:[0m577} INFO[0m - Received command c on object id p0[0m
[2023-01-26T23:51:45.661+0000] {spark_submit.py:495} INFO - -------------------------------------------
[2023-01-26T23:51:45.661+0000] {spark_submit.py:495} INFO - Batch: 17
[2023-01-26T23:51:45.661+0000] {spark_submit.py:495} INFO - -------------------------------------------
[2023-01-26T23:51:45.674+0000] {spark_submit.py:495} INFO - +-------------+--------------------+--------+-------------+--------------------+-------------------+------------+-----------------+--------------------+-------------+
[2023-01-26T23:51:45.674+0000] {spark_submit.py:495} INFO - |           id|       nameOfProduct|   price|ProductStatus|          SellerName|   RegistrationDate|QuantitySold|QuantityAvailable|      Free shipping?|Store ratings|
[2023-01-26T23:51:45.675+0000] {spark_submit.py:495} INFO - +-------------+--------------------+--------+-------------+--------------------+-------------------+------------+-----------------+--------------------+-------------+
[2023-01-26T23:51:45.675+0000] {spark_submit.py:495} INFO - |MLA1220964588|Procesador Gamer ...| 45899.0|          new|    OVERHARD DIGITAL|2015-01-08 12:19:41|          50|                1|Your shipment arr...|          4.5|
[2023-01-26T23:51:45.675+0000] {spark_submit.py:495} INFO - |MLA1157512489|Placa De Video Nv...|124999.0|          new|      PCREGISTRADAOK|2020-11-02 13:17:33|          50|                1|Your shipment arr...|          4.5|
[2023-01-26T23:51:45.675+0000] {spark_submit.py:495} INFO - |MLA1221192372|Disco Duro Extern...| 26299.0|          new|    OVERHARD DIGITAL|2015-01-08 12:19:41|         250|              100|Your shipment arr...|          4.5|
[2023-01-26T23:51:45.675+0000] {spark_submit.py:495} INFO - |MLA1163238194|Placa De Video Am...| 33999.0|          new|       MICYBERCOMPRA|2011-09-13 13:15:53|          50|                1|Your shipment arr...|          4.5|
[2023-01-26T23:51:45.675+0000] {spark_submit.py:495} INFO - |MLA1160204806|Memoria Ram Fury ...| 15499.0|          new|    OVERHARD DIGITAL|2015-01-08 12:19:41|         500|               50|Your shipment arr...|          4.5|
[2023-01-26T23:51:45.675+0000] {spark_submit.py:495} INFO - |MLA1115295971|Procesador Amd Ry...| 93999.0|          new|      PCREGISTRADAOK|2020-11-02 13:17:33|          25|                1|Your shipment arr...|          4.5|
[2023-01-26T23:51:45.675+0000] {spark_submit.py:495} INFO - |MLA1133481354|Disco Sólido Inte...| 13799.0|          new|           DALECLÍCK|2015-03-16 12:07:23|         500|                1|You have to pay t...|          4.5|
[2023-01-26T23:51:45.675+0000] {spark_submit.py:495} INFO - | MLA923637669|Placa De Video Ge...| 28699.0|          new|         INDIGOTRADE|2018-01-05 19:21:16|         500|               50|Your shipment arr...|          4.5|
[2023-01-26T23:51:45.675+0000] {spark_submit.py:495} INFO - |MLA1139583771|Disco Sólido Inte...|  8339.0|          new|DIAMONDSYSTEMPADUAML|2015-07-01 12:32:22|         250|              250|You have to pay t...|          4.5|
[2023-01-26T23:51:45.675+0000] {spark_submit.py:495} INFO - |MLA1228990119|Memoria Ram Fury ...| 23255.0|          new|MARSTECHSRLMARSTE...|2020-08-05 14:27:59|          50|                1|Your shipment arr...|          4.5|
[2023-01-26T23:51:45.676+0000] {spark_submit.py:495} INFO - | MLA882277177|Capturadora Video...|  2699.0|          new|         BSK-IMPORTS|2019-12-30 23:07:04|         500|               50|You have to pay t...|          3.0|
[2023-01-26T23:51:45.676+0000] {spark_submit.py:495} INFO - |MLA1148481665|Microprocesador P...| 58899.0|          new|            MALL WEB|2016-05-26 20:28:35|         150|                1|Your shipment arr...|          4.5|
[2023-01-26T23:51:45.676+0000] {spark_submit.py:495} INFO - | MLA844489972|Disco Duro Intern...| 11599.0|          new|           DALECLÍCK|2015-03-16 12:07:23|        5000|              500|You have to pay t...|          5.0|
[2023-01-26T23:51:45.676+0000] {spark_submit.py:495} INFO - |MLA1183925519|Placa De Video Nv...|299999.0|          new|ESTUDIOSMPSRLESTU...|2020-11-29 18:36:32|          50|                1|Your shipment arr...|          4.5|
[2023-01-26T23:51:45.676+0000] {spark_submit.py:495} INFO - |MLA1168362675|Memoria Ram Fury ...| 26698.0|          new|MARSTECHSRLMARSTE...|2020-08-05 14:27:59|         150|                1|Your shipment arr...|          4.5|
[2023-01-26T23:51:45.676+0000] {spark_submit.py:495} INFO - |MLA1314590445|Disco Sólido Inte...|  7999.0|          new|       ELECTRO-SHOWS|2010-03-26 01:08:16|         250|              250|You have to pay t...|          4.5|
[2023-01-26T23:51:45.676+0000] {spark_submit.py:495} INFO - |MLA1302136118|Disco Duro Intern...| 18271.0|          new|DIAMONDSYSTEMSANJ...|2016-04-08 17:21:06|         100|               50|Your shipment arr...|          4.5|
[2023-01-26T23:51:45.676+0000] {spark_submit.py:495} INFO - |MLA1163233412|Placa De Video Nv...| 34659.0|          new|      PCREGISTRADAOK|2020-11-02 13:17:33|           1|                1|Your shipment arr...|          4.5|
[2023-01-26T23:51:45.676+0000] {spark_submit.py:495} INFO - |MLA1137238635|Fuente De Aliment...|  3235.0|          new|     DIAMONDSYSTEMML|2014-04-07 22:00:36|          25|                1|You have to pay t...|          3.5|
[2023-01-26T23:51:45.676+0000] {spark_submit.py:495} INFO - |MLA1157644025|Disco Duro Extern...| 38790.0|          new|   ELECTRONIC-MARKET|2011-09-15 18:30:10|          25|               50|Your shipment arr...|          4.5|
[2023-01-26T23:51:45.676+0000] {spark_submit.py:495} INFO - +-------------+--------------------+--------+-------------+--------------------+-------------------+------------+-----------------+--------------------+-------------+
[2023-01-26T23:51:45.676+0000] {spark_submit.py:495} INFO - only showing top 20 rows
[2023-01-26T23:51:45.676+0000] {spark_submit.py:495} INFO - 
[2023-01-26T23:52:11.091+0000] {spark_submit.py:495} INFO - 23/01/26 23:52:11 ERROR Executor: Exception in task 0.0 in stage 48.0 (TID 48)
[2023-01-26T23:52:11.091+0000] {spark_submit.py:495} INFO - com.mongodb.MongoTimeoutException: Timed out after 30000 ms while waiting to connect. Client view of cluster state is {type=UNKNOWN, servers=[{address=172.22.0.2:27017, type=UNKNOWN, state=CONNECTING, exception={com.mongodb.MongoSocketOpenException: Exception opening socket}, caused by {java.net.SocketTimeoutException: connect timed out}}]
[2023-01-26T23:52:11.091+0000] {spark_submit.py:495} INFO - at com.mongodb.internal.connection.BaseCluster.getDescription(BaseCluster.java:177)
[2023-01-26T23:52:11.091+0000] {spark_submit.py:495} INFO - at com.mongodb.internal.connection.SingleServerCluster.getDescription(SingleServerCluster.java:41)
[2023-01-26T23:52:11.091+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate.getConnectedClusterDescription(MongoClientDelegate.java:147)
[2023-01-26T23:52:11.091+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate.createClientSession(MongoClientDelegate.java:98)
[2023-01-26T23:52:11.091+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.getClientSession(MongoClientDelegate.java:278)
[2023-01-26T23:52:11.091+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:202)
[2023-01-26T23:52:11.091+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:441)
[2023-01-26T23:52:11.091+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:421)
[2023-01-26T23:52:11.092+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$4(MongoSpark.scala:138)
[2023-01-26T23:52:11.092+0000] {spark_submit.py:495} INFO - at scala.collection.Iterator.foreach(Iterator.scala:943)
[2023-01-26T23:52:11.092+0000] {spark_submit.py:495} INFO - at scala.collection.Iterator.foreach$(Iterator.scala:943)
[2023-01-26T23:52:11.092+0000] {spark_submit.py:495} INFO - at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
[2023-01-26T23:52:11.092+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$3(MongoSpark.scala:122)
[2023-01-26T23:52:11.092+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$3$adapted(MongoSpark.scala:121)
[2023-01-26T23:52:11.092+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.$anonfun$withCollectionDo$1(MongoConnector.scala:186)
[2023-01-26T23:52:11.092+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.$anonfun$withDatabaseDo$1(MongoConnector.scala:171)
[2023-01-26T23:52:11.092+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withMongoClientDo(MongoConnector.scala:154)
[2023-01-26T23:52:11.092+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withDatabaseDo(MongoConnector.scala:171)
[2023-01-26T23:52:11.092+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withCollectionDo(MongoConnector.scala:184)
[2023-01-26T23:52:11.092+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$2(MongoSpark.scala:121)
[2023-01-26T23:52:11.092+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$2$adapted(MongoSpark.scala:120)
[2023-01-26T23:52:11.092+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1011)
[2023-01-26T23:52:11.093+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1011)
[2023-01-26T23:52:11.093+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2268)
[2023-01-26T23:52:11.093+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
[2023-01-26T23:52:11.093+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.Task.run(Task.scala:136)
[2023-01-26T23:52:11.093+0000] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)
[2023-01-26T23:52:11.093+0000] {spark_submit.py:495} INFO - at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)
[2023-01-26T23:52:11.093+0000] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)
[2023-01-26T23:52:11.093+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[2023-01-26T23:52:11.093+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[2023-01-26T23:52:11.093+0000] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2023-01-26T23:52:11.093+0000] {spark_submit.py:495} INFO - 23/01/26 23:52:11 ERROR TaskSetManager: Task 0 in stage 48.0 failed 1 times; aborting job
[2023-01-26T23:52:11.222+0000] {spark_submit.py:495} INFO - [[34m2023-01-26 23:52:11,220[0m] {[34mconsumerSpark.py:[0m37} ERROR[0m - Error al escribir en Mongo: An error occurred while calling o401.save.
[2023-01-26T23:52:11.222+0000] {spark_submit.py:495} INFO - : org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 48.0 failed 1 times, most recent failure: Lost task 0.0 in stage 48.0 (TID 48) (f1cbb5df32e6 executor driver): com.mongodb.MongoTimeoutException: Timed out after 30000 ms while waiting to connect. Client view of cluster state is {type=UNKNOWN, servers=[{address=172.22.0.2:27017, type=UNKNOWN, state=CONNECTING, exception={com.mongodb.MongoSocketOpenException: Exception opening socket}, caused by {java.net.SocketTimeoutException: connect timed out}}]
[2023-01-26T23:52:11.222+0000] {spark_submit.py:495} INFO - at com.mongodb.internal.connection.BaseCluster.getDescription(BaseCluster.java:177)
[2023-01-26T23:52:11.223+0000] {spark_submit.py:495} INFO - at com.mongodb.internal.connection.SingleServerCluster.getDescription(SingleServerCluster.java:41)
[2023-01-26T23:52:11.223+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate.getConnectedClusterDescription(MongoClientDelegate.java:147)
[2023-01-26T23:52:11.223+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate.createClientSession(MongoClientDelegate.java:98)
[2023-01-26T23:52:11.223+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.getClientSession(MongoClientDelegate.java:278)
[2023-01-26T23:52:11.223+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:202)
[2023-01-26T23:52:11.223+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:441)
[2023-01-26T23:52:11.223+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:421)
[2023-01-26T23:52:11.223+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$4(MongoSpark.scala:138)
[2023-01-26T23:52:11.224+0000] {spark_submit.py:495} INFO - at scala.collection.Iterator.foreach(Iterator.scala:943)
[2023-01-26T23:52:11.224+0000] {spark_submit.py:495} INFO - at scala.collection.Iterator.foreach$(Iterator.scala:943)
[2023-01-26T23:52:11.224+0000] {spark_submit.py:495} INFO - at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
[2023-01-26T23:52:11.224+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$3(MongoSpark.scala:122)
[2023-01-26T23:52:11.224+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$3$adapted(MongoSpark.scala:121)
[2023-01-26T23:52:11.224+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.$anonfun$withCollectionDo$1(MongoConnector.scala:186)
[2023-01-26T23:52:11.225+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.$anonfun$withDatabaseDo$1(MongoConnector.scala:171)
[2023-01-26T23:52:11.225+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withMongoClientDo(MongoConnector.scala:154)
[2023-01-26T23:52:11.225+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withDatabaseDo(MongoConnector.scala:171)
[2023-01-26T23:52:11.225+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withCollectionDo(MongoConnector.scala:184)
[2023-01-26T23:52:11.225+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$2(MongoSpark.scala:121)
[2023-01-26T23:52:11.225+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$2$adapted(MongoSpark.scala:120)
[2023-01-26T23:52:11.225+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1011)
[2023-01-26T23:52:11.225+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1011)
[2023-01-26T23:52:11.225+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2268)
[2023-01-26T23:52:11.225+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
[2023-01-26T23:52:11.226+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.Task.run(Task.scala:136)
[2023-01-26T23:52:11.226+0000] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)
[2023-01-26T23:52:11.226+0000] {spark_submit.py:495} INFO - at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)
[2023-01-26T23:52:11.226+0000] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)
[2023-01-26T23:52:11.226+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[2023-01-26T23:52:11.226+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[2023-01-26T23:52:11.226+0000] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2023-01-26T23:52:11.226+0000] {spark_submit.py:495} INFO - 
[2023-01-26T23:52:11.226+0000] {spark_submit.py:495} INFO - Driver stacktrace:
[2023-01-26T23:52:11.226+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)
[2023-01-26T23:52:11.226+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)
[2023-01-26T23:52:11.226+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)
[2023-01-26T23:52:11.226+0000] {spark_submit.py:495} INFO - at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
[2023-01-26T23:52:11.227+0000] {spark_submit.py:495} INFO - at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
[2023-01-26T23:52:11.227+0000] {spark_submit.py:495} INFO - at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
[2023-01-26T23:52:11.227+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)
[2023-01-26T23:52:11.227+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)
[2023-01-26T23:52:11.227+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)
[2023-01-26T23:52:11.227+0000] {spark_submit.py:495} INFO - at scala.Option.foreach(Option.scala:407)
[2023-01-26T23:52:11.227+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)
[2023-01-26T23:52:11.227+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)
[2023-01-26T23:52:11.227+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)
[2023-01-26T23:52:11.227+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)
[2023-01-26T23:52:11.227+0000] {spark_submit.py:495} INFO - at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
[2023-01-26T23:52:11.227+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)
[2023-01-26T23:52:11.227+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.runJob(SparkContext.scala:2228)
[2023-01-26T23:52:11.228+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.runJob(SparkContext.scala:2249)
[2023-01-26T23:52:11.228+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.runJob(SparkContext.scala:2268)
[2023-01-26T23:52:11.228+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.runJob(SparkContext.scala:2293)
[2023-01-26T23:52:11.228+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$1(RDD.scala:1011)
[2023-01-26T23:52:11.228+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
[2023-01-26T23:52:11.228+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
[2023-01-26T23:52:11.229+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.withScope(RDD.scala:406)
[2023-01-26T23:52:11.229+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.foreachPartition(RDD.scala:1009)
[2023-01-26T23:52:11.229+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.save(MongoSpark.scala:120)
[2023-01-26T23:52:11.229+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.save(MongoSpark.scala:169)
[2023-01-26T23:52:11.229+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.sql.DefaultSource.createRelation(DefaultSource.scala:70)
[2023-01-26T23:52:11.229+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:47)
[2023-01-26T23:52:11.229+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)
[2023-01-26T23:52:11.229+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)
[2023-01-26T23:52:11.229+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)
[2023-01-26T23:52:11.230+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)
[2023-01-26T23:52:11.230+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)
[2023-01-26T23:52:11.230+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)
[2023-01-26T23:52:11.230+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)
[2023-01-26T23:52:11.230+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
[2023-01-26T23:52:11.230+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
[2023-01-26T23:52:11.230+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)
[2023-01-26T23:52:11.230+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)
[2023-01-26T23:52:11.230+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:584)
[2023-01-26T23:52:11.231+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:176)
[2023-01-26T23:52:11.231+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:584)
[2023-01-26T23:52:11.231+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)
[2023-01-26T23:52:11.231+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
[2023-01-26T23:52:11.231+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
[2023-01-26T23:52:11.231+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
[2023-01-26T23:52:11.231+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
[2023-01-26T23:52:11.231+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:560)
[2023-01-26T23:52:11.232+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)
[2023-01-26T23:52:11.232+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)
[2023-01-26T23:52:11.232+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)
[2023-01-26T23:52:11.232+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:116)
[2023-01-26T23:52:11.232+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:860)
[2023-01-26T23:52:11.232+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:390)
[2023-01-26T23:52:11.232+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:363)
[2023-01-26T23:52:11.232+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:247)
[2023-01-26T23:52:11.232+0000] {spark_submit.py:495} INFO - at jdk.internal.reflect.GeneratedMethodAccessor69.invoke(Unknown Source)
[2023-01-26T23:52:11.233+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
[2023-01-26T23:52:11.233+0000] {spark_submit.py:495} INFO - at java.base/java.lang.reflect.Method.invoke(Method.java:566)
[2023-01-26T23:52:11.233+0000] {spark_submit.py:495} INFO - at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
[2023-01-26T23:52:11.233+0000] {spark_submit.py:495} INFO - at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
[2023-01-26T23:52:11.233+0000] {spark_submit.py:495} INFO - at py4j.Gateway.invoke(Gateway.java:282)
[2023-01-26T23:52:11.233+0000] {spark_submit.py:495} INFO - at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
[2023-01-26T23:52:11.233+0000] {spark_submit.py:495} INFO - at py4j.commands.CallCommand.execute(CallCommand.java:79)
[2023-01-26T23:52:11.234+0000] {spark_submit.py:495} INFO - at py4j.ClientServerConnection.sendCommand(ClientServerConnection.java:244)
[2023-01-26T23:52:11.234+0000] {spark_submit.py:495} INFO - at py4j.CallbackClient.sendCommand(CallbackClient.java:384)
[2023-01-26T23:52:11.234+0000] {spark_submit.py:495} INFO - at py4j.CallbackClient.sendCommand(CallbackClient.java:356)
[2023-01-26T23:52:11.234+0000] {spark_submit.py:495} INFO - at py4j.reflection.PythonProxyHandler.invoke(PythonProxyHandler.java:106)
[2023-01-26T23:52:11.234+0000] {spark_submit.py:495} INFO - at com.sun.proxy.$Proxy32.call(Unknown Source)
[2023-01-26T23:52:11.234+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1(ForeachBatchSink.scala:51)
[2023-01-26T23:52:11.234+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1$adapted(ForeachBatchSink.scala:51)
[2023-01-26T23:52:11.234+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.sources.ForeachBatchSink.addBatch(ForeachBatchSink.scala:32)
[2023-01-26T23:52:11.234+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:666)
[2023-01-26T23:52:11.234+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)
[2023-01-26T23:52:11.234+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)
[2023-01-26T23:52:11.234+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)
[2023-01-26T23:52:11.234+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
[2023-01-26T23:52:11.235+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
[2023-01-26T23:52:11.235+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:664)
[2023-01-26T23:52:11.235+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:375)
[2023-01-26T23:52:11.235+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:373)
[2023-01-26T23:52:11.235+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:68)
[2023-01-26T23:52:11.235+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:664)
[2023-01-26T23:52:11.235+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:256)
[2023-01-26T23:52:11.235+0000] {spark_submit.py:495} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2023-01-26T23:52:11.235+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:375)
[2023-01-26T23:52:11.235+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:373)
[2023-01-26T23:52:11.235+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:68)
[2023-01-26T23:52:11.235+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:219)
[2023-01-26T23:52:11.237+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)
[2023-01-26T23:52:11.237+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:213)
[2023-01-26T23:52:11.237+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:307)
[2023-01-26T23:52:11.237+0000] {spark_submit.py:495} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2023-01-26T23:52:11.237+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
[2023-01-26T23:52:11.237+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:285)
[2023-01-26T23:52:11.237+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:208)
[2023-01-26T23:52:11.237+0000] {spark_submit.py:495} INFO - Caused by: com.mongodb.MongoTimeoutException: Timed out after 30000 ms while waiting to connect. Client view of cluster state is {type=UNKNOWN, servers=[{address=172.22.0.2:27017, type=UNKNOWN, state=CONNECTING, exception={com.mongodb.MongoSocketOpenException: Exception opening socket}, caused by {java.net.SocketTimeoutException: connect timed out}}]
[2023-01-26T23:52:11.238+0000] {spark_submit.py:495} INFO - at com.mongodb.internal.connection.BaseCluster.getDescription(BaseCluster.java:177)
[2023-01-26T23:52:11.238+0000] {spark_submit.py:495} INFO - at com.mongodb.internal.connection.SingleServerCluster.getDescription(SingleServerCluster.java:41)
[2023-01-26T23:52:11.238+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate.getConnectedClusterDescription(MongoClientDelegate.java:147)
[2023-01-26T23:52:11.238+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate.createClientSession(MongoClientDelegate.java:98)
[2023-01-26T23:52:11.238+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.getClientSession(MongoClientDelegate.java:278)
[2023-01-26T23:52:11.238+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:202)
[2023-01-26T23:52:11.238+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:441)
[2023-01-26T23:52:11.238+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:421)
[2023-01-26T23:52:11.239+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$4(MongoSpark.scala:138)
[2023-01-26T23:52:11.239+0000] {spark_submit.py:495} INFO - at scala.collection.Iterator.foreach(Iterator.scala:943)
[2023-01-26T23:52:11.241+0000] {spark_submit.py:495} INFO - at scala.collection.Iterator.foreach$(Iterator.scala:943)
[2023-01-26T23:52:11.241+0000] {spark_submit.py:495} INFO - at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
[2023-01-26T23:52:11.241+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$3(MongoSpark.scala:122)
[2023-01-26T23:52:11.241+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$3$adapted(MongoSpark.scala:121)
[2023-01-26T23:52:11.241+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.$anonfun$withCollectionDo$1(MongoConnector.scala:186)
[2023-01-26T23:52:11.241+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.$anonfun$withDatabaseDo$1(MongoConnector.scala:171)
[2023-01-26T23:52:11.241+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withMongoClientDo(MongoConnector.scala:154)
[2023-01-26T23:52:11.241+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withDatabaseDo(MongoConnector.scala:171)
[2023-01-26T23:52:11.241+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withCollectionDo(MongoConnector.scala:184)
[2023-01-26T23:52:11.241+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$2(MongoSpark.scala:121)
[2023-01-26T23:52:11.242+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$2$adapted(MongoSpark.scala:120)
[2023-01-26T23:52:11.242+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1011)
[2023-01-26T23:52:11.242+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1011)
[2023-01-26T23:52:11.243+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2268)
[2023-01-26T23:52:11.243+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
[2023-01-26T23:52:11.243+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.Task.run(Task.scala:136)
[2023-01-26T23:52:11.243+0000] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)
[2023-01-26T23:52:11.244+0000] {spark_submit.py:495} INFO - at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)
[2023-01-26T23:52:11.244+0000] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)
[2023-01-26T23:52:11.244+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[2023-01-26T23:52:11.244+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[2023-01-26T23:52:11.244+0000] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2023-01-26T23:52:11.244+0000] {spark_submit.py:495} INFO - [0m
[2023-01-26T23:52:11.455+0000] {spark_submit.py:495} INFO - [[34m2023-01-26 23:52:11,455[0m] {[34mclientserver.py:[0m577} INFO[0m - Received command c on object id p0[0m
[2023-01-26T23:52:41.552+0000] {spark_submit.py:495} INFO - 23/01/26 23:52:41 ERROR Executor: Exception in task 0.0 in stage 50.0 (TID 50)
[2023-01-26T23:52:41.553+0000] {spark_submit.py:495} INFO - com.mongodb.MongoTimeoutException: Timed out after 30000 ms while waiting to connect. Client view of cluster state is {type=UNKNOWN, servers=[{address=172.22.0.2:27017, type=UNKNOWN, state=CONNECTING, exception={com.mongodb.MongoSocketOpenException: Exception opening socket}, caused by {java.net.SocketTimeoutException: connect timed out}}]
[2023-01-26T23:52:41.553+0000] {spark_submit.py:495} INFO - at com.mongodb.internal.connection.BaseCluster.getDescription(BaseCluster.java:177)
[2023-01-26T23:52:41.553+0000] {spark_submit.py:495} INFO - at com.mongodb.internal.connection.SingleServerCluster.getDescription(SingleServerCluster.java:41)
[2023-01-26T23:52:41.554+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate.getConnectedClusterDescription(MongoClientDelegate.java:147)
[2023-01-26T23:52:41.554+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate.createClientSession(MongoClientDelegate.java:98)
[2023-01-26T23:52:41.554+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.getClientSession(MongoClientDelegate.java:278)
[2023-01-26T23:52:41.555+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:202)
[2023-01-26T23:52:41.555+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:441)
[2023-01-26T23:52:41.555+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:421)
[2023-01-26T23:52:41.555+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$4(MongoSpark.scala:138)
[2023-01-26T23:52:41.556+0000] {spark_submit.py:495} INFO - at scala.collection.Iterator.foreach(Iterator.scala:943)
[2023-01-26T23:52:41.556+0000] {spark_submit.py:495} INFO - at scala.collection.Iterator.foreach$(Iterator.scala:943)
[2023-01-26T23:52:41.556+0000] {spark_submit.py:495} INFO - at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
[2023-01-26T23:52:41.556+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$3(MongoSpark.scala:122)
[2023-01-26T23:52:41.556+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$3$adapted(MongoSpark.scala:121)
[2023-01-26T23:52:41.557+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.$anonfun$withCollectionDo$1(MongoConnector.scala:186)
[2023-01-26T23:52:41.557+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.$anonfun$withDatabaseDo$1(MongoConnector.scala:171)
[2023-01-26T23:52:41.557+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withMongoClientDo(MongoConnector.scala:154)
[2023-01-26T23:52:41.557+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withDatabaseDo(MongoConnector.scala:171)
[2023-01-26T23:52:41.558+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withCollectionDo(MongoConnector.scala:184)
[2023-01-26T23:52:41.558+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$2(MongoSpark.scala:121)
[2023-01-26T23:52:41.558+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$2$adapted(MongoSpark.scala:120)
[2023-01-26T23:52:41.558+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1011)
[2023-01-26T23:52:41.558+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1011)
[2023-01-26T23:52:41.559+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2268)
[2023-01-26T23:52:41.559+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
[2023-01-26T23:52:41.559+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.Task.run(Task.scala:136)
[2023-01-26T23:52:41.559+0000] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)
[2023-01-26T23:52:41.559+0000] {spark_submit.py:495} INFO - at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)
[2023-01-26T23:52:41.560+0000] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)
[2023-01-26T23:52:41.560+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[2023-01-26T23:52:41.560+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[2023-01-26T23:52:41.560+0000] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2023-01-26T23:52:41.560+0000] {spark_submit.py:495} INFO - 23/01/26 23:52:41 ERROR TaskSetManager: Task 0 in stage 50.0 failed 1 times; aborting job
[2023-01-26T23:52:41.697+0000] {spark_submit.py:495} INFO - [[34m2023-01-26 23:52:41,697[0m] {[34mconsumerSpark.py:[0m37} ERROR[0m - Error al escribir en Mongo: An error occurred while calling o410.save.
[2023-01-26T23:52:41.698+0000] {spark_submit.py:495} INFO - : org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 50.0 failed 1 times, most recent failure: Lost task 0.0 in stage 50.0 (TID 50) (f1cbb5df32e6 executor driver): com.mongodb.MongoTimeoutException: Timed out after 30000 ms while waiting to connect. Client view of cluster state is {type=UNKNOWN, servers=[{address=172.22.0.2:27017, type=UNKNOWN, state=CONNECTING, exception={com.mongodb.MongoSocketOpenException: Exception opening socket}, caused by {java.net.SocketTimeoutException: connect timed out}}]
[2023-01-26T23:52:41.699+0000] {spark_submit.py:495} INFO - at com.mongodb.internal.connection.BaseCluster.getDescription(BaseCluster.java:177)
[2023-01-26T23:52:41.699+0000] {spark_submit.py:495} INFO - at com.mongodb.internal.connection.SingleServerCluster.getDescription(SingleServerCluster.java:41)
[2023-01-26T23:52:41.699+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate.getConnectedClusterDescription(MongoClientDelegate.java:147)
[2023-01-26T23:52:41.700+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate.createClientSession(MongoClientDelegate.java:98)
[2023-01-26T23:52:41.700+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.getClientSession(MongoClientDelegate.java:278)
[2023-01-26T23:52:41.700+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:202)
[2023-01-26T23:52:41.700+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:441)
[2023-01-26T23:52:41.701+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:421)
[2023-01-26T23:52:41.701+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$4(MongoSpark.scala:138)
[2023-01-26T23:52:41.701+0000] {spark_submit.py:495} INFO - at scala.collection.Iterator.foreach(Iterator.scala:943)
[2023-01-26T23:52:41.702+0000] {spark_submit.py:495} INFO - at scala.collection.Iterator.foreach$(Iterator.scala:943)
[2023-01-26T23:52:41.702+0000] {spark_submit.py:495} INFO - at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
[2023-01-26T23:52:41.702+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$3(MongoSpark.scala:122)
[2023-01-26T23:52:41.702+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$3$adapted(MongoSpark.scala:121)
[2023-01-26T23:52:41.703+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.$anonfun$withCollectionDo$1(MongoConnector.scala:186)
[2023-01-26T23:52:41.703+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.$anonfun$withDatabaseDo$1(MongoConnector.scala:171)
[2023-01-26T23:52:41.703+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withMongoClientDo(MongoConnector.scala:154)
[2023-01-26T23:52:41.704+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withDatabaseDo(MongoConnector.scala:171)
[2023-01-26T23:52:41.704+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withCollectionDo(MongoConnector.scala:184)
[2023-01-26T23:52:41.704+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$2(MongoSpark.scala:121)
[2023-01-26T23:52:41.705+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$2$adapted(MongoSpark.scala:120)
[2023-01-26T23:52:41.705+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1011)
[2023-01-26T23:52:41.705+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1011)
[2023-01-26T23:52:41.705+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2268)
[2023-01-26T23:52:41.706+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
[2023-01-26T23:52:41.706+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.Task.run(Task.scala:136)
[2023-01-26T23:52:41.706+0000] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)
[2023-01-26T23:52:41.707+0000] {spark_submit.py:495} INFO - at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)
[2023-01-26T23:52:41.707+0000] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)
[2023-01-26T23:52:41.707+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[2023-01-26T23:52:41.707+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[2023-01-26T23:52:41.708+0000] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2023-01-26T23:52:41.708+0000] {spark_submit.py:495} INFO - 
[2023-01-26T23:52:41.708+0000] {spark_submit.py:495} INFO - Driver stacktrace:
[2023-01-26T23:52:41.709+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)
[2023-01-26T23:52:41.709+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)
[2023-01-26T23:52:41.709+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)
[2023-01-26T23:52:41.709+0000] {spark_submit.py:495} INFO - at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
[2023-01-26T23:52:41.710+0000] {spark_submit.py:495} INFO - at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
[2023-01-26T23:52:41.710+0000] {spark_submit.py:495} INFO - at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
[2023-01-26T23:52:41.710+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)
[2023-01-26T23:52:41.711+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)
[2023-01-26T23:52:41.711+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)
[2023-01-26T23:52:41.711+0000] {spark_submit.py:495} INFO - at scala.Option.foreach(Option.scala:407)
[2023-01-26T23:52:41.712+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)
[2023-01-26T23:52:41.712+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)
[2023-01-26T23:52:41.712+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)
[2023-01-26T23:52:41.713+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)
[2023-01-26T23:52:41.713+0000] {spark_submit.py:495} INFO - at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
[2023-01-26T23:52:41.713+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)
[2023-01-26T23:52:41.714+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.runJob(SparkContext.scala:2228)
[2023-01-26T23:52:41.714+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.runJob(SparkContext.scala:2249)
[2023-01-26T23:52:41.714+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.runJob(SparkContext.scala:2268)
[2023-01-26T23:52:41.714+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.runJob(SparkContext.scala:2293)
[2023-01-26T23:52:41.715+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$1(RDD.scala:1011)
[2023-01-26T23:52:41.715+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
[2023-01-26T23:52:41.715+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
[2023-01-26T23:52:41.716+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.withScope(RDD.scala:406)
[2023-01-26T23:52:41.716+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.foreachPartition(RDD.scala:1009)
[2023-01-26T23:52:41.716+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.save(MongoSpark.scala:120)
[2023-01-26T23:52:41.716+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.save(MongoSpark.scala:169)
[2023-01-26T23:52:41.717+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.sql.DefaultSource.createRelation(DefaultSource.scala:70)
[2023-01-26T23:52:41.717+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:47)
[2023-01-26T23:52:41.717+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)
[2023-01-26T23:52:41.717+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)
[2023-01-26T23:52:41.718+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)
[2023-01-26T23:52:41.718+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)
[2023-01-26T23:52:41.718+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)
[2023-01-26T23:52:41.718+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)
[2023-01-26T23:52:41.718+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)
[2023-01-26T23:52:41.719+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
[2023-01-26T23:52:41.719+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
[2023-01-26T23:52:41.719+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)
[2023-01-26T23:52:41.719+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)
[2023-01-26T23:52:41.719+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:584)
[2023-01-26T23:52:41.720+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:176)
[2023-01-26T23:52:41.720+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:584)
[2023-01-26T23:52:41.720+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)
[2023-01-26T23:52:41.720+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
[2023-01-26T23:52:41.721+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
[2023-01-26T23:52:41.721+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
[2023-01-26T23:52:41.721+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
[2023-01-26T23:52:41.721+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:560)
[2023-01-26T23:52:41.721+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)
[2023-01-26T23:52:41.721+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)
[2023-01-26T23:52:41.722+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)
[2023-01-26T23:52:41.722+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:116)
[2023-01-26T23:52:41.722+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:860)
[2023-01-26T23:52:41.722+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:390)
[2023-01-26T23:52:41.722+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:363)
[2023-01-26T23:52:41.722+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:247)
[2023-01-26T23:52:41.722+0000] {spark_submit.py:495} INFO - at jdk.internal.reflect.GeneratedMethodAccessor69.invoke(Unknown Source)
[2023-01-26T23:52:41.723+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
[2023-01-26T23:52:41.723+0000] {spark_submit.py:495} INFO - at java.base/java.lang.reflect.Method.invoke(Method.java:566)
[2023-01-26T23:52:41.723+0000] {spark_submit.py:495} INFO - at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
[2023-01-26T23:52:41.723+0000] {spark_submit.py:495} INFO - at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
[2023-01-26T23:52:41.723+0000] {spark_submit.py:495} INFO - at py4j.Gateway.invoke(Gateway.java:282)
[2023-01-26T23:52:41.723+0000] {spark_submit.py:495} INFO - at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
[2023-01-26T23:52:41.723+0000] {spark_submit.py:495} INFO - at py4j.commands.CallCommand.execute(CallCommand.java:79)
[2023-01-26T23:52:41.724+0000] {spark_submit.py:495} INFO - at py4j.ClientServerConnection.sendCommand(ClientServerConnection.java:244)
[2023-01-26T23:52:41.724+0000] {spark_submit.py:495} INFO - at py4j.CallbackClient.sendCommand(CallbackClient.java:384)
[2023-01-26T23:52:41.724+0000] {spark_submit.py:495} INFO - at py4j.CallbackClient.sendCommand(CallbackClient.java:356)
[2023-01-26T23:52:41.724+0000] {spark_submit.py:495} INFO - at py4j.reflection.PythonProxyHandler.invoke(PythonProxyHandler.java:106)
[2023-01-26T23:52:41.724+0000] {spark_submit.py:495} INFO - at com.sun.proxy.$Proxy32.call(Unknown Source)
[2023-01-26T23:52:41.724+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1(ForeachBatchSink.scala:51)
[2023-01-26T23:52:41.724+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1$adapted(ForeachBatchSink.scala:51)
[2023-01-26T23:52:41.725+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.sources.ForeachBatchSink.addBatch(ForeachBatchSink.scala:32)
[2023-01-26T23:52:41.725+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:666)
[2023-01-26T23:52:41.725+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)
[2023-01-26T23:52:41.725+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)
[2023-01-26T23:52:41.725+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)
[2023-01-26T23:52:41.725+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
[2023-01-26T23:52:41.725+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
[2023-01-26T23:52:41.726+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:664)
[2023-01-26T23:52:41.726+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:375)
[2023-01-26T23:52:41.726+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:373)
[2023-01-26T23:52:41.726+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:68)
[2023-01-26T23:52:41.726+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:664)
[2023-01-26T23:52:41.726+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:256)
[2023-01-26T23:52:41.727+0000] {spark_submit.py:495} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2023-01-26T23:52:41.727+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:375)
[2023-01-26T23:52:41.727+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:373)
[2023-01-26T23:52:41.727+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:68)
[2023-01-26T23:52:41.727+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:219)
[2023-01-26T23:52:41.727+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)
[2023-01-26T23:52:41.727+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:213)
[2023-01-26T23:52:41.727+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:307)
[2023-01-26T23:52:41.728+0000] {spark_submit.py:495} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2023-01-26T23:52:41.728+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
[2023-01-26T23:52:41.728+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:285)
[2023-01-26T23:52:41.728+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:208)
[2023-01-26T23:52:41.728+0000] {spark_submit.py:495} INFO - Caused by: com.mongodb.MongoTimeoutException: Timed out after 30000 ms while waiting to connect. Client view of cluster state is {type=UNKNOWN, servers=[{address=172.22.0.2:27017, type=UNKNOWN, state=CONNECTING, exception={com.mongodb.MongoSocketOpenException: Exception opening socket}, caused by {java.net.SocketTimeoutException: connect timed out}}]
[2023-01-26T23:52:41.728+0000] {spark_submit.py:495} INFO - at com.mongodb.internal.connection.BaseCluster.getDescription(BaseCluster.java:177)
[2023-01-26T23:52:41.728+0000] {spark_submit.py:495} INFO - at com.mongodb.internal.connection.SingleServerCluster.getDescription(SingleServerCluster.java:41)
[2023-01-26T23:52:41.728+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate.getConnectedClusterDescription(MongoClientDelegate.java:147)
[2023-01-26T23:52:41.728+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate.createClientSession(MongoClientDelegate.java:98)
[2023-01-26T23:52:41.729+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.getClientSession(MongoClientDelegate.java:278)
[2023-01-26T23:52:41.729+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:202)
[2023-01-26T23:52:41.729+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:441)
[2023-01-26T23:52:41.729+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:421)
[2023-01-26T23:52:41.729+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$4(MongoSpark.scala:138)
[2023-01-26T23:52:41.729+0000] {spark_submit.py:495} INFO - at scala.collection.Iterator.foreach(Iterator.scala:943)
[2023-01-26T23:52:41.729+0000] {spark_submit.py:495} INFO - at scala.collection.Iterator.foreach$(Iterator.scala:943)
[2023-01-26T23:52:41.729+0000] {spark_submit.py:495} INFO - at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
[2023-01-26T23:52:41.730+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$3(MongoSpark.scala:122)
[2023-01-26T23:52:41.730+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$3$adapted(MongoSpark.scala:121)
[2023-01-26T23:52:41.730+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.$anonfun$withCollectionDo$1(MongoConnector.scala:186)
[2023-01-26T23:52:41.730+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.$anonfun$withDatabaseDo$1(MongoConnector.scala:171)
[2023-01-26T23:52:41.730+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withMongoClientDo(MongoConnector.scala:154)
[2023-01-26T23:52:41.730+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withDatabaseDo(MongoConnector.scala:171)
[2023-01-26T23:52:41.730+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withCollectionDo(MongoConnector.scala:184)
[2023-01-26T23:52:41.730+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$2(MongoSpark.scala:121)
[2023-01-26T23:52:41.730+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$2$adapted(MongoSpark.scala:120)
[2023-01-26T23:52:41.731+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1011)
[2023-01-26T23:52:41.731+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1011)
[2023-01-26T23:52:41.731+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2268)
[2023-01-26T23:52:41.731+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
[2023-01-26T23:52:41.731+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.Task.run(Task.scala:136)
[2023-01-26T23:52:41.731+0000] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)
[2023-01-26T23:52:41.731+0000] {spark_submit.py:495} INFO - at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)
[2023-01-26T23:52:41.731+0000] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)
[2023-01-26T23:52:41.732+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[2023-01-26T23:52:41.732+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[2023-01-26T23:52:41.732+0000] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2023-01-26T23:52:41.732+0000] {spark_submit.py:495} INFO - [0m
[2023-01-26T23:58:58.093+0000] {spark_submit.py:495} INFO - [[34m2023-01-26 23:58:58,089[0m] {[34mclientserver.py:[0m577} INFO[0m - Received command c on object id p0[0m
[2023-01-26T23:59:00.716+0000] {spark_submit.py:495} INFO - -------------------------------------------
[2023-01-26T23:59:00.716+0000] {spark_submit.py:495} INFO - Batch: 18
[2023-01-26T23:59:00.716+0000] {spark_submit.py:495} INFO - -------------------------------------------
[2023-01-26T23:59:00.740+0000] {spark_submit.py:495} INFO - +-------------+--------------------+--------+-------------+--------------------+-------------------+------------+-----------------+--------------------+-------------+
[2023-01-26T23:59:00.740+0000] {spark_submit.py:495} INFO - |           id|       nameOfProduct|   price|ProductStatus|          SellerName|   RegistrationDate|QuantitySold|QuantityAvailable|      Free shipping?|Store ratings|
[2023-01-26T23:59:00.740+0000] {spark_submit.py:495} INFO - +-------------+--------------------+--------+-------------+--------------------+-------------------+------------+-----------------+--------------------+-------------+
[2023-01-26T23:59:00.740+0000] {spark_submit.py:495} INFO - |MLA1220964588|Procesador Gamer ...| 45899.0|          new|    OVERHARD DIGITAL|2015-01-08 12:19:41|          50|                1|Your shipment arr...|          4.5|
[2023-01-26T23:59:00.740+0000] {spark_submit.py:495} INFO - |MLA1157512489|Placa De Video Nv...|124999.0|          new|      PCREGISTRADAOK|2020-11-02 13:17:33|          50|                1|Your shipment arr...|          4.5|
[2023-01-26T23:59:00.741+0000] {spark_submit.py:495} INFO - |MLA1221192372|Disco Duro Extern...| 26299.0|          new|    OVERHARD DIGITAL|2015-01-08 12:19:41|         250|              100|Your shipment arr...|          4.5|
[2023-01-26T23:59:00.741+0000] {spark_submit.py:495} INFO - |MLA1163238194|Placa De Video Am...| 33999.0|          new|       MICYBERCOMPRA|2011-09-13 13:15:53|          50|                1|Your shipment arr...|          4.5|
[2023-01-26T23:59:00.741+0000] {spark_submit.py:495} INFO - |MLA1160204806|Memoria Ram Fury ...| 15499.0|          new|    OVERHARD DIGITAL|2015-01-08 12:19:41|         500|               50|Your shipment arr...|          4.5|
[2023-01-26T23:59:00.741+0000] {spark_submit.py:495} INFO - |MLA1115295971|Procesador Amd Ry...| 93999.0|          new|      PCREGISTRADAOK|2020-11-02 13:17:33|          25|                1|Your shipment arr...|          4.5|
[2023-01-26T23:59:00.741+0000] {spark_submit.py:495} INFO - |MLA1133481354|Disco Sólido Inte...| 13799.0|          new|           DALECLÍCK|2015-03-16 12:07:23|         500|                1|You have to pay t...|          4.5|
[2023-01-26T23:59:00.741+0000] {spark_submit.py:495} INFO - | MLA923637669|Placa De Video Ge...| 28699.0|          new|         INDIGOTRADE|2018-01-05 19:21:16|         500|               50|Your shipment arr...|          4.5|
[2023-01-26T23:59:00.741+0000] {spark_submit.py:495} INFO - |MLA1139583771|Disco Sólido Inte...|  8339.0|          new|DIAMONDSYSTEMPADUAML|2015-07-01 12:32:22|         250|              250|You have to pay t...|          4.5|
[2023-01-26T23:59:00.741+0000] {spark_submit.py:495} INFO - |MLA1228990119|Memoria Ram Fury ...| 23255.0|          new|MARSTECHSRLMARSTE...|2020-08-05 14:27:59|          50|                1|Your shipment arr...|          4.5|
[2023-01-26T23:59:00.741+0000] {spark_submit.py:495} INFO - | MLA882277177|Capturadora Video...|  2699.0|          new|         BSK-IMPORTS|2019-12-30 23:07:04|         500|               50|You have to pay t...|          3.0|
[2023-01-26T23:59:00.741+0000] {spark_submit.py:495} INFO - |MLA1148481665|Microprocesador P...| 58899.0|          new|            MALL WEB|2016-05-26 20:28:35|         150|                1|Your shipment arr...|          4.5|
[2023-01-26T23:59:00.741+0000] {spark_submit.py:495} INFO - | MLA844489972|Disco Duro Intern...| 11599.0|          new|           DALECLÍCK|2015-03-16 12:07:23|        5000|              500|You have to pay t...|          5.0|
[2023-01-26T23:59:00.741+0000] {spark_submit.py:495} INFO - |MLA1183925519|Placa De Video Nv...|299999.0|          new|ESTUDIOSMPSRLESTU...|2020-11-29 18:36:32|          50|                1|Your shipment arr...|          4.5|
[2023-01-26T23:59:00.741+0000] {spark_submit.py:495} INFO - |MLA1168362675|Memoria Ram Fury ...| 26698.0|          new|MARSTECHSRLMARSTE...|2020-08-05 14:27:59|         150|                1|Your shipment arr...|          4.5|
[2023-01-26T23:59:00.741+0000] {spark_submit.py:495} INFO - |MLA1314590445|Disco Sólido Inte...|  7999.0|          new|       ELECTRO-SHOWS|2010-03-26 01:08:16|         250|              250|You have to pay t...|          4.5|
[2023-01-26T23:59:00.741+0000] {spark_submit.py:495} INFO - |MLA1302136118|Disco Duro Intern...| 18271.0|          new|DIAMONDSYSTEMSANJ...|2016-04-08 17:21:06|         100|               50|Your shipment arr...|          4.5|
[2023-01-26T23:59:00.742+0000] {spark_submit.py:495} INFO - |MLA1163233412|Placa De Video Nv...| 34659.0|          new|      PCREGISTRADAOK|2020-11-02 13:17:33|           2|                1|Your shipment arr...|          4.5|
[2023-01-26T23:59:00.742+0000] {spark_submit.py:495} INFO - |MLA1137238635|Fuente De Aliment...|  3235.0|          new|     DIAMONDSYSTEMML|2014-04-07 22:00:36|          25|                1|You have to pay t...|          3.5|
[2023-01-26T23:59:00.742+0000] {spark_submit.py:495} INFO - |MLA1157644025|Disco Duro Extern...| 38790.0|          new|   ELECTRONIC-MARKET|2011-09-15 18:30:10|          25|               50|Your shipment arr...|          4.5|
[2023-01-26T23:59:00.742+0000] {spark_submit.py:495} INFO - +-------------+--------------------+--------+-------------+--------------------+-------------------+------------+-----------------+--------------------+-------------+
[2023-01-26T23:59:00.742+0000] {spark_submit.py:495} INFO - only showing top 20 rows
[2023-01-26T23:59:00.742+0000] {spark_submit.py:495} INFO - 
[2023-01-26T23:59:28.939+0000] {spark_submit.py:495} INFO - 23/01/26 23:59:28 ERROR Executor: Exception in task 0.0 in stage 51.0 (TID 51)
[2023-01-26T23:59:28.939+0000] {spark_submit.py:495} INFO - com.mongodb.MongoTimeoutException: Timed out after 30000 ms while waiting to connect. Client view of cluster state is {type=UNKNOWN, servers=[{address=172.22.0.2:27017, type=UNKNOWN, state=CONNECTING, exception={com.mongodb.MongoSocketOpenException: Exception opening socket}, caused by {java.net.SocketTimeoutException: connect timed out}}]
[2023-01-26T23:59:28.939+0000] {spark_submit.py:495} INFO - at com.mongodb.internal.connection.BaseCluster.getDescription(BaseCluster.java:177)
[2023-01-26T23:59:28.939+0000] {spark_submit.py:495} INFO - at com.mongodb.internal.connection.SingleServerCluster.getDescription(SingleServerCluster.java:41)
[2023-01-26T23:59:28.940+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate.getConnectedClusterDescription(MongoClientDelegate.java:147)
[2023-01-26T23:59:28.940+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate.createClientSession(MongoClientDelegate.java:98)
[2023-01-26T23:59:28.940+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.getClientSession(MongoClientDelegate.java:278)
[2023-01-26T23:59:28.940+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:202)
[2023-01-26T23:59:28.940+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:441)
[2023-01-26T23:59:28.940+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:421)
[2023-01-26T23:59:28.940+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$4(MongoSpark.scala:138)
[2023-01-26T23:59:28.940+0000] {spark_submit.py:495} INFO - at scala.collection.Iterator.foreach(Iterator.scala:943)
[2023-01-26T23:59:28.941+0000] {spark_submit.py:495} INFO - at scala.collection.Iterator.foreach$(Iterator.scala:943)
[2023-01-26T23:59:28.941+0000] {spark_submit.py:495} INFO - at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
[2023-01-26T23:59:28.941+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$3(MongoSpark.scala:122)
[2023-01-26T23:59:28.941+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$3$adapted(MongoSpark.scala:121)
[2023-01-26T23:59:28.941+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.$anonfun$withCollectionDo$1(MongoConnector.scala:186)
[2023-01-26T23:59:28.941+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.$anonfun$withDatabaseDo$1(MongoConnector.scala:171)
[2023-01-26T23:59:28.941+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withMongoClientDo(MongoConnector.scala:154)
[2023-01-26T23:59:28.941+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withDatabaseDo(MongoConnector.scala:171)
[2023-01-26T23:59:28.941+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withCollectionDo(MongoConnector.scala:184)
[2023-01-26T23:59:28.942+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$2(MongoSpark.scala:121)
[2023-01-26T23:59:28.942+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$2$adapted(MongoSpark.scala:120)
[2023-01-26T23:59:28.942+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1011)
[2023-01-26T23:59:28.942+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1011)
[2023-01-26T23:59:28.942+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2268)
[2023-01-26T23:59:28.942+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
[2023-01-26T23:59:28.943+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.Task.run(Task.scala:136)
[2023-01-26T23:59:28.943+0000] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)
[2023-01-26T23:59:28.943+0000] {spark_submit.py:495} INFO - at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)
[2023-01-26T23:59:28.943+0000] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)
[2023-01-26T23:59:28.943+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[2023-01-26T23:59:28.943+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[2023-01-26T23:59:28.943+0000] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2023-01-26T23:59:28.944+0000] {spark_submit.py:495} INFO - 23/01/26 23:59:28 ERROR TaskSetManager: Task 0 in stage 51.0 failed 1 times; aborting job
[2023-01-26T23:59:29.100+0000] {spark_submit.py:495} INFO - [[34m2023-01-26 23:59:29,100[0m] {[34mconsumerSpark.py:[0m37} ERROR[0m - Error al escribir en Mongo: An error occurred while calling o419.save.
[2023-01-26T23:59:29.101+0000] {spark_submit.py:495} INFO - : org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 51.0 failed 1 times, most recent failure: Lost task 0.0 in stage 51.0 (TID 51) (f1cbb5df32e6 executor driver): com.mongodb.MongoTimeoutException: Timed out after 30000 ms while waiting to connect. Client view of cluster state is {type=UNKNOWN, servers=[{address=172.22.0.2:27017, type=UNKNOWN, state=CONNECTING, exception={com.mongodb.MongoSocketOpenException: Exception opening socket}, caused by {java.net.SocketTimeoutException: connect timed out}}]
[2023-01-26T23:59:29.101+0000] {spark_submit.py:495} INFO - at com.mongodb.internal.connection.BaseCluster.getDescription(BaseCluster.java:177)
[2023-01-26T23:59:29.101+0000] {spark_submit.py:495} INFO - at com.mongodb.internal.connection.SingleServerCluster.getDescription(SingleServerCluster.java:41)
[2023-01-26T23:59:29.101+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate.getConnectedClusterDescription(MongoClientDelegate.java:147)
[2023-01-26T23:59:29.101+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate.createClientSession(MongoClientDelegate.java:98)
[2023-01-26T23:59:29.101+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.getClientSession(MongoClientDelegate.java:278)
[2023-01-26T23:59:29.101+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:202)
[2023-01-26T23:59:29.101+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:441)
[2023-01-26T23:59:29.101+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:421)
[2023-01-26T23:59:29.101+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$4(MongoSpark.scala:138)
[2023-01-26T23:59:29.101+0000] {spark_submit.py:495} INFO - at scala.collection.Iterator.foreach(Iterator.scala:943)
[2023-01-26T23:59:29.102+0000] {spark_submit.py:495} INFO - at scala.collection.Iterator.foreach$(Iterator.scala:943)
[2023-01-26T23:59:29.102+0000] {spark_submit.py:495} INFO - at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
[2023-01-26T23:59:29.102+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$3(MongoSpark.scala:122)
[2023-01-26T23:59:29.102+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$3$adapted(MongoSpark.scala:121)
[2023-01-26T23:59:29.102+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.$anonfun$withCollectionDo$1(MongoConnector.scala:186)
[2023-01-26T23:59:29.102+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.$anonfun$withDatabaseDo$1(MongoConnector.scala:171)
[2023-01-26T23:59:29.102+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withMongoClientDo(MongoConnector.scala:154)
[2023-01-26T23:59:29.102+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withDatabaseDo(MongoConnector.scala:171)
[2023-01-26T23:59:29.102+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withCollectionDo(MongoConnector.scala:184)
[2023-01-26T23:59:29.102+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$2(MongoSpark.scala:121)
[2023-01-26T23:59:29.102+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$2$adapted(MongoSpark.scala:120)
[2023-01-26T23:59:29.102+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1011)
[2023-01-26T23:59:29.102+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1011)
[2023-01-26T23:59:29.103+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2268)
[2023-01-26T23:59:29.103+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
[2023-01-26T23:59:29.103+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.Task.run(Task.scala:136)
[2023-01-26T23:59:29.103+0000] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)
[2023-01-26T23:59:29.103+0000] {spark_submit.py:495} INFO - at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)
[2023-01-26T23:59:29.103+0000] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)
[2023-01-26T23:59:29.103+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[2023-01-26T23:59:29.103+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[2023-01-26T23:59:29.103+0000] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2023-01-26T23:59:29.103+0000] {spark_submit.py:495} INFO - 
[2023-01-26T23:59:29.103+0000] {spark_submit.py:495} INFO - Driver stacktrace:
[2023-01-26T23:59:29.103+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)
[2023-01-26T23:59:29.103+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)
[2023-01-26T23:59:29.104+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)
[2023-01-26T23:59:29.104+0000] {spark_submit.py:495} INFO - at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
[2023-01-26T23:59:29.104+0000] {spark_submit.py:495} INFO - at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
[2023-01-26T23:59:29.104+0000] {spark_submit.py:495} INFO - at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
[2023-01-26T23:59:29.104+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)
[2023-01-26T23:59:29.104+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)
[2023-01-26T23:59:29.104+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)
[2023-01-26T23:59:29.104+0000] {spark_submit.py:495} INFO - at scala.Option.foreach(Option.scala:407)
[2023-01-26T23:59:29.104+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)
[2023-01-26T23:59:29.104+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)
[2023-01-26T23:59:29.104+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)
[2023-01-26T23:59:29.104+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)
[2023-01-26T23:59:29.104+0000] {spark_submit.py:495} INFO - at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
[2023-01-26T23:59:29.104+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)
[2023-01-26T23:59:29.105+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.runJob(SparkContext.scala:2228)
[2023-01-26T23:59:29.105+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.runJob(SparkContext.scala:2249)
[2023-01-26T23:59:29.105+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.runJob(SparkContext.scala:2268)
[2023-01-26T23:59:29.105+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.runJob(SparkContext.scala:2293)
[2023-01-26T23:59:29.105+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$1(RDD.scala:1011)
[2023-01-26T23:59:29.105+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
[2023-01-26T23:59:29.105+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
[2023-01-26T23:59:29.105+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.withScope(RDD.scala:406)
[2023-01-26T23:59:29.105+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.foreachPartition(RDD.scala:1009)
[2023-01-26T23:59:29.105+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.save(MongoSpark.scala:120)
[2023-01-26T23:59:29.105+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.save(MongoSpark.scala:169)
[2023-01-26T23:59:29.105+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.sql.DefaultSource.createRelation(DefaultSource.scala:70)
[2023-01-26T23:59:29.105+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:47)
[2023-01-26T23:59:29.105+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)
[2023-01-26T23:59:29.105+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)
[2023-01-26T23:59:29.106+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)
[2023-01-26T23:59:29.106+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)
[2023-01-26T23:59:29.106+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)
[2023-01-26T23:59:29.106+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)
[2023-01-26T23:59:29.106+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)
[2023-01-26T23:59:29.106+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
[2023-01-26T23:59:29.106+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
[2023-01-26T23:59:29.106+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)
[2023-01-26T23:59:29.106+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)
[2023-01-26T23:59:29.106+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:584)
[2023-01-26T23:59:29.106+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:176)
[2023-01-26T23:59:29.106+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:584)
[2023-01-26T23:59:29.106+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)
[2023-01-26T23:59:29.106+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
[2023-01-26T23:59:29.106+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
[2023-01-26T23:59:29.107+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
[2023-01-26T23:59:29.107+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
[2023-01-26T23:59:29.107+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:560)
[2023-01-26T23:59:29.107+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)
[2023-01-26T23:59:29.107+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)
[2023-01-26T23:59:29.107+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)
[2023-01-26T23:59:29.107+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:116)
[2023-01-26T23:59:29.107+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:860)
[2023-01-26T23:59:29.107+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:390)
[2023-01-26T23:59:29.107+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:363)
[2023-01-26T23:59:29.107+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:247)
[2023-01-26T23:59:29.107+0000] {spark_submit.py:495} INFO - at jdk.internal.reflect.GeneratedMethodAccessor69.invoke(Unknown Source)
[2023-01-26T23:59:29.107+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
[2023-01-26T23:59:29.107+0000] {spark_submit.py:495} INFO - at java.base/java.lang.reflect.Method.invoke(Method.java:566)
[2023-01-26T23:59:29.107+0000] {spark_submit.py:495} INFO - at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
[2023-01-26T23:59:29.108+0000] {spark_submit.py:495} INFO - at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
[2023-01-26T23:59:29.108+0000] {spark_submit.py:495} INFO - at py4j.Gateway.invoke(Gateway.java:282)
[2023-01-26T23:59:29.108+0000] {spark_submit.py:495} INFO - at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
[2023-01-26T23:59:29.108+0000] {spark_submit.py:495} INFO - at py4j.commands.CallCommand.execute(CallCommand.java:79)
[2023-01-26T23:59:29.108+0000] {spark_submit.py:495} INFO - at py4j.ClientServerConnection.sendCommand(ClientServerConnection.java:244)
[2023-01-26T23:59:29.108+0000] {spark_submit.py:495} INFO - at py4j.CallbackClient.sendCommand(CallbackClient.java:384)
[2023-01-26T23:59:29.108+0000] {spark_submit.py:495} INFO - at py4j.CallbackClient.sendCommand(CallbackClient.java:356)
[2023-01-26T23:59:29.108+0000] {spark_submit.py:495} INFO - at py4j.reflection.PythonProxyHandler.invoke(PythonProxyHandler.java:106)
[2023-01-26T23:59:29.108+0000] {spark_submit.py:495} INFO - at com.sun.proxy.$Proxy32.call(Unknown Source)
[2023-01-26T23:59:29.108+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1(ForeachBatchSink.scala:51)
[2023-01-26T23:59:29.108+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1$adapted(ForeachBatchSink.scala:51)
[2023-01-26T23:59:29.108+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.sources.ForeachBatchSink.addBatch(ForeachBatchSink.scala:32)
[2023-01-26T23:59:29.108+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:666)
[2023-01-26T23:59:29.108+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)
[2023-01-26T23:59:29.109+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)
[2023-01-26T23:59:29.109+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)
[2023-01-26T23:59:29.109+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
[2023-01-26T23:59:29.109+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
[2023-01-26T23:59:29.109+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:664)
[2023-01-26T23:59:29.109+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:375)
[2023-01-26T23:59:29.109+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:373)
[2023-01-26T23:59:29.109+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:68)
[2023-01-26T23:59:29.109+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:664)
[2023-01-26T23:59:29.109+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:256)
[2023-01-26T23:59:29.109+0000] {spark_submit.py:495} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2023-01-26T23:59:29.109+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:375)
[2023-01-26T23:59:29.109+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:373)
[2023-01-26T23:59:29.109+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:68)
[2023-01-26T23:59:29.109+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:219)
[2023-01-26T23:59:29.110+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)
[2023-01-26T23:59:29.110+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:213)
[2023-01-26T23:59:29.110+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:307)
[2023-01-26T23:59:29.110+0000] {spark_submit.py:495} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2023-01-26T23:59:29.110+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
[2023-01-26T23:59:29.110+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:285)
[2023-01-26T23:59:29.110+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:208)
[2023-01-26T23:59:29.110+0000] {spark_submit.py:495} INFO - Caused by: com.mongodb.MongoTimeoutException: Timed out after 30000 ms while waiting to connect. Client view of cluster state is {type=UNKNOWN, servers=[{address=172.22.0.2:27017, type=UNKNOWN, state=CONNECTING, exception={com.mongodb.MongoSocketOpenException: Exception opening socket}, caused by {java.net.SocketTimeoutException: connect timed out}}]
[2023-01-26T23:59:29.110+0000] {spark_submit.py:495} INFO - at com.mongodb.internal.connection.BaseCluster.getDescription(BaseCluster.java:177)
[2023-01-26T23:59:29.110+0000] {spark_submit.py:495} INFO - at com.mongodb.internal.connection.SingleServerCluster.getDescription(SingleServerCluster.java:41)
[2023-01-26T23:59:29.110+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate.getConnectedClusterDescription(MongoClientDelegate.java:147)
[2023-01-26T23:59:29.110+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate.createClientSession(MongoClientDelegate.java:98)
[2023-01-26T23:59:29.110+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.getClientSession(MongoClientDelegate.java:278)
[2023-01-26T23:59:29.110+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:202)
[2023-01-26T23:59:29.110+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:441)
[2023-01-26T23:59:29.111+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:421)
[2023-01-26T23:59:29.111+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$4(MongoSpark.scala:138)
[2023-01-26T23:59:29.111+0000] {spark_submit.py:495} INFO - at scala.collection.Iterator.foreach(Iterator.scala:943)
[2023-01-26T23:59:29.111+0000] {spark_submit.py:495} INFO - at scala.collection.Iterator.foreach$(Iterator.scala:943)
[2023-01-26T23:59:29.111+0000] {spark_submit.py:495} INFO - at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
[2023-01-26T23:59:29.111+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$3(MongoSpark.scala:122)
[2023-01-26T23:59:29.111+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$3$adapted(MongoSpark.scala:121)
[2023-01-26T23:59:29.111+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.$anonfun$withCollectionDo$1(MongoConnector.scala:186)
[2023-01-26T23:59:29.111+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.$anonfun$withDatabaseDo$1(MongoConnector.scala:171)
[2023-01-26T23:59:29.111+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withMongoClientDo(MongoConnector.scala:154)
[2023-01-26T23:59:29.111+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withDatabaseDo(MongoConnector.scala:171)
[2023-01-26T23:59:29.111+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withCollectionDo(MongoConnector.scala:184)
[2023-01-26T23:59:29.112+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$2(MongoSpark.scala:121)
[2023-01-26T23:59:29.112+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$2$adapted(MongoSpark.scala:120)
[2023-01-26T23:59:29.112+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1011)
[2023-01-26T23:59:29.112+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1011)
[2023-01-26T23:59:29.112+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2268)
[2023-01-26T23:59:29.112+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
[2023-01-26T23:59:29.112+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.Task.run(Task.scala:136)
[2023-01-26T23:59:29.112+0000] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)
[2023-01-26T23:59:29.112+0000] {spark_submit.py:495} INFO - at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)
[2023-01-26T23:59:29.112+0000] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)
[2023-01-26T23:59:29.112+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[2023-01-26T23:59:29.112+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[2023-01-26T23:59:29.113+0000] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2023-01-26T23:59:29.113+0000] {spark_submit.py:495} INFO - [0m
[2023-01-26T23:59:29.185+0000] {spark_submit.py:495} INFO - [[34m2023-01-26 23:59:29,185[0m] {[34mclientserver.py:[0m577} INFO[0m - Received command c on object id p0[0m
[2023-01-26T23:59:59.238+0000] {spark_submit.py:495} INFO - 23/01/26 23:59:59 ERROR Executor: Exception in task 0.0 in stage 53.0 (TID 53)
[2023-01-26T23:59:59.239+0000] {spark_submit.py:495} INFO - com.mongodb.MongoTimeoutException: Timed out after 30000 ms while waiting to connect. Client view of cluster state is {type=UNKNOWN, servers=[{address=172.22.0.2:27017, type=UNKNOWN, state=CONNECTING, exception={com.mongodb.MongoSocketOpenException: Exception opening socket}, caused by {java.net.SocketTimeoutException: connect timed out}}]
[2023-01-26T23:59:59.239+0000] {spark_submit.py:495} INFO - at com.mongodb.internal.connection.BaseCluster.getDescription(BaseCluster.java:177)
[2023-01-26T23:59:59.239+0000] {spark_submit.py:495} INFO - at com.mongodb.internal.connection.SingleServerCluster.getDescription(SingleServerCluster.java:41)
[2023-01-26T23:59:59.240+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate.getConnectedClusterDescription(MongoClientDelegate.java:147)
[2023-01-26T23:59:59.240+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate.createClientSession(MongoClientDelegate.java:98)
[2023-01-26T23:59:59.241+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.getClientSession(MongoClientDelegate.java:278)
[2023-01-26T23:59:59.248+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:202)
[2023-01-26T23:59:59.248+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:441)
[2023-01-26T23:59:59.249+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:421)
[2023-01-26T23:59:59.249+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$4(MongoSpark.scala:138)
[2023-01-26T23:59:59.249+0000] {spark_submit.py:495} INFO - at scala.collection.Iterator.foreach(Iterator.scala:943)
[2023-01-26T23:59:59.249+0000] {spark_submit.py:495} INFO - at scala.collection.Iterator.foreach$(Iterator.scala:943)
[2023-01-26T23:59:59.255+0000] {spark_submit.py:495} INFO - at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
[2023-01-26T23:59:59.255+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$3(MongoSpark.scala:122)
[2023-01-26T23:59:59.255+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$3$adapted(MongoSpark.scala:121)
[2023-01-26T23:59:59.255+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.$anonfun$withCollectionDo$1(MongoConnector.scala:186)
[2023-01-26T23:59:59.255+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.$anonfun$withDatabaseDo$1(MongoConnector.scala:171)
[2023-01-26T23:59:59.255+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withMongoClientDo(MongoConnector.scala:154)
[2023-01-26T23:59:59.255+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withDatabaseDo(MongoConnector.scala:171)
[2023-01-26T23:59:59.256+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withCollectionDo(MongoConnector.scala:184)
[2023-01-26T23:59:59.256+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$2(MongoSpark.scala:121)
[2023-01-26T23:59:59.256+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$2$adapted(MongoSpark.scala:120)
[2023-01-26T23:59:59.256+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1011)
[2023-01-26T23:59:59.256+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1011)
[2023-01-26T23:59:59.256+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2268)
[2023-01-26T23:59:59.256+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
[2023-01-26T23:59:59.256+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.Task.run(Task.scala:136)
[2023-01-26T23:59:59.256+0000] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)
[2023-01-26T23:59:59.256+0000] {spark_submit.py:495} INFO - at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)
[2023-01-26T23:59:59.256+0000] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)
[2023-01-26T23:59:59.257+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[2023-01-26T23:59:59.257+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[2023-01-26T23:59:59.257+0000] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2023-01-26T23:59:59.257+0000] {spark_submit.py:495} INFO - 23/01/26 23:59:59 ERROR TaskSetManager: Task 0 in stage 53.0 failed 1 times; aborting job
[2023-01-26T23:59:59.548+0000] {spark_submit.py:495} INFO - [[34m2023-01-26 23:59:59,548[0m] {[34mconsumerSpark.py:[0m37} ERROR[0m - Error al escribir en Mongo: An error occurred while calling o428.save.
[2023-01-26T23:59:59.549+0000] {spark_submit.py:495} INFO - : org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 53.0 failed 1 times, most recent failure: Lost task 0.0 in stage 53.0 (TID 53) (f1cbb5df32e6 executor driver): com.mongodb.MongoTimeoutException: Timed out after 30000 ms while waiting to connect. Client view of cluster state is {type=UNKNOWN, servers=[{address=172.22.0.2:27017, type=UNKNOWN, state=CONNECTING, exception={com.mongodb.MongoSocketOpenException: Exception opening socket}, caused by {java.net.SocketTimeoutException: connect timed out}}]
[2023-01-26T23:59:59.549+0000] {spark_submit.py:495} INFO - at com.mongodb.internal.connection.BaseCluster.getDescription(BaseCluster.java:177)
[2023-01-26T23:59:59.549+0000] {spark_submit.py:495} INFO - at com.mongodb.internal.connection.SingleServerCluster.getDescription(SingleServerCluster.java:41)
[2023-01-26T23:59:59.549+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate.getConnectedClusterDescription(MongoClientDelegate.java:147)
[2023-01-26T23:59:59.549+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate.createClientSession(MongoClientDelegate.java:98)
[2023-01-26T23:59:59.549+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.getClientSession(MongoClientDelegate.java:278)
[2023-01-26T23:59:59.549+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:202)
[2023-01-26T23:59:59.549+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:441)
[2023-01-26T23:59:59.549+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:421)
[2023-01-26T23:59:59.549+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$4(MongoSpark.scala:138)
[2023-01-26T23:59:59.550+0000] {spark_submit.py:495} INFO - at scala.collection.Iterator.foreach(Iterator.scala:943)
[2023-01-26T23:59:59.550+0000] {spark_submit.py:495} INFO - at scala.collection.Iterator.foreach$(Iterator.scala:943)
[2023-01-26T23:59:59.550+0000] {spark_submit.py:495} INFO - at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
[2023-01-26T23:59:59.550+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$3(MongoSpark.scala:122)
[2023-01-26T23:59:59.550+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$3$adapted(MongoSpark.scala:121)
[2023-01-26T23:59:59.550+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.$anonfun$withCollectionDo$1(MongoConnector.scala:186)
[2023-01-26T23:59:59.550+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.$anonfun$withDatabaseDo$1(MongoConnector.scala:171)
[2023-01-26T23:59:59.550+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withMongoClientDo(MongoConnector.scala:154)
[2023-01-26T23:59:59.550+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withDatabaseDo(MongoConnector.scala:171)
[2023-01-26T23:59:59.550+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withCollectionDo(MongoConnector.scala:184)
[2023-01-26T23:59:59.550+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$2(MongoSpark.scala:121)
[2023-01-26T23:59:59.550+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$2$adapted(MongoSpark.scala:120)
[2023-01-26T23:59:59.550+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1011)
[2023-01-26T23:59:59.551+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1011)
[2023-01-26T23:59:59.551+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2268)
[2023-01-26T23:59:59.551+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
[2023-01-26T23:59:59.551+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.Task.run(Task.scala:136)
[2023-01-26T23:59:59.551+0000] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)
[2023-01-26T23:59:59.551+0000] {spark_submit.py:495} INFO - at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)
[2023-01-26T23:59:59.551+0000] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)
[2023-01-26T23:59:59.551+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[2023-01-26T23:59:59.551+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[2023-01-26T23:59:59.551+0000] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2023-01-26T23:59:59.551+0000] {spark_submit.py:495} INFO - 
[2023-01-26T23:59:59.551+0000] {spark_submit.py:495} INFO - Driver stacktrace:
[2023-01-26T23:59:59.551+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)
[2023-01-26T23:59:59.551+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)
[2023-01-26T23:59:59.552+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)
[2023-01-26T23:59:59.552+0000] {spark_submit.py:495} INFO - at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
[2023-01-26T23:59:59.552+0000] {spark_submit.py:495} INFO - at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
[2023-01-26T23:59:59.552+0000] {spark_submit.py:495} INFO - at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
[2023-01-26T23:59:59.552+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)
[2023-01-26T23:59:59.552+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)
[2023-01-26T23:59:59.552+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)
[2023-01-26T23:59:59.553+0000] {spark_submit.py:495} INFO - at scala.Option.foreach(Option.scala:407)
[2023-01-26T23:59:59.553+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)
[2023-01-26T23:59:59.553+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)
[2023-01-26T23:59:59.553+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)
[2023-01-26T23:59:59.553+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)
[2023-01-26T23:59:59.553+0000] {spark_submit.py:495} INFO - at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
[2023-01-26T23:59:59.553+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)
[2023-01-26T23:59:59.553+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.runJob(SparkContext.scala:2228)
[2023-01-26T23:59:59.553+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.runJob(SparkContext.scala:2249)
[2023-01-26T23:59:59.553+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.runJob(SparkContext.scala:2268)
[2023-01-26T23:59:59.553+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.runJob(SparkContext.scala:2293)
[2023-01-26T23:59:59.553+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$1(RDD.scala:1011)
[2023-01-26T23:59:59.553+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
[2023-01-26T23:59:59.554+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
[2023-01-26T23:59:59.554+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.withScope(RDD.scala:406)
[2023-01-26T23:59:59.554+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.foreachPartition(RDD.scala:1009)
[2023-01-26T23:59:59.554+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.save(MongoSpark.scala:120)
[2023-01-26T23:59:59.554+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.save(MongoSpark.scala:169)
[2023-01-26T23:59:59.554+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.sql.DefaultSource.createRelation(DefaultSource.scala:70)
[2023-01-26T23:59:59.554+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:47)
[2023-01-26T23:59:59.554+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)
[2023-01-26T23:59:59.554+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)
[2023-01-26T23:59:59.554+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)
[2023-01-26T23:59:59.554+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)
[2023-01-26T23:59:59.554+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)
[2023-01-26T23:59:59.554+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)
[2023-01-26T23:59:59.555+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)
[2023-01-26T23:59:59.555+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
[2023-01-26T23:59:59.555+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
[2023-01-26T23:59:59.555+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)
[2023-01-26T23:59:59.555+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)
[2023-01-26T23:59:59.555+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:584)
[2023-01-26T23:59:59.555+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:176)
[2023-01-26T23:59:59.555+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:584)
[2023-01-26T23:59:59.555+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)
[2023-01-26T23:59:59.555+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
[2023-01-26T23:59:59.555+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
[2023-01-26T23:59:59.555+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
[2023-01-26T23:59:59.555+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
[2023-01-26T23:59:59.556+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:560)
[2023-01-26T23:59:59.556+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)
[2023-01-26T23:59:59.556+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)
[2023-01-26T23:59:59.556+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)
[2023-01-26T23:59:59.556+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:116)
[2023-01-26T23:59:59.556+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:860)
[2023-01-26T23:59:59.560+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:390)
[2023-01-26T23:59:59.560+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:363)
[2023-01-26T23:59:59.560+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:247)
[2023-01-26T23:59:59.560+0000] {spark_submit.py:495} INFO - at jdk.internal.reflect.GeneratedMethodAccessor69.invoke(Unknown Source)
[2023-01-26T23:59:59.561+0000] {spark_submit.py:495} INFO - at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
[2023-01-26T23:59:59.561+0000] {spark_submit.py:495} INFO - at java.base/java.lang.reflect.Method.invoke(Method.java:566)
[2023-01-26T23:59:59.561+0000] {spark_submit.py:495} INFO - at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
[2023-01-26T23:59:59.561+0000] {spark_submit.py:495} INFO - at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
[2023-01-26T23:59:59.561+0000] {spark_submit.py:495} INFO - at py4j.Gateway.invoke(Gateway.java:282)
[2023-01-26T23:59:59.561+0000] {spark_submit.py:495} INFO - at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
[2023-01-26T23:59:59.561+0000] {spark_submit.py:495} INFO - at py4j.commands.CallCommand.execute(CallCommand.java:79)
[2023-01-26T23:59:59.561+0000] {spark_submit.py:495} INFO - at py4j.ClientServerConnection.sendCommand(ClientServerConnection.java:244)
[2023-01-26T23:59:59.561+0000] {spark_submit.py:495} INFO - at py4j.CallbackClient.sendCommand(CallbackClient.java:384)
[2023-01-26T23:59:59.561+0000] {spark_submit.py:495} INFO - at py4j.CallbackClient.sendCommand(CallbackClient.java:356)
[2023-01-26T23:59:59.561+0000] {spark_submit.py:495} INFO - at py4j.reflection.PythonProxyHandler.invoke(PythonProxyHandler.java:106)
[2023-01-26T23:59:59.561+0000] {spark_submit.py:495} INFO - at com.sun.proxy.$Proxy32.call(Unknown Source)
[2023-01-26T23:59:59.561+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1(ForeachBatchSink.scala:51)
[2023-01-26T23:59:59.562+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1$adapted(ForeachBatchSink.scala:51)
[2023-01-26T23:59:59.562+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.sources.ForeachBatchSink.addBatch(ForeachBatchSink.scala:32)
[2023-01-26T23:59:59.562+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:666)
[2023-01-26T23:59:59.562+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)
[2023-01-26T23:59:59.562+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)
[2023-01-26T23:59:59.562+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)
[2023-01-26T23:59:59.562+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
[2023-01-26T23:59:59.562+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
[2023-01-26T23:59:59.562+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:664)
[2023-01-26T23:59:59.562+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:375)
[2023-01-26T23:59:59.562+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:373)
[2023-01-26T23:59:59.562+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:68)
[2023-01-26T23:59:59.562+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:664)
[2023-01-26T23:59:59.562+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:256)
[2023-01-26T23:59:59.563+0000] {spark_submit.py:495} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2023-01-26T23:59:59.563+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:375)
[2023-01-26T23:59:59.563+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:373)
[2023-01-26T23:59:59.563+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:68)
[2023-01-26T23:59:59.563+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:219)
[2023-01-26T23:59:59.563+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)
[2023-01-26T23:59:59.563+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:213)
[2023-01-26T23:59:59.563+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:307)
[2023-01-26T23:59:59.563+0000] {spark_submit.py:495} INFO - at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2023-01-26T23:59:59.563+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
[2023-01-26T23:59:59.563+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:285)
[2023-01-26T23:59:59.563+0000] {spark_submit.py:495} INFO - at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:208)
[2023-01-26T23:59:59.563+0000] {spark_submit.py:495} INFO - Caused by: com.mongodb.MongoTimeoutException: Timed out after 30000 ms while waiting to connect. Client view of cluster state is {type=UNKNOWN, servers=[{address=172.22.0.2:27017, type=UNKNOWN, state=CONNECTING, exception={com.mongodb.MongoSocketOpenException: Exception opening socket}, caused by {java.net.SocketTimeoutException: connect timed out}}]
[2023-01-26T23:59:59.564+0000] {spark_submit.py:495} INFO - at com.mongodb.internal.connection.BaseCluster.getDescription(BaseCluster.java:177)
[2023-01-26T23:59:59.564+0000] {spark_submit.py:495} INFO - at com.mongodb.internal.connection.SingleServerCluster.getDescription(SingleServerCluster.java:41)
[2023-01-26T23:59:59.564+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate.getConnectedClusterDescription(MongoClientDelegate.java:147)
[2023-01-26T23:59:59.564+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate.createClientSession(MongoClientDelegate.java:98)
[2023-01-26T23:59:59.564+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.getClientSession(MongoClientDelegate.java:278)
[2023-01-26T23:59:59.564+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:202)
[2023-01-26T23:59:59.566+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:441)
[2023-01-26T23:59:59.566+0000] {spark_submit.py:495} INFO - at com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:421)
[2023-01-26T23:59:59.566+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$4(MongoSpark.scala:138)
[2023-01-26T23:59:59.567+0000] {spark_submit.py:495} INFO - at scala.collection.Iterator.foreach(Iterator.scala:943)
[2023-01-26T23:59:59.567+0000] {spark_submit.py:495} INFO - at scala.collection.Iterator.foreach$(Iterator.scala:943)
[2023-01-26T23:59:59.567+0000] {spark_submit.py:495} INFO - at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
[2023-01-26T23:59:59.567+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$3(MongoSpark.scala:122)
[2023-01-26T23:59:59.567+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$3$adapted(MongoSpark.scala:121)
[2023-01-26T23:59:59.567+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.$anonfun$withCollectionDo$1(MongoConnector.scala:186)
[2023-01-26T23:59:59.567+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.$anonfun$withDatabaseDo$1(MongoConnector.scala:171)
[2023-01-26T23:59:59.567+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withMongoClientDo(MongoConnector.scala:154)
[2023-01-26T23:59:59.567+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withDatabaseDo(MongoConnector.scala:171)
[2023-01-26T23:59:59.567+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoConnector.withCollectionDo(MongoConnector.scala:184)
[2023-01-26T23:59:59.567+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$2(MongoSpark.scala:121)
[2023-01-26T23:59:59.567+0000] {spark_submit.py:495} INFO - at com.mongodb.spark.MongoSpark$.$anonfun$save$2$adapted(MongoSpark.scala:120)
[2023-01-26T23:59:59.567+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1011)
[2023-01-26T23:59:59.568+0000] {spark_submit.py:495} INFO - at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1011)
[2023-01-26T23:59:59.568+0000] {spark_submit.py:495} INFO - at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2268)
[2023-01-26T23:59:59.568+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
[2023-01-26T23:59:59.568+0000] {spark_submit.py:495} INFO - at org.apache.spark.scheduler.Task.run(Task.scala:136)
[2023-01-26T23:59:59.568+0000] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)
[2023-01-26T23:59:59.568+0000] {spark_submit.py:495} INFO - at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)
[2023-01-26T23:59:59.568+0000] {spark_submit.py:495} INFO - at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)
[2023-01-26T23:59:59.568+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[2023-01-26T23:59:59.568+0000] {spark_submit.py:495} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[2023-01-26T23:59:59.568+0000] {spark_submit.py:495} INFO - at java.base/java.lang.Thread.run(Thread.java:829)
[2023-01-26T23:59:59.568+0000] {spark_submit.py:495} INFO - [0m
[2023-01-27T00:11:31.611+0000] {taskinstance.py:1081} INFO - Dependencies not met for <TaskInstance: Streaming_pipeline_meli.consumer_data_of_topic manual__2023-01-21T05:36:28.572625+00:00 [running]>, dependency 'Task Instance State' FAILED: Task is in the 'running' state.
[2023-01-27T00:11:31.619+0000] {taskinstance.py:1081} INFO - Dependencies not met for <TaskInstance: Streaming_pipeline_meli.consumer_data_of_topic manual__2023-01-21T05:36:28.572625+00:00 [running]>, dependency 'Task Instance Not Running' FAILED: Task is in the running state
[2023-01-27T00:11:31.620+0000] {local_task_job.py:98} INFO - Task is not able to be run
